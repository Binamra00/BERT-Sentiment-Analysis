"""
calibrate.py

This script performs post-hoc probability calibration using Isotonic Regression.
It loads uncalibrated probabilities generated by 'run_probability_generation.py',
calculates baseline ECE and Brier scores, fits a calibrator on the
validation set, and then reports the calibrated scores on the test set.

It also generates two plots:
1. A Reliability Diagram (before vs. after calibration).
2. The Isotonic Regression correction function itself.

Usage:
    python src/postprocessing/calibrate.py --run_name "bert_full_finetune_seed_123.pt"
"""

import numpy as np
import os
import argparse
import matplotlib.pyplot as plt
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss

# --- 1. Helper Function to Calculate Metrics ---

def calculate_metrics(probs, labels, n_bins=15):
    """
    Calculates Expected Calibration Error (ECE) and Brier Score.
    Also returns the binned confidences and accuracies for plotting.
    """
    # Calculate Brier Score
    brier = brier_score_loss(labels, probs)
    
    # Calculate ECE
    bin_limits = np.linspace(0, 1, n_bins + 1)
    bin_lowers = bin_limits[:-1]
    bin_uppers = bin_limits[1:]

    ece = 0
    total_samples = len(probs)
    
    # For plotting reliability diagram
    bin_confidences = []
    bin_accuracies = []
    
    for i in range(n_bins):
        # Find samples in this bin
        in_bin = (probs > bin_lowers[i]) & (probs <= bin_uppers[i])
        prop_in_bin = np.mean(in_bin) # Percentage of samples in this bin
        
        if prop_in_bin > 0:
            # Calculate accuracy in this bin
            accuracy_in_bin = np.mean(labels[in_bin])
            
            # Calculate average confidence in this bin
            avg_confidence_in_bin = np.mean(probs[in_bin])
            
            # Add to ECE
            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
            
            # Store for plotting
            bin_confidences.append(avg_confidence_in_bin)
            bin_accuracies.append(accuracy_in_bin)
            
    return ece, brier, bin_confidences, bin_accuracies

# --- 2. Plotting Functions ---

def plot_reliability_diagram(uncal_conf, uncal_acc, cal_conf, cal_acc, plot_path):
    """
    Plots the Reliability Diagram (before vs. after calibration).
    """
    plt.figure(figsize=(10, 7))
    # Plot the perfect calibration line
    plt.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')
    
    # Plot the uncalibrated model
    plt.plot(uncal_conf, uncal_acc, 'bo-', label='Uncalibrated Model')
    
    # Plot the calibrated model
    plt.plot(cal_conf, cal_acc, 'go-', label='Calibrated Model (Isotonic)')
    
    plt.title('Reliability Diagram (Before vs. After Calibration)', fontsize=16)
    plt.xlabel('Average Confidence', fontsize=12)
    plt.ylabel('Actual Accuracy', fontsize=12)
    plt.legend(loc='lower right')
    plt.grid(alpha=0.5)
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300)
    print(f"\nReliability Diagram saved to {plot_path}")

def plot_correction_function(ir_model, plot_path):
    """
    Plots the Isotonic Regression correction function itself.
    """
    # Create a range of input probabilities
    x_range = np.linspace(0, 1, 100)
    # Get the "corrected" output for each
    y_corrected = ir_model.transform(x_range)
    
    plt.figure(figsize=(10, 7))
    # Plot the perfect calibration line
    plt.plot([0, 1], [0, 1], 'r--', label='Perfect (No Correction Needed)')
    
    # Plot the learned correction function
    plt.plot(x_range, y_corrected, 'b-', label='Learned Correction Function')
    
    plt.title('Isotonic Regression Correction Function', fontsize=16)
    plt.xlabel('Uncalibrated Probability (Input)', fontsize=12)
    plt.ylabel('Calibrated Probability (Output)', fontsize=12)
    plt.legend(loc='lower right')
    plt.grid(alpha=0.5)
    plt.tight_layout()
    plt.savefig(plot_path, dpi=300)
    print(f"Correction Function plot saved to {plot_path}")

# --- 3. Main Execution ---

def main(args):
    # --- Define Paths ---
    base_dir = os.path.join(os.path.dirname(__file__), "..", "..") # Project root
    probs_dir = os.path.join(base_dir, "outputs", "probabilities")
    plot_dir = os.path.join(base_dir, "outputs", "plots")
    os.makedirs(plot_dir, exist_ok=True) # Ensure plot directory exists
    
    val_path = os.path.join(probs_dir, f"{args.run_name}_validation_outputs.npz")
    test_path = os.path.join(probs_dir, f"{args.run_name}_test_outputs.npz")
    
    print(f"Loading probabilities for: {args.run_name}\n")
    
    # --- Load Data ---
    try:
        val_data = np.load(val_path)
        test_data = np.load(test_path)
    except FileNotFoundError:
        print(f"Error: Probability files not found at {val_path} or {test_path}")
        print("Please run 'run_probability_generation.py' first.")
        return

    val_probs = val_data['probs']
    val_labels = val_data['labels']
    test_probs = test_data['probs']
    test_labels = test_data['labels']
    
    print(f"Loaded {len(val_probs)} validation samples and {len(test_probs)} test samples.")
    
    # --- 1. Calculate Uncalibrated Metrics ---
    print("\n--- Uncalibrated Model (Baseline) ---")
    uncal_ece, uncal_brier, uncal_conf, uncal_acc = calculate_metrics(test_probs, test_labels)
    print(f"   Test ECE:    {uncal_ece:.6f}")
    print(f"   Test Brier:  {uncal_brier:.6f}")
    
    # --- 2. Fit Calibrator ---
    print("\n--- Calibrating Model ---")
    ir = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')
    
    print("Fitting IsotonicRegression on validation data...")
    ir.fit(val_probs, val_labels)
    print("Calibration model fitted.")
    
    # --- 3. Apply Calibrator and Get New Metrics ---
    print("\n--- Calibrated Model (Results) ---")
    cal_test_probs = ir.transform(test_probs)
    cal_ece, cal_brier, cal_conf, cal_acc = calculate_metrics(cal_test_probs, test_labels)
    print(f"   Test ECE:    {cal_ece:.6f}")
    print(f"   Test Brier:  {cal_brier:.6f}")
    
    # --- 4. Show Improvement ---
    print("\n--- Summary of Improvements ---")
    ece_change = (cal_ece - uncal_ece) / uncal_ece * 100
    brier_change = (cal_brier - uncal_brier) / uncal_brier * 100
    
    print(f"   ECE Change:      {ece_change:.2f}%")
    print(f"   Brier Change:    {brier_change:.2f}%")
    
    if ece_change < 0 and brier_change < 0:
        print("\nHypothesis 2 Confirmed: ECE and Brier Score were both reduced.")
    else:
        print("\nCalibration did not improve all metrics.")

    # --- 5. Generate and Save Plots ---
    print("\n--- Generating Plots ---")
    
    # Plot 1: Reliability Diagram
    plot_path_reliability = os.path.join(plot_dir, f"{args.run_name}_reliability_diagram.png")
    plot_reliability_diagram(uncal_conf, uncal_acc, cal_conf, cal_acc, plot_path_reliability)
    
    # Plot 2: Correction Function
    plot_path_correction = os.path.join(plot_dir, f"{args.run_name}_correction_function.png")
    plot_correction_function(ir, plot_path_correction)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run post-hoc probability calibration and generate plots."
    )
    
    parser.add_argument(
        "--run_name",
        type=str,
        required=True,
        help="The full name of the probability run to load (e.g., 'bert_full_finetune_seed_123.pt')"
    )
    
    args = parser.parse_args()
    main(args)