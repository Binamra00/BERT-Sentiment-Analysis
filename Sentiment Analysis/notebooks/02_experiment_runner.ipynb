{"cells":[{"cell_type":"markdown","metadata":{"id":"sqlvkrXpvG5x"},"source":["# Section 1: Initial Setup"]},{"cell_type":"markdown","metadata":{"id":"PpmWPFem2BKj"},"source":["### 1. Setup and Environment\n","This section will mount Google Drive and install dependencies.\n","CSE 6363 Project-Sentiment Analysis With BERT-based-uncased"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22251,"status":"ok","timestamp":1763261278292,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":360},"id":"LZDuJ8n8lYiC","outputId":"5d2af502-cd44-4e48-da4f-b3f75924aac6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","Drive mounted successfully.\n","Changing directory to: /content/drive/My Drive/CSE 6363 Project-Sentiment Analysis With BERT-based-uncased/Sentiment Analysis\n"]}],"source":["from google.colab import drive\n","import os\n","\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","print(\"Drive mounted successfully.\")\n","\n","project_path = \"/content/drive/My Drive/CSE 6363 Project-Sentiment Analysis With BERT-based-uncased/Sentiment Analysis\"\n","\n","print(f\"Changing directory to: {project_path}\")\n","os.chdir(project_path)\n"]},{"cell_type":"markdown","metadata":{"id":"BdWg1z5_81pI"},"source":["### 2. Data Analysis: Justifying Truncation Length\n","Before building the models, we need to justify our choice for `max_length`. This section analyzes the distribution of review lengths in our processed datasets to ensure that a `max_length` of 512 is a safe and effective choice that minimizes potential information loss."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3146,"status":"ok","timestamp":1761261570654,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"ICRMP2lk-5F6","outputId":"c30b8ff4-1270-449a-d0bd-32f999c71255"},"outputs":[{"name":"stdout","output_type":"stream","text":["Datasets loaded successfully!\n","--------------------------------------------------\n","Analysis for: Training Dataset\n","--------------------------------------------------\n","Total reviews: 22500\n","Average review length: 263.80 words\n","Median review length: 198 words\n","Standard Deviation: 194.90\n","Min length: 11 | Max length: 2717\n","\n","--- Percentiles ---\n","90% of reviews are shorter than 516 words.\n","95% of reviews are shorter than 673 words.\n","99% of reviews are shorter than 1020 words.\n","\n","--- Coverage at max_length=512 ---\n","Percentage of reviews covered: 89.82%\n","--------------------------------------------------\n","\n","--------------------------------------------------\n","Analysis for: Validation Dataset\n","--------------------------------------------------\n","Total reviews: 2500\n","Average review length: 262.99 words\n","Median review length: 196 words\n","Standard Deviation: 192.76\n","Min length: 12 | Max length: 1671\n","\n","--- Percentiles ---\n","90% of reviews are shorter than 506 words.\n","95% of reviews are shorter than 670 words.\n","99% of reviews are shorter than 1017 words.\n","\n","--- Coverage at max_length=512 ---\n","Percentage of reviews covered: 90.12%\n","--------------------------------------------------\n","\n","--------------------------------------------------\n","Analysis for: Test Dataset\n","--------------------------------------------------\n","Total reviews: 25000\n","Average review length: 257.88 words\n","Median review length: 195 words\n","Standard Deviation: 189.25\n","Min length: 8 | Max length: 2530\n","\n","--- Percentiles ---\n","90% of reviews are shorter than 499 words.\n","95% of reviews are shorter than 654 words.\n","99% of reviews are shorter than 1008 words.\n","\n","--- Coverage at max_length=512 ---\n","Percentage of reviews covered: 90.53%\n","--------------------------------------------------\n","\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","train_path = 'data/processed/train_clean.csv'\n","val_path = 'data/processed/validation_clean.csv'\n","test_path = 'data/processed/test_clean.csv'\n","\n","try:\n","    df_train = pd.read_csv(train_path)\n","    df_val = pd.read_csv(val_path)\n","    df_test = pd.read_csv(test_path)\n","    print(\"Datasets loaded successfully!\")\n","except FileNotFoundError as e:\n","    print(f\"Error: {e}. Make sure the CSV files are in the 'data/processed/' directory.\")\n","\n","def analyze_lengths(df, name):\n","    df['word_count'] = df['review_text'].str.split().str.len()\n","\n","    desc_stats = df['word_count'].describe()\n","    percentiles = df['word_count'].quantile([0.90, 0.95, 0.99])\n","\n","    coverage_512 = (df['word_count'] <= 512).mean() * 100\n","\n","    print(\"-\" * 50)\n","    print(f\"Analysis for: {name} Dataset\")\n","    print(\"-\" * 50)\n","    print(f\"Total reviews: {len(df)}\")\n","    print(f\"Average review length: {desc_stats['mean']:.2f} words\")\n","    print(f\"Median review length: {desc_stats['50%']:.0f} words\")\n","    print(f\"Standard Deviation: {desc_stats['std']:.2f}\")\n","    print(f\"Min length: {desc_stats['min']:.0f} | Max length: {desc_stats['max']:.0f}\")\n","    print(\"\\n--- Percentiles ---\")\n","    print(f\"90% of reviews are shorter than {percentiles.loc[0.90]:.0f} words.\")\n","    print(f\"95% of reviews are shorter than {percentiles.loc[0.95]:.0f} words.\")\n","    print(f\"99% of reviews are shorter than {percentiles.loc[0.99]:.0f} words.\")\n","    print(\"\\n--- Coverage at max_length=512 ---\")\n","    print(f\"Percentage of reviews covered: {coverage_512:.2f}%\")\n","    print(\"-\" * 50 + \"\\n\")\n","\n","    return df\n","\n","if 'df_train' in locals():\n","    df_train = analyze_lengths(df_train, \"Training\")\n","    df_val = analyze_lengths(df_val, \"Validation\")\n","    df_test = analyze_lengths(df_test, \"Test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"executionInfo":{"elapsed":1721,"status":"ok","timestamp":1761091091312,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"mkx7gLIyAZiJ","outputId":"9e734f71-2344-49d1-99c9-cab638793f3c"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABBAAAAJ2CAYAAADmGHGNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA+ehJREFUeJzs3Xd4VGX6//H3zCSTHkIaJaFjEkqABAREEAUURVEBRVeFRZFVxFXUr4K7yq66u9hWBXXXn4ootrUhuoq4YkGRtghIb1JTII30MpOZ8/sjZpYhCemZlM/ruriSnPOc59xn8syQc5+nmAzDMBAREREREREROQuzpwMQERERERERkeZPCQQRERERERERqZYSCCIiIiIiIiJSLSUQRERERERERKRaSiCIiIiIiIiISLWUQBARERERERGRaimBICIiIiIiIiLVUgJBRERERERERKqlBIKIiIiIiIiIVEsJBBFpccaMGUNsbKzrX1xcHAkJCVxwwQVMmzaNJ554gu3bt5+1jmnTphEbG8vGjRubKOqzK7+mpKQkt+3NLU6A+fPnExsby/Llyz0dSqP45ptvuOGGG0hMTHS1sZq8/me2y9jYWPr3788FF1zA7Nmz+fbbb5sg+rNrju2pOs8//zyxsbFMmzbN06E0ufJ21BysXr2a2NhYXnvtNde2+++/n9jYWB599NEqj7v44ouJjY3l4osvrrLMo48+SmxsLPfff3+DxlxbSUlJxMbGMmbMmDrX8eOPP/Lggw8yfvx4EhMT6d+/PyNHjuTmm2/m9ddfJysrqwEjbj4cDgeXXnopF110EcXFxZ4OR0QakRIIItJiJSYmMmnSJK6++mpGjx5Njx492LdvH6+99hrXXnst06ZN4/jx440aQ1U3/i3V8uXLiY2NZf78+Z4OxSP27NnDXXfdxbZt2xg0aBBXX301kyZNIjw8vMZ1lLfLSZMmMXr0aLy8vPjmm2+4/fbbWbhwYSNGLy1JS0rE2Ww2Hn/8cTp16sSNN97o2j5s2DAANm3aVOlxqampHDt2DIBjx45x4sSJSsuVJ7TK62uJsrKyuPnmm7nllltYvnw5drudYcOGMX78eHr27MnWrVtZuHAhY8eO5eeff/Z0uLVWnsh7/vnnK91vsViYO3cuKSkpvPrqq00cnYg0JS9PByAiUlfXXnstkydPdttmGAbff/89f/vb39i0aRPXX389//rXv+jSpYtbuSeeeIKioiI6d+7clCFX6fXXX8dut9OhQwdPh1Kte++9l1mzZhEZGenpUBrc6tWrsdvt3H777dxzzz11quPMdllaWsrChQt56623eP3117n88ssZMGBAQ4VcK82t3UvL8Oabb3L8+HEWLFiAj4+Pa3v5Df+BAwfIzMwkLCzM7bjyxEC/fv3YtWsXGzdu5KqrrnIrk5mZycGDB93qa2ny8vK44YYbOHz4MD179uSxxx5jyJAhbmVsNhsff/wxzz//POnp6R6KtHFdeumlxMTE8Morr3DdddcRERHh6ZBEpBGoB4KItComk4nRo0fzwQcf0L17dzIyMnjooYcqlOvcuTO9evXCz8/PA1FW1LVrV3r16oW3t7enQ6lWZGQkvXr1IigoyNOhNLiUlBQAunXr1mB1enl58cADDxAYGAjg0aEMza3dS/PncDh466238PHx4corr3Tb16VLF6KiogAqHRZT3jPhjjvuqLJM+baoqKgKid6W4rHHHuPw4cNERUXx7rvvVkgeAFitVq677jpWrFhBz549PRBl05gyZQrFxcW89957ng5FRBqJEggi0ioFBwfzhz/8AYANGzawc+dOt/1VjQW32Wy8+uqrTJ48mYSEBPr378/555/PlClTePLJJ8nOzgb+19U/OTkZgLFjx7qNfS+vd+PGja7x20VFRSxatIjLLruMgQMHuo2zrclQiE2bNnHLLbcwdOhQBg4cyDXXXMOKFSsqLVvdWPfKuqOOGTOGBx98EICPP/7Y7XpOH39eXdfrzz//nN/+9rcMHTqU/v37c9FFF/Hggw9y+PDhSsuffu0bNmzglltu4dxzz2XAgAFMmjSpymusTmlpKe+++y7XX389gwcPJj4+nksuuYS//OUvnDx5stLXo/yaHnzwwUqvva58fHxcSYnMzMxKy6xfv54777yTkSNH0r9/f8477zzmzJnD1q1b3cr98ssvxMbGcu6551JSUlLlOSdPnkxsbCyrV692bauuXdQ0BsMwGDZsGHFxcZw6dcpt3/bt212v3dtvv13hHOXvlcYeXlTTayl3+nwDX375Jb/5zW9ITExk0KBBXH/99axZs6bKcyUnJzN//nzOP/98VztbvHgxJSUlFV7z8nH2H3/8MeDe1s7WRbw2MaWlpfGXv/yF8ePHEx8fz8CBAxk9ejS//e1vWbJkSY1fQyibEyQlJYVx48ZVmjQ82zCGTZs2ERoaytixY4mMjKyyzOn1lCsqKuLll19m0qRJJCQkMHDgQC6//HKeffZZcnJyKtRz+vwFDoeDpUuXcvXVV5OQkFBhHolvv/2Wm266iYSEBAYPHswNN9zg9j6pjePHj/PZZ58BZb/LkJCQs5YPDw+vNIFQ28/N6ubHqOq9fvr2PXv2cOeddzJs2DD69+/PhAkTeO211zAMo8K5XnjhBQBeeOEFt/Z65nC3K6+8Ei8vL9577z1KS0vP+lqISMukIQwi0mpdcMEFhISEkJ2dzbp16+jfv/9ZyzudTn73u9+xfv16AgMDGTJkCMHBwWRlZXH06FGWLFnCxIkTCQkJoWvXrkyaNIkvv/ySwsJCxo8fj7+/v6uuM8fMl99I/PLLLwwZMoS4uDhXMqImvvrqK95++2169uzJyJEjSUtL46effmLevHns3bu3QeYsGD9+PNu2bWPLli107dqVwYMHu/bV5ImZYRjMnz+fFStW4OXlxZAhQwgLC2PXrl0sX76cL774gsWLF3PBBRdUevxHH33EP//5T/r27cuoUaNITk5m27ZtzJs3j+zsbGbMmFHja7HZbNx2222sW7cOHx8fhg0bRmBgIFu3buXNN9/ks88+Y8mSJfTr1w+APn36MGnSJH766SeOHTtGYmKi64a/oZ4WFhQUAFTo5g1lQwtee+01zGYz/fv3Z/DgwaSmpvL111/z7bff8thjjzFlyhQAevXqRUJCAlu3bmX16tVcfvnlFerbt28fu3btIjw8nAsvvLBG8dUmBpPJxPDhw1m1ahXr169nwoQJrnrWrVvn+n79+vVuY+aPHz9OUlIS0dHRjfq0uTbXcqbFixfzj3/8g4SEBEaPHs2hQ4fYunUrt912G88//3yFyQAPHjzITTfdxKlTp4iMjGTs2LEUFRWxdOlSNmzYgNPpdCvv7+9fZVuDsrZYn5jS09OZMmUKaWlpdO7cmVGjRuHj40NaWhp79+5l165dzJw5s8avZfmN9XnnnVfp/mHDhrF8+fIKN6qpqakcP36c8ePHYzKZGDp0KJ999hkpKSluQ2gqm/+g/P2+Z88eAgMDGT58ON7e3mzatImXXnqJzz77jDfeeIPo6OgK8RiGwZ133skPP/zAkCFD6NWrFwcOHHDtf/31111zkQwYMICuXbty5MgR5syZw80331zj16Xct99+i8PhIDg4uE6TL9b3c7Ou1q5dy9KlS+natSvnn38+6enp/PTTTzzxxBOkpqbyxz/+0VV20qRJ7Nmzh7179xIXF+fWRk//fwIgNDSUuLg4du7cyY4dO0hISGjQuEWkGTBERFqYiy66yIiJiTE++uijasvOmDHDiImJMf7v//7PbftNN91kxMTEGBs2bHBt27RpkxETE2NcffXVRl5eXoW6tm/fbmRlZVUay/Hjxys9/4YNG4yYmBgjJibGmDhxopGWlnbWazqznvI4Y2JijJdeeslt38aNG40BAwYYMTExxvfff1/t9Z1u8eLFRkxMjLF48WK37R999JERExNjzJs3r9LjDMMw5s2bV+nr/8477xgxMTHGsGHDjN27d7u2O51O1/mGDBliZGZmVnrt/fr1M7755ptK4xk8eLBRVFRUZUxneuqpp4yYmBhj3Lhxbq+pzWYz/vCHPxgxMTHGmDFjjJKSkhpdW02crV0ePHjQ6NOnjxETE2Ns377dbd97771nxMTEGBdffLGxZ88et32bNm0yEhISjH79+hmHDx92bX///feNmJgY45Zbbqk0lr/97W9GTEyM8fjjj7ttr6pd1CWGf/3rX0ZMTIzx0EMPuZWfNm2a0a9fP+PSSy81hgwZYpSWllZ7zNmUt52bbrqpRuXrci2GYbjeZ0OGDDG2bdtWaQyXXHJJhfNNmjTJiImJMe655x639nTixAlj/PjxrnrPfM1r0tbqEtPzzz9vxMTEGA8//LDhdDrd9tlsNmPdunVVnq8yo0ePNmJiYowDBw5Uuj8lJcUV5+mfbx9//LERExNjvPnmm4ZhGMa7775rxMTEGB9//LGrTFpamuvYlJQU1/a5c+caMTExxrXXXuv2mZufn2/ceuutRkxMjHHddde5xXH8+HFXXRdccIFx6NChCrHu2bPH6NOnjxEXF2d88cUXbvs++eQTIzY21oiJiTEuuuiiGr8+999/vxETE2NMnz69xsecrq6fm+XXWpWq3uun/5/y7rvvuu1bt26dERsba/Tp08dITU1121fV/xmVeeyxx4yYmBjjxRdfrLasiLQ8GsIgIq1a+/btAWr0tD8jIwMoe6JSPl79dPHx8a766mLBggV1nlSqb9++3HbbbW7bhg4dyg033ADA0qVL6xxXQylf3m3OnDluT6hMJhN33nknsbGx5Obm8v7771d6/E033cRFF13ktm3y5Mn07NmTvLy8CsNQqlJSUuLqOv/ggw+6PaX09vbmoYceIjw8nKSkJL788staXWNt5eXlsXbtWu68804cDgezZ88mPj7etd/pdLq6rD/zzDPExcW5HX/uuedyxx13YLfb3cYUX3bZZfj5+bFu3boKwzHsdjuffvopQIVJRitT1xhGjBgBuPc4KC4uZuvWrSQkJHDRRReRm5vr9nsrL1vV0+z6quu1nO6uu+5i4MCBbttuu+02goKCOHLkCKmpqa7tmzdvZteuXfj7+7NgwQKsVqtrX4cOHRpsNZPaxFQ+RGbUqFGYTCa3Y7y9vWv12mdlZZGamorZbKZHjx6VlunUqRNdu3YF3Oc4KP9+6NChQNlrf2aZ8uELXbt2pVOnTkDZPCSrVq3CZDLx6KOPun3mBgQE8Je//AUfHx+2bt3Kli1bKo3pnnvuqTTet956y7Xc4KWXXuq278orr6xTD4LyZRkr61lUE/X93KyrSy65hOuvv95t23nnncfIkSNxOBxs2LChznWfc845AOzevbteMYpI86QEgoi0auXdh8/8Q7oy/fr1w2Kx8NFHH/H222+TlpbWYHGEhYVVOrFWTZ05c3m5q6++GoCffvoJh8NR5/rr68SJE67l2iZNmlRhv8lkct3MVjX+/szkQblevXoBVLhRrsqOHTsoLCwkJCSk0hsCPz8/V5f7qmKpj9PHtA8ZMoSZM2dy9OhRnnrqKebOnetWdvfu3aSlpdG1a9cqh9iU34CdPnY/MDCQ8ePH43Q6K8wRsWbNGrKyshgwYIDrD/mzqWsMXbp0ITo6mqSkJNfvfvPmzdhsNkaMGFEhwWAYBhs2bMBkMjVaAqGu13K6ytqh1Wp1Dbk4vR2W3wCPGjWq0rHvF154IcHBwbW6hvrGVL7Cx9NPP81//vMf19CZuihPRgQFBWGxWKosV9k8CJs2baJ9+/auNtirVy/Cw8MrTTKcPnzhv//9L06nk759+1ZIAEFZYmbkyJFux59p/PjxlW4vj+/MySDLVfbZ1Zga4nOzrqr7vK3P/3/l74XypLyItC6aA0FEWrXyCd7atWtXbdmuXbvy4IMP8uSTT/Loo4/y6KOPEhUVxaBBg7jwwgu59NJL3Z4w1kb5TOV1VdlY39O3FxcXk52dXeenYPVVfgMTEhJSae8NwPWUsqpEQFVLC5bXd7YJA09X/ofv2V7z6mKpj9PHtGdlZbF582YKCgr485//TPfu3d2WcCyfSPDYsWNnnRCtvK7TTZkyhRUrVrB8+XK33ikfffQRULPeB/WNYcSIEbz//vusW7eOrl27upIF559/PjExMVitVtatW8fs2bPZvXs32dnZ9O3bt149eRrrWsrVph2eOHECOHtb69y5M7m5uWeNpTq1iemqq67ixx9/5N///je///3vsVgs9OrVi8GDBzN+/PhaJW/y8vLczlOVYcOG8cEHH7huclNSUkhKSuKSSy5xS94OGTKEVatWkZycTFRUVKUTKJa/J6v6zIOzv3/DwsKqXGWk/PdV3edpbYSGhgJVT456Ng3xuVlX5T0+zlTbz9uz1VHfdi8izZMSCCLSahmGwZ49ewCIiYmp0THTpk3jsssu45tvvuGnn37ip59+4vPPP+fzzz/n+eef5+233yYyMrLWsfj6+tb6mNoyzpg5+2zOnNitOahJL5GW4Nprr3W7ec/Ly2POnDls3LiRuXPn8vnnn7tucMp/ZxEREa6nqlU586b73HPPdU0At2XLFhITE8nMzOT777/Hx8en0skVK1OfGM477zxXAuH6669n/fr1tGvXjv79+2M2m0lISGDLli0UFRU1+vAFqN+1lDOba98582xttyHadW1iMpvNPP3009x+++189913bNmyhS1btvDuu+/y7rvvctFFF/Hiiy+etUdBufLeE/n5+WctV54AOHLkCCdPnnQlBsp7e5QbOnQoq1atYuPGjYwcOdK1wsCZKzDUR1N81p6uX79+fPLJJ+zevRuHw1Gj17UpVPcZX5d2XlPliaeG6H0jIs2PEggi0mqtWbPGtdxXdTcTpwsPD2fq1KlMnToVKFs2749//CNbt27l73//O0888USjxHs2VS3vWL6MpI+Pj1sXam9vb4Aquy+npKQ0aHwdOnQAyuaayM/Pr/RpWvnT4fKyjaU8wVP+2lSmqWKBsu7fzz33HJdddhnJycksXbqUO+64A4COHTsCZU8gH3/88VrVazKZmDRpEosWLWL58uUkJiby6aefUlpayqWXXlrjP97rE8N5552HyWRi48aNZGZmsmfPHi6++GLXzcmIESPYuHEj//3vf1m/fr1rW2Opz7XURXn7OVtba+j3Wk317t2b3r17A/8bPnLffffx7bffsmLFiipXoThd+dP13Nzcs94cR0ZG0qNHDw4fPsyGDRtcPRHK5z0oV/7zpk2bXL25evTo4ZaULX9Nz7bMZ13fvx06dODYsWMkJydXOrznbL/Hqlx00UU8/vjj5Obm8s0331RYpaO6eKBun5ve3t7Y7fYqj/NUu4P/zTl05mpEItI6aA4EEWmV8vLyXEt1nX/++ZUujVZTvXr14tZbbwVw9WgoV36j3tjzD5RPinem8vHvgwcPxsvrfznh8j82f/nllwrHFBUVVTmetvx6art+d8eOHV1dbZcvX15hv2EYrnXvG/JpY2Xi4+Px9/cnOzubr7/+usL+4uJiVq5c2SSxlAsNDWX27NlA2aRp5V17yyfmPHjwoNtSczU1efJkzGYzX3zxBUVFRa7XviY3h+XqE0P79u3p06cP2dnZvPrqqxiG4ZYgKP/+u+++46effsJqtdZrLpDq1Pf1rK3yG+IffvjBlaw83elJzDM11WcH4Jp34oorrgAqfo5VJTQ0lE6dOmEYBocOHTpr2dPnQdi0aRMhISEVhpGcc845hISEsGnTpkrnP4Cy19RsNruWDTxTWloaP/zwQ6XHVqf89/Xvf/+70v1nzidSE127dnX19nn88cernbA3MzPT9VrW53OzPOlS2e9l7969bhNrNoTa/N9Q/t4rXyZXRFoXJRBEpFUxDIM1a9ZwzTXXcOTIESIiInjsscdqdOz69etZs2YNdru9Qp3fffcdUHEscvmNemPfrOzatYtXXnnFbdvmzZt55513AJgxY4bbvvJu4u+8847b2NnCwkIefvjhKv+4LH+CW1nioTq33HILAP/4xz/c/vA3DIN//OMf7Nmzh+DgYFfPjsbi4+PDjTfeCMATTzzh9lTRbrfz17/+lfT0dKKjo6ucbK0x3HDDDXTu3Jm8vDzXzOve3t7ceeedrrXrN2/eXOE4h8PB+vXr2bZtW4V9HTt2ZMSIEeTn5/PMM8+wf/9+OnfuzPDhw2scV31jKG9r5StfnH/++a59/fv3Jzg4mA8//JDi4mISEhIatYt5fa+lts4991zi4uIoKCjgsccew2azufadPHnyrL2VGuuzY8WKFZWuWJKfn+8aWlCbOVnKb1yre73Ky61evZqkpCTOPffcCsM3TCYT5557LsnJyfznP/9xO65c586dufTSSzEMgwULFrjmsYGyz68FCxZQUlJCQkICiYmJNb4OKBuiZrFY+OKLL/jqq6/c9n3++eesXr26VvWVe/jhh+nWrRtJSUnccMMNlbY7m83Ghx9+yNVXX+1201/Xz83y5NwLL7zg1u6SkpKYP39+rYa01UT5/w0HDx6stmz5BKW1+RwSkZZDQxhEpMX64IMPXH8Q22w2Tp065ZqoDcrG2/7tb3+r8R/L+/btY+HChQQGBtK3b18iIyMpKSlh9+7dJCcnExQUxN133+12zPjx49m4cSP3338/I0eOdHUbnzlzJj179mywa502bRrPPPMMn3zyCbGxsaSlpbF582acTifTp09n9OjRbuUvu+wy3njjDXbu3Mnll1/O4MGDcTqd7Ny5E29vb6ZMmeKabO90AwcOJDIykt27dzNp0iRiYmLw8vKiR48erl4YVbn++uvZunUrn3zyCVOmTOHcc88lLCyMXbt2cfjwYXx9fXn66add3aIb01133cXOnTtZv349EyZMYNiwYQQEBLBt2zZSUlIICQlh0aJFdZ4Usy6sVit33nknf/jDH1i2bBkzZswgJCSEm266iZSUFJYsWcKNN97IOeecQ9euXfH19SU9PZ29e/eSm5vLn//8ZwYNGlSh3ilTprB27VqWLVsGlM3mXtvxzfWJYcSIESxZsoSSkhKio6NdT1ShbJz1sGHDXDdr9Rm+sGvXrrMmn0aPHs2cOXPq/XrWhslk4qmnnmLatGn8+9//ZtOmTSQmJlJcXMzGjRuJi4sjISGBrVu3up7glhs3bhwvvvgib775JgcOHKBjx46YzWbGjBnD2LFj6xzTf/7zH+bNm0dkZCR9+vQhODiY3NxctmzZQl5eHjExMVx77bU1rm/cuHGsWLGCH3/88azHlScCyj9/zxy+UO7cc8/lq6++cpWrrBfBggULOHToED///DMXX3wxw4YNw2Kx8N///pesrCyio6N5+umna3wN5fr06cO9997LU089xZ133snAgQPp0qULR48eZceOHcyYMYPXX3+91vW2a9eOd999l7lz57Jp0yZuvPFGoqOjiY2Nxc/Pj4yMDLZv305hYSGBgYFuQzbq+rl522238eWXX7JmzRrGjx9PfHw8WVlZ7Nixg8TERFe7aygjR47E39+f1atX85vf/Ibu3btjNptJTEx06/GUlZXFvn37iIyMdFuyVkRaDyUQRKTFKp8cDMDf35/AwEBiYmLo378/l112mdts9zUxZswY8vPz2bx5M0ePHuXnn3/G19eXjh078rvf/Y4bb7zR9RSm3G9+8xsKCgr49NNPWbNmjWvm6iuvvLJBEwgXX3wxY8eO5f/9v//n6iXRt29fbrrppkqX//L29mbp0qUsWrSI1atX8+OPPxIaGsrFF1/M3Xff7eq5cCar1cqSJUt49tln2bZtG3v37sXpdDJ06NBqEwgmk4knn3ySCy64gPfee49du3ZRVFREeHg4kydPZtasWQ36mpyN1Wrl1Vdf5f333+eTTz5xLS/YqVMnpk2bxqxZs5pk/oMzXX311bz22mscPHiQJUuWcN999wHwwAMPMG7cON555x22bNnCDz/8gLe3NxEREQwdOpQLL7yQSy65pNI6x40bR0hICNnZ2a55EeqirjEMGTIEq9XqWr7xTOedd16DJBAKCgr4+eefq9x/etuqz+tZWzExMXz00UcsXryYtWvXsnr1ajp16sT06dOZPXu2a9jAmZM2xsXF8fzzz7NkyRJ+/vln1q9fj2EYdOzYsV4JhFtuuYXo6Gi2bt3qSqiGhITQu3dvrrjiCiZPnoy/v3+N6xszZgydO3fmm2++IScnp8oVbcLCwjjnnHNcPSqqGl5w+sSK55xzTqUrx7Rv355//etfvPnmm6xcuZIff/wRp9NJdHQ0U6dO5ZZbbqnRyjqVufXWW+nRowdLlixhz549HDhwgNjYWBYvXky/fv3qlECAsut/8803+f777/n888/ZunUr69evx263ExISQkJCAqNHj+aqq65ym6+mrp+bXbp04V//+hfPPfccGzdu5NtvvyUqKorbb7+dW2+91dWzoaGEh4fzyiuv8OKLL7Jr1y62bduG0+nE4XC4JRDK52G57rrr3IbViUjrYTIauo+TiIiIiHD8+HEuueQSAgIC2LRpU6POfN+YlixZwpNPPslDDz3EtGnTPB2ONFOGYXDllVdy7NgxVq9eTUREhKdDEpFG0DL/JxMRERFpBgoLCyudxyA5OZn7778fp9PJ1Vdf3WKTB1A2hKpLly68+uqrrl5WImdatWoV+/fvZ9asWUoeiLRi6oEgIiIiUkdJSUmMHTuWrl270r17dwIDA0lNTWXXrl3YbDbi4uJ4++23K11qryVZvXo1c+bM4YEHHmDmzJmeDkeaGYfDweWXX05JSQlffPFFo06WKiKepQSCiIiISB0VFBTwwgsvsHHjRlJSUsjLy8PX15cePXpwySWXMG3aNPz8/DwdpoiISINQAkFEREREREREqtVyB+SJiIiIiIiISJNRAkFEREREREREqqUFWhuI0+mktLQUs9mMyWTydDgiIiIiIiLSyhmGgdPpxMvLq0lW/FECoYGUlpayY8cOT4chIiIiIiIibUx8fDxWq7XRz6MEQgMpz/b07du3SX5xIp7gcDjYsWMH8fHxWCwWT4cj0ijUzpu5uDhITYVOnWDvXk9H02KpnUtboHYubYHNZmP37t1N0vsAlEBoMOXDFiwWiz6gpNVTO5e2QO28mSoqgoKCsq/6/dSb2rm0BWrn0pqVt+2mGkavSRRFREREREREpFpKIIiIiIiIiIhItZRAEBEREREREZFqaQ4EEREREZE2yDAMSktLcTgcng6lUZRfV3FxseZAkBbLYrHg5eXVZHMcVEcJBBERERGRNsZms5GamkphYaGnQ2k0hmHg5eXF0aNHm83Nl0hd+Pv706lTp2ax2p8SCCIiIiIibYjT6eTw4cNYLBY6d+6M1WptlTfYhmFQVFSEn59fq7w+af0Mw8Bms5Gens7hw4c555xzmmy5xqoogSAiIiItx733Qm4uBAd7OhKRFstms+F0OunSpQv+/v6eDqfRGIaB0+nE19dXCQRpsfz8/PD29ubo0aPYbDZ8fX09Go8SCCIiItJy3HuvpyMQaTU8/SRTRGqmOb1Xm08kIiIiIiIiItJsKYEgIiIiIiIiItVSAkFERERajry8sjkQ8vI8HYmISJ1t3LiR2NhYcnNz22Qcy5cvZ8iQIfWqIykpidjYWPbs2VNlmZpe3/r167nsssuafEnTrKwszjvvPE6cONGk560PzYEgIiIiLUefPpCcDFFRkJTk6WhEpInExsaedf+dd97J73//+yaKpnamTZtGXFwcf/zjH13bEhISWLt2LUFBQY123pq8ZkOHDm2087ckTz31FLNnz8ZisQBliYfp06dXKLd27VoiIiIA+O9//8uSJUvYuXMn6enpvPjii4wbN85V1m6389xzz/H9999z/PhxAgMDGTFiBPfddx8dOnQAIDQ0lKuvvprFixfzt7/9rQmutP6UQBARERERkWZt7dq1ru9XrlzJ4sWLWbVqlWvb6atJGIaBw+Fw3Qw2R1ar1XUj2lhq8prt3Lmz1vXabDasVmuDxNgcbN68mWPHjjF+/PgK+1atWkVgYKDr57CwMNf3hYWFxMbGMmXKFO68884KxxYXF7N7925mz55NXFwcubm5/PWvf2X27NksX77cVW7y5MlMnjyZBx54gJCQkIa9uEagIQwiIiIiItKsRUREuP4FBQVhMplcPx86dIjExETWrFnD5MmTiY+P56effmL+/Pnce8bKLX/961+ZNm2a6+dp06bxl7/8hSeffJKhQ4dy/vnn8/zzz7sdk5uby4IFCxgxYgTx8fFcccUVfPvttwCcOnWKe++9l1GjRjFw4EAmTpzIZ5995jp2/vz5bNq0iWXLlhEbG0tsbCxJSUmVdq3/8ssvufzyy+nfvz9jxozhtddec4tjzJgxvPTSSzz44IMkJCRw4YUX8t5779XpNYuIiCAgIMBVdteuXUyePJmBAwdy/fXXc+jQIde+559/nquuuooPPviAMWPGMGDAANfr8sc//pHhw4eTmJjI9OnT2bt3r+u4vXv3Mm3aNBISEkhMTGTy5Mns2LHDLcYffviByy67jISEBGbOnElaWpprn9Pp5IUXXuCCCy6gf//+XHXVVXz//fdVXi/AmjVrGD9+PAMGDGDatGkkJyeftTyUJVdGjBiBj49PhX1hYWFur9npqyGMHj2ae+65h4svvrjSeoOCgli6dCkTJkygZ8+eDBo0iIcffphdu3aRkpLiKnfOOecQGRnJV199VW2szYESCCIiIiIiAs88A9HR1f+78sqKx155Zc2OfeaZRgv/73//O/fddx8rV66stvv+6T7++GP8/f15//33uf/++3nxxRf58ccfgbKb2FmzZrFlyxaeeuopVq5cyX333ee6kbTZbPTr14+XX36Zzz77jKlTp/LAAw+wfft2AP74xz+SkJDA1KlTWbt2LWvXrqVTp04VYti5cydz585lwoQJ/Pvf/+bOO+9k0aJFbk+qAZYuXUr//v1ZsWIFN9xwA3/+85/dbvbr6tlnn2X+/Pl89NFHWCwW/vCHP7jtP3bsGF9++SUvvPACK1asAODuu+8mMzOTV155heXLl9OvXz9++9vfkp2dDcD//d//0bFjRz788EOWL1/OrFmz8Pb2dtVZXFzMa6+9xpNPPslbb71FamoqTzzxhGv/smXLWLp0KfPmzePTTz9l5MiR3HHHHRw5cqTSa0hNTeXOO+/koosuYsWKFVx77bX8/e9/r/baN2/eTP/+/Svdd/XVVzNy5Ehuvvlmfvrpp2rrqk5+fj4mk4ng4GC37QMGDGiQ+puChjCIiIiIiEjZBKU1eGJLly4Vt6Wn1+zYRpys76677uL888+v9XGxsbGuLujdu3fnrbfeYv369Zx//vmsW7eO7du3s3LlSnr06AFAl9Ouv0OHDsycOdP187Rp01i7di1ffPEFAwYMICgoCG9vb3x9fc86ZGHp0qWcd955zJkzB4AePXpw8OBBlixZwuTJk13lLrjgAm688UYAZs2axeuvv87GjRvp2bNnra/7dPfcc49rPoTf/e53/O53v6OkpMT1VN5ut/Pkk08SGhoKlN10b9++nfXr17uGM8ybN4/Vq1fz5Zdfct1115GSksLMmTPp1auX67U9nd1u55FHHqFr164A3HjjjfzjH/9w7V+yZAmzZs3i8ssvB+D+++9n48aNvPHGG/zpT3+qcA3vvvsuXbt2Zf78+QD07NmT/fv388orr5z12lNSUoiMjHTbFhERwSOPPEL//v2x2Wx88MEHTJ8+nffff59+/fpV/4JWoqSkhKeffprLL7/cbVgEQGRkJLt3765TvU1NCQQREREREYHg4LIJSqtT2Y1wRETNjj3jyWtDio+Pr9NxZ/ZWiIiIIDMzE4A9e/bQsWNHV/LgTA6Hg5deeolVq1Zx8uRJ7HY7NpsNX1/fWsVw6NAhxo4d67YtMTGRZcuWuc3ncHqsJpOJ8PBwV6z1cXq95YmOzMxMOnfuDEDnzp1dyQOAffv2UVhYyLBhw9zqKS4u5tixYwDcfPPNPPTQQ3zyySeMGDGCSy+91JUsAPDz83P7OTIy0nUt+fn5pKWlkZiY6FZ/YmKi2zCJ0/3yyy+u4RXlBg0aVO21FxcXVxi+0LNnT7ekTGJiIsePH+f111/nqaeeqrbOM9ntdu6++24Mw+CRRx6psN/X15fi4uJa1+sJSiCIiIiIiAjce2/Zv7r49NOGjaUO/Pz83H42m80YhuG2rbS0tMJxXl7ut0Qmk8l1XHWJgCVLlrBs2TL+8Ic/EBsbi5+fH3/729+w2+11uYRqnS3WhqrXZDIBZcM3yp352hYUFBAREcGbb75Zoa7ylSV+//vfc8UVV7BmzRq+//57Fi9ezLPPPuuaM6CxrqW22rdvX6NlLOPj49myZUut67fb7cydO5eUlBTeeOONCr0PALKzs90SNM2Z5kAQEREREZFWJzQ0lIyMDLdte/bsqVUdsbGxnDhxgsOHD1e6f8uWLYwdO5arrrqKuLg4unTpUmGMvre3t9vNeGV69uxZ4eZ0y5YtdO/evVmuJtGvXz8yMjKwWCx069bN7d/pN8I9evRgxowZvPbaa1xyySV89NFHNao/MDCQyMjISl+T3r17V3pMr169KkzS+PPPP1d7rr59+3Lw4MFqy+3du7fWK2eUJw+OHj3K66+/Tvv27Sstd+DAAfr06VOruj1FCQQREREREWl1hg0bxu7du1mxYgVHjhxh8eLFHDhwoFZ1DB06lCFDhnDXXXfx448/cvz4cdcTdYBu3bqxbt06tmzZwi+//MKCBQsqJC2ioqL4+eefSUpKIisrq9Jkwi233ML69et58cUXOXz4MB9//DFvv/02t9xyS91fgEY0YsQIBg0axJw5c1i7di1JSUls2bKFZ599lh07dlBcXMyjjz7Kxo0bSU5O5qeffmLHjh2u+RBqYubMmbzyyiusXLmSQ4cO8fTTT7N3716mT59eafnrr7+eI0eO8MQTT3Do0CH+/e9/8/HHH1d7npEjR1aYwPD1119n9erVHD16lP379/PXv/6VDRs2uOafgLJeGHv27HElpZKSktizZ49rhQW73c5dd93Fzp07efrpp3E4HKSnp5Oeno7NZnPVU1RUxK5duxg5cmSNXxtP0hAGERERERFpdUaNGsWtt97K008/TUlJCVOmTOHqq69m//79tarn+eef54knnuDee++lqKiIbt26cd999wEwe/Zsjh8/zsyZM/Hz82Pq1KmMGzeOvLw81/G33HIL8+fP5/LLL6e4uJivv/66wjn69evHc889x+LFi/nnP/9JREQEd911l9sEis2JyWTi5Zdf5rnnnuPBBx/k1KlThIeHM2TIEMLDwzGbzWRnZzNv3jwyMjJo3749l1xyCXfddVeNzzF9+nTy8/N5/PHHycrKolevXvzjH/+oMBljuc6dO/P888+zcOFC3nrrLQYMGMA999xTYUWJM02cOJGnnnqKQ4cOueY9sNvtPPHEE5w8eRI/Pz9iYmJYunQpw4cPdx23c+dOt2TGwoULAZg0aRKPP/44J0+e5JtvvgHgqquucjvnsmXLXPNHfP3113Tq1IkhQ4bU+LXxJJPhiYEmrZDD4WDbtm3Ex8e7ZiIVaW3K2/mgQYOaZXc6kYagdt7M/fQT2GxgtcLgwZ6OpsVSO2/biouLOXz4MD169Kj1ZH8tiWEYFBYW4u/v7xrXL1KZJ554goKCAh599NEmP/fUqVOZNm0aEydOrLLM2d6zNpuNHTt2NNnnuXogiDSADUUbalx2uN/w6guJiEjllDQQEZEGNnv2bN555x2cTidmc9ON8s/KyuLiiy/miiuuaLJz1pcSCCIiIiIiItJmBQcHc/vttzf5eUNDQ5k1a1aTn7c+NImiiIiIiIiIiFRLPRBERESk5fjsMygqAj8/aEFdPkVERFoDJRBERESk5bj9dkhOhqgoSErydDQiIiJtioYwiIiIiIiIiEi1lEAQERERERERkWopgSAiIiIiIiIi1VICQURERERERESqpUkURURERESEH1ILmvR8ozoFNOn5KjNmzBimT5/OjBkzPB2KSIugBIKIiIiIiDRrsbGxZ91/55138vvf/77W9X744Yf4+fnVNSyRNkcJBBERERERadbWrl3r+n7lypUsXryYVatWubb5+/u7vjcMA4fDgcViqbbe0NDQhg1UpJXTHAgiIiIiItKsRUREuP4FBQVhMplcPx86dIjExETWrFnD5MmTiY+P56effuLYsWPcc889nH/++SQkJDBlyhTWrVvnVu+YMWN4/fXXXT/HxsbywQcfMGfOHAYOHMgll1zC119/3cRXK9J8KYEgIiIiLUdgIAQFlX0VETnN3//+d+677z5WrlxJbGwshYWFnH/++SxdupSPP/6YUaNGcfvtt5OSknLWel544QUuu+wyPv30Uy644AL+7//+j+zs7Ka5CJFmTgkEERERaTn27oXc3LKvIiKnueuuuzj//PPp2rUrISEhxMXFcc011xATE0P37t2ZO3cuXbt25ZtvvjlrPZMmTeKKK66gW7du3HvvvRQWFrJ9+/YmugqR5q3ZJRDefvttxowZQ3x8PNdee221b9YvvviCSy+9lPj4eCZOnMiaNWvc9huGwaJFixg5ciQDBgxgxowZHDlypEI93333Hddeey0DBgzg3HPP5Y477mjIyxIRERERkUYUHx/v9nNBQQHPPvssEyZMYMiQISQkJPDLL79U2wPh9Akb/f39CQwMJCsrq1FiFmlpmlUCYeXKlSxcuJA5c+bw8ccfExcXx8yZM8nMzKy0/JYtW7jvvvu45pprWLFiBWPHjmXOnDns37/fVeaVV17hzTff5M9//jPvv/8+fn5+zJw5k5KSEleZL7/8kgceeIDJkyfzySef8O6773LFFVc0+vWKiIiIiEjDOHM1hSeffJJvv/2We+65h7fffpsVK1YQExOD3W4/az3e3t5uP5tMJpxOZ4PHK9ISNasEwtKlS5k6dSpTpkyhd+/ePPLII/j6+vLRRx9VWn7ZsmWMGjWKW2+9lV69ejF37lz69u3LW2+9BZT1Pli2bBmzZ89m3LhxxMXF8eSTT5KWlsbq1asBKC0t5a9//Sv3338/v/nNb+jRowe9e/dmwoQJTXbdIiIiIiLSsLZs2cLEiRO5+OKLiY2NJTw8nOTkZE+HJdKiNZtlHG02G7t27eK2225zbTObzYwYMYKtW7dWesy2bduYMWOG27aRI0e6kgNJSUmkp6czYsQI1/6goCAGDhzI1q1bufzyy9m9ezcnT57EbDZz9dVXk5GRQVxcHA888AAxMTG1vg6Hw4HD4aj1cdKy1SYr3ZLbR3nsLfkaRKqjdt68mR54ALKzISQE48knPR1Oi6V23rY5HA4Mw3D9czGqPqYxuJ27DsdV9vX0Ort168Y333zDxRdfjNlsZtGiRa6/2aqqo7J6qtom0lTK219l95pN/TnebBIIp06dwuFwEBYW5rY9LCyMQ4cOVXpMRkYG4eHhFcpnZGQAkJ6e7tpWVZnjx48DZbOtzp8/n6ioKJYuXcq0adP48ssvCQkJqdV17N69u1blpXU40f5EjctuO7Wt8QJpIjt27PB0CCKNTu28eYp/6y2saWnYIiPZccMNng6nxVM7b7u8vLwoKipyewiS2K5pYygsLKzTcTabDcMwXMeXD00uKirCy+t/tzdz587lz3/+MzfccAMhISH89re/JTc3F7vd7jrWMAxsNptbLCUlJW4/V1ZGpCmVlJRgt9vZ2wwmEG42CQRPKf/QvP322xk/fjwACxcu5IILLmDVqlVcf/31taqvb9++WK3WBo9TmreS4pLqC/1qULdBjRdII3M4HOzYsYP4+HgsFounwxFpFGrnzZv517HJ3t7eDBo0yLPBtGBq521bcXExR48exc/PD19fX0+HU2vXX3+929/oF1xwQaU3Vr169eLll1/Gz88Pk8kEwM033+xW5ttvv3X7ubJ6Nm/e3BBhi9SZ2WzG29ub3r17V3jP2my2Jn2I3WwSCO3bt8disVSYMDEzM7NCL4Ny4eHhrp4ElZWPiIhwbYuMjHQrExcX51amV69erv1Wq5UuXbqQmppa6+uwWCz6j7gNMptrPp1Ia2gfaufSFqidN28mWsfnqaepnbdNFosFk8nk+tfatZXrlNarvA1X9pnd1J/hzWYSRavVSr9+/Vi/fr1rm9PpZP369SQkJFR6zKBBg9iwYYPbtnXr1rmeSERHRxMREeFWZ35+Pj///LOrzv79+2O1Wjl8+LCrjN1uJzk5mc6dOzfU5YmIiIiIiIi0aM2mBwKUdSmaN28e/fv3Z8CAAbzxxhsUFRUxefJkAB544AE6dOjAfffdB8D06dOZNm0ar732GqNHj2blypXs3LmTRx99FCjL1EyfPp1//vOfdOvWjejoaBYtWkRkZCTjxo0DIDAwkOuvv57nn3+eTp060blzZ5YsWQLApZde6oFXQURERERERKT5aVYJhAkTJpCVlcXixYtJT0+nT58+vPrqq64hCampqW5dxRMTE3n66ad57rnneOaZZ+jevTsvvvii2+oJs2bNoqioiAULFpCbm8vgwYN59dVX8fHxcZV54IEH8PLy4oEHHqC4uJiBAwfyxhtv0K5dE88kIyIiIiIiItJMmQytR9IgHA4H27ZtIz4+XpMotkEbijZUX+hXw/2GN2Ikjau8nQ8aNEhjZqXVUjtv5qKjITkZoqIgKcnT0bRYaudtW3FxMYcPH6ZHjx4tchLFmipfqcHf319zIEiLdrb3rM1mY8eOHU32ed5s5kAQERERERERkearWQ1hEGlpfkgtACDFy1aj8t0C1TtFRERERERaJiUQREREpOW4/HLIyoLQUE9HIiIi0uYogSAiIiItx//7f56OQEREpM1SAkFERERERHB892WTns9y4fgmPd+0adOIi4vjj3/8IwBjxoxh+vTpzJgxo8pjYmNjefHFF11LwNdVQ9Uj4mmaRFFERERERJq122+/nZkzZ1a6b/PmzcTGxrJ3795a1fnhhx9y3XXXNUR4Ls8//zxXXXVVhe1r167lggsuaNBziXiCEggiIiIiItKsXXPNNaxbt44TJ05U2PfRRx/Rv39/4uLialVnaGgofn5+DRXiWUVERGipd2kVlEAQERGRlmPIEIiOLvsqIm3GhRdeSGhoKMuXL3fbXlBQwKpVqxg3bhz33nsvo0aNYuDAgUycOJHPPvvsrHWOGTOG119/3fXzkSNHuPHGG4mPj2fChAn8+OOPFY556qmnGD9+PAMHDmTs2LE899xz2O12AJYvX84LL7zA3r17iY2NJTY21hVvbGwsq1evdtWzb98+pk+fzoABAxg2bBgPP/wwBQUFrv3z58/njjvuYMmSJYwcOZJhw4bxyCOPuM4l4imaA0FERERajhMnIDnZ01GISBPz8vLiqquu4uOPP2b27NmYTCYAVq1ahdPp5Morr2TVqlXMmjWLwMBAvvvuO+bNm8fSpUsZOnRotfU7nU5+//vfExYWxgcffEBeXh5/+9vfKpQLCAhg4cKFREZGsn//fh5++GECAgKYNWsWEyZM4MCBA/zwww8sXboUgKCgoAp1FBYWMnPmTBISEvjwww/JzMzkoYce4rHHHuPxxx93ldu4cSMRERG88cYbHDt2jHvuuYc+ffowderUur6MIvWmHggiIiIiItLsTZkyhWPHjrFp0ybXtuXLl3PJJZcQFRXFzJkz6dOnD126dGHatGmMHDmSr776qkZ1r1u3jkOHDvHEE08QFxfHueeeyz333FOh3B133EFiYiLR0dGMGTOGW265hS+++AIAX19f/P39sVgsREREEBERga+vb4U6PvvsM2w2G0888QQxMTGcd955LFiwgE8++YSMjAxXuXbt2rFgwQJ69erFRRddxOjRo1m/fn1tXzaRBqUeCCIiIiIi0uz16tWLhIQEPvroI4YNG8bRo0fZvHkzy5Ytw+Fw8NJLL7Fq1SpOnjyJ3W7HZrPVeN6BX375hY4dO9KhQwfXtoSEhArlVq5cybJlyzh+/DiFhYWUlpYSGBhYq+v45ZdfiI2Nxd/f37UtMTERp9PJ4cOHCQ8PB6B3795YLBZXmYiICPbv31+rc4k0NPVAEBERERGRFuGaa67hP//5D/n5+SxfvpyuXbsydOhQlixZwrJly7j11ltZtmwZK1as4Pzzz2/QOQO2bt3K//3f/zF69GheeuklPv74Y26//fZGm5fAy8v9Wa/JZMIwjEY5l0hNKYEgIiIiIiItwmWXXYbJZOKzzz5jxYoVTJkyBZPJxJYtWxg7dixXXXUVcXFxdOnShSNHjtS43l69enHixAnS0tJc27Zt2+ZWZuvWrXTu3JnZs2cTHx9P9+7dSUlJcSvj7e2N0+ms9lz79u2jsLDQtW3Lli2YzWZ69OhR45hFPEEJBBERERERaRECAgKYMGECzzzzDOnp6UyaNAmAbt26sW7dOrZs2cIvv/zCggULyMzMrHG9I0aMoHv37syfP5+9e/eyefNmnn32Wbcy3bp1IzU1lc8//5xjx46xbNkyt5UVAKKiokhKSmLPnj1kZWVhs9kqnGvixIlYrVbmz5/P/v372bBhA4899hhXXXWVa/iCSHOlORBERERERATLheM9HUKNXHPNNXz44YeMHj3aNWfB7NmzOX78ODNnzsTPz4+pU6cyduxYcnJyalSn2WzmhRde4I9//CPXXHMNUVFRPPTQQ9x6662uMmPHjuW3v/0tjz76KDabjQsvvJDZs2fzwgsvuMqMHz+er776iunTp5Obm8vChQuZPHmy27n8/PxYsmQJf/3rX7nmmmvw8/PjkksuYf78+Q3w6og0LpOhgTQNwuFwsG3bNuLj42s8WYu0fD+klq3Xm+L1U43Kdwu0MtxveGOG1KjK2/mgQYPcJvURaU3Uzpu56OiyZRyjoiApydPRtFhq521bcXExhw8fpkePHpWuEtBaGIZBYWEh/v7+rmUfRVqis71nbTYbO3bsaLLPcw1hEBEREREREZFqaQiDiIiItBxPPgmFhXDa8mciIiLSNJRAEBERkZbjhhs8HYGIiEibpSEMIiIiIiIiIlItJRBEREREREREpFoawiAiIiItx759UFoKXl4QG+vpaERERNoUJRBERESk5Rg7Vss4ioiIeIiGMIiIiIiIiIhItZRAEBEREREREZFqaQiDiIiIiIiwoWhDk55vuN/wJj2fiNSfEggiIiIiItKsxVYzaeqdd97J73//+zrX/eKLLzJu3Lg6HS/SliiBICIiIiIizdratWtd369cuZLFixezatUq1zZ/f39PhCXS5iiBICIiIiIizVpERITr+6CgIEwmk9u2Dz74gNdee42kpCSioqKYNm0aN9xwAwA2m40nnniC//znP+Tk5BAeHs7111/PbbfdxpgxYwCYM2cOAFFRUXzzzTdNeGUiLYsSCCIiIiIi0mJ9+umnLFq0iAULFtCnTx/27NnDww8/jJ+fH+PHj+fNN9/km2++4bnnnqNTp06kpqZy4sQJAD788EPOO+88Fi5cyKhRo7BYLB6+GpHmTQkEERERERFpsZ5//nnmz5/PJZdcAkCXLl04ePAg7733HuPHjyc1NZVu3boxePBgTCYTUVFRrmNDQ0MBCA4OduvRICKVUwJBRERERERapMLCQo4dO8Yf//hHHn74Ydf20tJSgoKCAJg0aRIzZ87k0ksvZdSoUVx44YWMHDnSUyGLtGhKIIiIiEjL8d//gsMB6mYsIpQlEAAee+wxBg4c6LbPbDYD0K9fP77++mu+//571q1bx9y5cxkxYgSLFy9u8nhFWjolEERERKTl6NTJ0xGISDMSHh5OZGQkx48f58orr3TbZxiGK8EQGBjIhAkTmDBhAuPHj+fWW28lOzubkJAQvL29cTgcnghfpMVRAkFERERERFqsu+66i7/85S8EBQUxatQobDYbO3fuJCcnh+uuu46lS5cSGRlJnz59MJvNrFq1ioiICIKDg4GylRfWr19PYmIiVquVdu3aefiKRJovJRBERERERIThfsM9HUKdXHvttfj6+rJkyRKefPJJ/P39iYmJYfr06QAEBATw6quvcvToUcxmM/Hx8bz88suuIQ7z5s3j8ccf54MPPqBDhw5axlHkLJRAEBERkZbj5ZchPx8CA+F3v/N0NCLiAZMnT2by5Mlu2yZOnMjEiRPdtpUPYZg6dSrXXXddlfWNGTOGMWPGNEqsIq2NEggiTWxD0YYalWupTwFERBrVo49CcjJERSmBICIi0sTMng5ARERERERERJo/JRBEREREREREpFpKIIiIiIiIiIhItZRAEBERERFpgwzD8HQIIlIDzem9qgSCiIiIiEgb4u3tDUBhYaGHIxGRmih/r5a/dz1JqzCIiIiIiLQhFouFkJAQ0tLSAPD398dkMnk4qoZnGAYlJSWYzeZWeX3S+pUvRZqWlkZISAgWi8XTISmBICIiIiLS1nTs2BHAlURojQzDwG634+3trQSCtGghISGu96ynKYEg0oSO5ttqXNaeXcCoTgGNGI2IiIi0VSaTiU6dOhEZGYndbvd0OI3C4XCwd+9eevfu3Sye3IrUhbe3d7Nqv0ogiIiISMsREwPt2kGHDp6ORKRVsFgszermpCE5HA4AfH19W+01ijQ1JRBERESk5fjmG09HICIi0mZpFQYRERERERERqZZ6IIg0ouCUw3U+NvpYDo4ga7XlLBeOr/M5REREREREako9EERERERERESkWuqBICIiIi3HjTdCRgaEh8Pbb3s6GhERkTZFCQQRERFpOdasgeRkiIrydCQiIiJtjoYwiIiIiIiIiEi1lEAQERERERERkWopgSAiIiIiIiIi1VICQURERERERESqpQSCiIiIiIiIiFRLCQQRERERERERqZYSCCIiIiIiIiJSLSUQRERERERERKRaXp4OQERERKTGZs2CnBxo187TkYiIiLQ5SiCIiIhIy/GnP3k6AhERkTZLQxhEREREREREpFpKIIiIiIiIiIhItZRAEBEREREREZFqaQ4EkWbsWJ6t2jJJqQUAjOoU0NjhiIh4XnQ0JCdDVBQkJXk6GhERkTZFPRBEREREREREpFpKIIiIiIiIiIhItZRAEBEREREREZFqKYEgIiIiIiIiItVqlgmEt99+mzFjxhAfH8+1117L9u3bz1r+iy++4NJLLyU+Pp6JEyeyZs0at/2GYbBo0SJGjhzJgAEDmDFjBkeOHHErM2bMGGJjY93+vfzyyw19aSIiIiIiIiItUrNLIKxcuZKFCxcyZ84cPv74Y+Li4pg5cyaZmZmVlt+yZQv33Xcf11xzDStWrGDs2LHMmTOH/fv3u8q88sorvPnmm/z5z3/m/fffx8/Pj5kzZ1JSUuJW11133cXatWtd/2666aZGvVYRERERERGRlqLZJRCWLl3K1KlTmTJlCr179+aRRx7B19eXjz76qNLyy5YtY9SoUdx666306tWLuXPn0rdvX9566y2grPfBsmXLmD17NuPGjSMuLo4nn3yStLQ0Vq9e7VZXQEAAERERrn/+/v6Nfr0iIiIiIiIiLYGXpwM4nc1mY9euXdx2222ubWazmREjRrB169ZKj9m2bRszZsxw2zZy5EhXciApKYn09HRGjBjh2h8UFMTAgQPZunUrl19+uWv7K6+8wj//+U86derEFVdcwYwZM/Dyqt1L5HA4cDgctTpGWi7Dafz6TVUF6lF3DY8tj6Ep2l35OdTGpTVTO2/ezICJso9Xp35HdaZ2Lm2B2rm0BU3dvptVAuHUqVM4HA7CwsLctoeFhXHo0KFKj8nIyCA8PLxC+YyMDADS09Nd26oqAzBt2jT69u1Lu3bt2Lp1K8888wzp6ek8+OCDtbqG3bt316q8tGwnTO0AyA/Jr3S/r91W57rzCyqvs0IMJ08AsO1ETp3PVVs7duxosnOJeIraefMUb7djBex2Ozu2bfN0OC2e2rm0BWrnIg2nWSUQPOnmm292fR8XF4e3tzd/+tOfuO+++7BarTWup2/fvrUqLy1bwYkiAAzvlEr3W3Pr3hYCAwJrVK5jh44ADOrYo87nqimHw8GOHTuIj4/HYrE0+vlEPEHtvJl75x0cJSVYfHwYNGiQp6NpsdTOpS1QO5e2wGazNelD7GaVQGjfvj0Wi6XChImZmZkVehmUCw8Pd+tJcGb5iIgI17bIyEi3MnFxcVXGMnDgQEpLS0lKSqJnz541vgaLxaIPqDbEZDb9+k1VBepRdw2PLY+hKdud2rm0BWrnzdTYsZ6OoFVRO5e2QO1cWrOmbtvNahJFq9VKv379WL9+vWub0+lk/fr1JCQkVHrMoEGD2LBhg9u2devWuZ5KREdHExER4VZnfn4+P//8c5V1AuzZswez2Vxh6IOIiIiIiIhIW9SseiBA2VCCefPm0b9/fwYMGMAbb7xBUVERkydPBuCBBx6gQ4cO3HfffQBMnz6dadOm8dprrzF69GhWrlzJzp07efTRRwEwmUxMnz6df/7zn3Tr1o3o6GgWLVpEZGQk48aNA2Dr1q38/PPPDB8+nICAALZu3crChQu58soradeunWdeCBEREREREZFmpNklECZMmEBWVhaLFy8mPT2dPn368Oqrr7qGJKSmpmI2/6/jRGJiIk8//TTPPfcczzzzDN27d+fFF18kJibGVWbWrFkUFRWxYMECcnNzGTx4MK+++io+Pj5AWc+HlStX8sILL2Cz2YiOjmbGjBlu8yKIiIhIM/Ddd1BSAj4+cOGFno5GRESkTTEZRk0Xi5OzcTgcbNu2jfj4eE2i2Ib8kFoAQIrXT5XuD045XOe6446F1qhc0pAxAIzqFFDnc9VUeTsfNGiQxhJKq6V23sxFR0NyMkRFQVKSp6NpsdTOpS1QO5e2wGazsWPHjiZr581qDgQRERERERERaZ6UQBARERERERGRaimBICIiIiIiIiLVUgJBRERERERERKqlBIKIiIiIiIiIVEsJBBERERERERGplhIIIiIiIiIiIlItJRBEREREREREpFpKIIiIiIiIiIhItbw8HYCIiIhIjSUleToCERGRNks9EERERERERESkWkogiIiIiIiIiEi1lEAQERERERERkWppDgSRKmwo2lBtmRQvWxNEIiIiLo88Ajk50K4d/OlPno5GRESkTVECQaSZ2ts1q0blcr1+onPp4EaORkSkmXjlFUhOhqgoJRBERESamIYwiIiIiIiIiEi1lEAQERERERERkWopgSAiIiIiIiIi1VICQURERERERESqpQSCiIiIiIiIiFRLCQQRERERERERqZYSCCIiIiIiIiJSLS9PByAiYGBQajEotpZSYnVQbHVg83YQmutLSL6Pp8MTERERERFRAkGkqZUnC0qspRT/miwosTpwWIwKZdNDiggs8sbLoc5CIiIAjB4NGRkQHu7pSERERNocJRBEmoCBQXagjUI/O8VVJAswwMduwcdmwddmITfARrGPg4x2xXTM8m/6oEVEmqO33/Z0BCIiIm2WEggiTaDIp5T00KL/bTDAajfja/PC11aWNPCxWTBjchXxsVk43jGf3AAb7fN88LFbPBC5iIiIiIhIGSUQRJpATqANgIAiL0JzfPGxWzAbprMe42fzIrDQm3x/OxkhRUSlBzZFqCIiIiIiIpVSAkHkDD+kFgCQ4mVrkPocJoN8PzsAoTm++Nlq/rYLz/Yl389OgV8phT52/Eu8GyQmERERERGR2lICQdo0x3dfVtgWnVeWOMjvmtUg58j3t2GYy4csVD8MocTw55QRxSmiwWYQkvclecFFpIcU0/WkFybO3nNBRKRVGzMGTp6EDh3gm288HY2IiEibogSCSCMrH74QnG+t8ubfYXiRTSdOGVEUEOa2z54djzlwEyU+DvL87QQXWhs9ZhGRZmv/fkhOhpwcT0ciIiLS5iiBINKIbF4Oin0cYFDhxt8wIJ8wsowu5NARg/LeCQZBpBNgyuKEEccpZ2865uwjp30OGSFFBBZ6u022KCIiIiIi0hSUQBBpRDkBv06eWOyFl8MMQIkRQJYRzSmisOPnKutDHqGmJNqTjLepBIBiI5hsOpObOxRL0GpKvQyyg0oIzfNt+osREREREZE2TQkEkUZiYJD7awIhKN9KptGFLKMLhbR3lbFgJ4QUQk3H8SMH0xkdCzqZ9pJjdKDQiKRDdgdyw0+Q1a6EdgVWLE5zU16OiIiIiIi0cUogiDSSQt9SHF4GZoeJ/IK+pBP76x4nQaQTakoimDTMJmeVdVhNRUQYh0jjHE7lD8Mv+DNsVgdZwSVEZPtVeZyIiIiIiEhDUwJBpJGUD18IKrCSQTcAIviFCNNh1xCFmog0/UKW0QUbgQSf6o6twy9kB5UQkmfF21H9qg4iIiIiIiINQX2gRRqBw+ykwN8OgHdBBKX4YsFGR9O+WiUPACwmB51MewHIKkrEt8iKYYKMkOIGj1tERERERKQqSiCINII8fzuGCaw2M3klPQEIIRWzyahTfe1Jxp9TOLFinOpXdo4AO8XepQ0Ws4iIiIiIyNkogSDSCMqHLwTn+5JLJwDam5LrXJ/JBJ1NuwHItvXBP98fgPT2xRjULSkhIiIiIiJSG5oDQaSBlXg7KPFxgAFGQRROvLBSiD+n6lVvgCmbECOZbKIoOnUupoA1FPmW4uvIbaDIRURagAULID8fAgM9HYmIiEibowSCSAMrX7oxoMibXEfZ5IkhpFRYorEuypZ17EihI5qw3FAK22WRZ07BoOqVHEREWpXf/c7TEYiIiLRZGsIg0oAMDFcCITDfj1wigPoNXzid1VRMJL8AkJdzHmaHiVJTMZmWAw1Sv4iIiIiISFWUQBBpQAV+pTgsBhaHCVtRN8CMHzn4mvIb7BwRpkN4U4TNGYI1JwqAVK+fsBv2BjuHiIiIiIjImZRAEGlArskTC6xkG9EAhDRQ74Nypy/rmJM7HLPhg91UyJbiLQ16HhGRZik1FZKSyr6KiIhIk9IcCCINpNTspMCvrBeAb34QhYQCBu1JafBzhZBCBt0ppD1Gbl9ot5VNxZuwGTa8Td5nPXa43/AGj0dEpMmcey4kJ0NUVFkiQURERJqMeiCINJC8ABuYwKfEQqG9OwCBZOJtKmnwc52+rGPhqT5YHAE4cXKi9ESDn0tERERERASUQBBpEAbG/4Yv5Fs5ZXQGGm7yxMoEmLJpTxJgwp7TF4BMZyaGYTTaOUVEREREpO1SAkGkAZR4O7BZnZgM8C4Mo4QgTDhoR+P2COho2ofJ5KQkrycmw0yJUUKBUdCo5xQRERERkbZJCQSRBpAb+OvSjYXe5Dq6ABBMGhZTaaOe12oqpl1QBhheOIvKJm3MdGQ26jlFRERERKRtUgJBpJ6cGOT6l02eGJRv5RSNP3zhdO0CM7FY7NjyegBlCQQNYxARERERkYamBIJIPRX42XFaDLxKTTiLO1CKLxZsBJHeJOc3mw1C253EWdQJw2HFjp1cZ26TnFtERERERNoOJRBE6ql8+EJwgZVso2wYQTtSMZucTRZDgF8uwX42HIVlwycynRrGICIiIiIiDUsJBJF6KLU4KfAtm+cgMN+XHDoC0N6U0qRxmEzQKyITR0F3ALIcWTiNpktgiIiIiIhI66cEgkg95PrbwAS+JRaKSzvhxBtvigggq8ljCfYrxsfRHqPUDwcOcpw5TR6DiIiIiIi0Xl6eDkCkpTIwyPl1+EK7fCtZRhQAISRjMjV9PCYTRAYXkFLQDa92e8l0ZNLe0r7pAxERaUxffw2lpeClP2FERESamv73FamjYqsDu7cTkxP8CvzJIxJoutUXKhMZnMex5LIEwinnKRyGA4vJ4rF4REQaXGyspyMQERFpszSEQaSOcgPKeh8EFnmTZ3TGwIwvufiZ8j0WU4CPHX9TAE57IE6cZDuzPRaLiIiIiIi0LuqBIFIHBgZ5AXagbPhCyq/DFzzR+yA45TAATmtZT4NIRynHC7phDtlFRt5R2mefcivvOFm7uREsF45vmEBFRERERKRFUwJBpA5s3k6cZgOTEyzFQRQQBhiE0LSrL1QmwnKCIwWD8A7ZRY6PnVKTEy9DnY1EpJV45x0oLAR/f7jhBk9HIyIi0qYogSBSB0U+ZUs3+tm8yKas90EAWVhNxZ4MCwBfUzFBpU5KbCGYrdmc8rURUeTr6bBERBrGAw9AcjJERSmBICIi0sT0WFKkDoqtZQkE3xIL2UZnwLOTJ54p0pKKo6A7AJm+JZ4NRkREREREWgUlEETqoMjHAYC5OIhigjHhIIRUD0f1P+GWEzgKugCQZy3FZnZ6OCIREREREWnplEAQqaVSsxO7d9kNeVFJdwCCScdiKvVgVO6sJjshzhIcxeFggiz1QhARERERkXpSAkGklop/7X3gbTeT4+wKQEgzGr5QLuK0YQxZvjbPBiMiIiIiIi2eEggitVQ+/4F3SQB2/DBjJ5g0D0dVUbj5JM6CaAzDRIG1lGKLw9MhiYiIiIhIC6YEgkgtla/AYC/uCEAIqZhNzW+OAS+TgzDycBZ3ADSMQURERERE6kcJBJFaMDAotpY9yS8o6QlAe1OKJ0M6qwi31Rg0jEFEREREROpOCQSRWijxdmCYweQ047CH4U0RAWR6OqwqhZrTobAThmGm2NtBoVfzmehRRERERERaFiUQRGqhfPlGSkIAEyGkYDJ5MqKzs5ichHEKZ2FnQJMpikgr0LEjREWVfRUREZEm5eXpAERakuJf5z+wFZfdkLdvhqsvnCnSkkpmQXcsAUlk+pZgYGCiGWc9RETOZvNmT0cgIiLSZqkHgkgtlPdAcJZE4kM+vuR5OKLqhZgzMRdFYDi9sHk5Oelf7OmQRERERESkBVICQaSGSs1OSr2cYICzJJwAspr18IVyZpNBuDkDR2E0APtDcj0ckYiIiIiItERKIIjUUPnyjdgDwfAmwHTKswHVQuRpqzEcCMnFieHZgEREREREpMVplgmEt99+mzFjxhAfH8+1117L9u3bz1r+iy++4NJLLyU+Pp6JEyeyZs0at/2GYbBo0SJGjhzJgAEDmDFjBkeOHKm0LpvNxlVXXUVsbCx79uxpqEuSVqD41+ELpSUdAAig5SQQgk2n8CoKwXD4UOTtIDmw0NMhiYjUzW23wbXXln0VERGRJtXsEggrV65k4cKFzJkzh48//pi4uDhmzpxJZmblS+Vt2bKF++67j2uuuYYVK1YwduxY5syZw/79+11lXnnlFd58803+/Oc/8/777+Pn58fMmTMpKSmpUN+TTz5JZGRko12ftFzlPRCcxZFYsGGlwMMR1ZzJBJGWNByFXQANYxCRFuzzz+HDD8u+ioiISJNqdgmEpUuXMnXqVKZMmULv3r155JFH8PX15aOPPqq0/LJlyxg1ahS33norvXr1Yu7cufTt25e33noLKOt9sGzZMmbPns24ceOIi4vjySefJC0tjdWrV7vVtWbNGn788UfmzZvX6NcpLYuBQYm1fALFcAI41ejzH5gdTkIyCuh8NAufwvovvxhhScWR3x2AX0LycJic9a5TRERERETajma1jKPNZmPXrl3cdlq3RLPZzIgRI9i6dWulx2zbto0ZM2a4bRs5cqQrOZCUlER6ejojRoxw7Q8KCmLgwIFs3bqVyy+/HICMjAwefvhhXnzxRXx9fet8DQ6HA4fDUefjpWk5nRVvoo3y6QFOmyag2OrAMAEOb4zSIPxJoaGnEbDYHbQ7VUj7jAJCsgoIPlWIxVl2EqcJTkaFcKxXOHkh/pUebxhnDyiAHHxs/hilfti8ijgSmE+PnMDqAzutPZe3bbVxac3Uzps3M2Ci7CPYqd9RnamdS1ugdi5tQVO372aVQDh16hQOh4OwsDC37WFhYRw6dKjSYzIyMggPD69QPiMjA4D09HTXtqrKGIbB/Pnzuf7664mPjycpKanO17B79+46HytNr8OJExW25ZvKEkg2+/+e+uf72wFwloQBJqyladiof68AS6mTXr9kEpFeQHBuMWd2aiixWij29aZdbjGdkrLplJRNZqgfh3uEkhYZyOndIDLs1Z8vwJxEdkE3vNvtZYdvGv778qs95uS2bRW27dixo/qTibRwaufNU7zdjhWw2+3sqOTzSWpH7VzaArVzkYbTrBIInvLmm29SUFDg1vOhrvr27YvVam2AqKQpOHPSKmyz5ZfNdWD1/l+CwP5rAsFR0gFw0s6rALOpfr9nL7uDQZuPEZL1vwkNC/2tZIcFuP4VBljBZCIou5Cuv2TQITmbsKwiwrKSKQiwcqxXBKld2uP0qtlopDDjJJkF8Xi320tqRweh6ZF4O89+bKdBg1zfOxwOduzYQXx8PBaLpU7XLdLcqZ03b2ZvbwC8vb0ZdNrnk9SO2rm0BWrn0hbYbLYmfYjdrBII7du3x2KxVJgwMTMzs0Ivg3Lh4eGungSVlY+IiHBtO31yxMzMTOLi4gDYsGED27ZtIz4+3q2eKVOmMHHiRJ544okaX4PFYtEHVEtirnjz7Hqof1p3gOLyCRRLIvAjF7O5fvMHeJeUkrjuEEE5xdi9LeyP70RWRCAlfpUnJfLa+7NrSFcO9utIl0OZRB3OJKDARp/tyfTcd5Jt53WvcmjD6XxNBQTnWym2B4F3HkdDConJDj7rMZW1Z7VzaQvUzps3E5V/PkntqJ1LW6B2Lq1ZU7ftZjWJotVqpV+/fqxfv961zel0sn79ehISEio9ZtCgQWzYsMFt27p161xPJaKjo4mIiHCrMz8/n59//tlV50MPPcQnn3zCihUrWLFiBS+//DIAzz77LPfcc09DXqK0QHaLk1IvA4yyIQz1Xb7Rp8jGkB9+ISinmBIfL34a2ZPUrqFVJg9OV+Jn5WC/Tqy9tA/74jtT5O+NT0kpCesO459XXKPzdy/MwlHQFYBf2uXV61pERERERKTtaFY9EABuvvlm5s2bR//+/RkwYABvvPEGRUVFTJ48GYAHHniADh06cN999wEwffp0pk2bxmuvvcbo0aNZuXIlO3fu5NFHHwXAZDIxffp0/vnPf9KtWzeio6NZtGgRkZGRjBs3DoDOnTu7xeDvX/Ykt2vXrnTs2LGpLl2aqfLeB9iCwfDC31T3BIJffgmJPx7Cr8hOsZ83W87vSWGgT63rcXhZON4rnJSu7Un88RDtsotIXHeYzaN6Uex/9kREl6IMthf2wDtkF0eD87GbnHgbzSqXKCIiIiIizVCzSyBMmDCBrKwsFi9eTHp6On369OHVV191DUlITU3FfFq388TERJ5++mmee+45nnnmGbp3786LL75ITEyMq8ysWbMoKipiwYIF5ObmMnjwYF599VV8fGp/4yZtT5G1LIFQWtIBoM49EAJyym7yfUpKKQiwsuX8npRUc7NfHYe3hW3n9WDw2l8IzCsh4cdD/DSqFzZf76rjcNgIzbOQbw+g1LuAY8EF9MoJqlccIiIiIiLS+jW7BALATTfdxE033VTpvjfffLPCtssuu4zLLrusyvpMJhN33303d999d43OHx0dzb59+2oWrLR6xT5lS6M4SyLwohhvajZU4HTBWYUkrD+Mt91BXrAvW0f0OOtNfm3YfbzYOqIHQ77/hYACGwnrDvPTyF6UWqseD9WtMJPthV0xt9vDwXZ5SiCISMvxm9/AqVPQvr2nIxEREWlzmmUCQaS5cJoMiq3/SyAEc+r0lRNrpH16PgM3HMHL4SQ71J9tw7tTam3Yt16JX1mPhiE//EJQbjGDNhxmy4ieVa7O0KUok20F3aHdHg4H51NqcuKlYQwi0hI89ZSnIxAREWmzdMcgchYlVkfZVN8OK0ZpAAG1nP/At8DmSh5kRgSydUSPBk8elCsK9GHriB7YvS2EZBUyYNNRTM7KV4vwdZbStcAbZ6k/pRYnx4MKGiUmERERERFpPZRAEDmL8vkPnMXhgAn/2sx/YBjEbU/Gy+HkVJg/Pw/vjsOrcZdZyW/nx7bzuuOwmAhPy6P/5uNgGJWW7WM/hbOwC6DVGEREREREpHpKIIicRdGv8x84SjpgwoEfuTU+NjIlh/CTeTjNJvYMisZpaZq3W05oAD8P647TZKJDSg5xPydXWu4cezbkRwPwS7sCHKbKEw0iIiIiIiKgBIJIlQwM1xKOzpJw/MjBbKp8SMCZLHYHsTtSADhyTgSFQb6NFmdlsiKD2DmkKwYQfSSLiNScCmV8cNK9wIrh8MVucZAUqGEMItICxMVBcHDZVxEREWlSSiCIVMFuceKwGGCYcNrCarV8Y+/dJ/ApLqUg0MqRmMhGjLJqaVHtOHJOBACx21Ow2B0VysTas3EUaBiDiLQg+fmQl1f2VURERJqUEggiVShfvtGwtQPDUuMJFIOzCok+nAnA3oFNN3ShModjO1Dob8W3yE6vPScr7O9lz8EoKB/GkI8TDWMQEREREZHKKYEgUoWiX4cvOIo7AOBPdrXHmJwGfbYlYQJSurTnVERgI0ZYPaeXmb2DogDociiDoFOFbvutOOma74vh8KHEy0FKYGFl1YiIiIiIiCiBIFKV8h4IzpIIrBTgbSqp9pguv2QQlFuMzdvCgf6dGjvEGsmKDCI1OgQT0GdbEhju8zjE2nNwFJb3QtAwBhERERERqZwSCCKVKDU7KfEuX4EhokbzH/gW2ui19wQAB/p3wu7j1agx1saB/p2we1sIzikmMM19VYbe9hyMX+dBONAuH0PDGEREREREpBJKIIhUIjuoBExAqQ84/PE3ZZ/9AMMg9udkLA6DU2EBpHZt3xRh1pjN15sD/ToCEJx6GIut2LXPFwdRef4YTm+KvUtJDSjyVJgiIiIiItKMKYEgUomsoLIbbEdJ2QoK1fVAiEzJJeJkHk6TiT2DosBkavQYayulWyinwvwxO52EHDvAsdwSjuXZOJZnIyI/yzWMYZtftmv7sTwbP6RqeUcREREREVECQaRSp4LL5jtwlERiphRfqp4bwGJ3ELOjbFjAkZgICoN8myTGWjOZ2DMoGsNkwi83E7/sDNeuqKIsnL8OY0gOL9QwBhERERERqUAJBJEzGBic+rUHgrM4HH+yMZmqvqHuteckvsWlFAZYORIT2VRh1klhkC95HboCEJJ0AJOjbKUJX2cp7XMCMJxelPiUkh1Y/YSRIiIiIiLStjSfWd5EmokCXzs2bycYJgxbe/w5VGVZa7GdqCOZAOwdGIXT0vxzcrkdu+J3Kg3vkiLapRwiu0sMAF0KcthZGIVX4FFSwgton99Me1KISNv20ktQVAR+fp6OREREpM1p/nc7Ik0s69fhC0ZJe8BCgKnq+Q+6HMrE4jTIDvUnKzKoiSKsJ7OF7K5lSYOA9BSsBTkARBdl4Sgs652QomEMItJcXXEFXHtt2VcRERFpUkogiJyhfPhCaUkHAPyrmEDRYncQfbis98HR3hFNE1wDKQlqT0FoB0xAyPEDYBj4O22EZAdhOC0U+drJDbB5OkwREREREWlGlEAQOUNW8K/zH5RE4EMeXqbSSst1PpqFt91BQaCV9E7BTRlig8iJ6oXTbMZamI9vTlkipEthDs6izgCkhGv1BRERERER+R8lEEROU2J2kOdvB8BZEl7l8o0mp0G3X8pWMTjaO6JZLttYHae3lfyIsqUbg1OPgGG4DWNIDi/QMAYRaX5++gnWry/7KiIiIk1KkyiKnOakfzGYwGTzB4cf/lXMf9AhORvfIjslPl6c6NK+iaNsOPkdoglMT8ZalI9vTgaERBCYE0JJuJlCP7srmSIi0mxcdRUkJ0NUFCQleToaERGRNkU9EEROcyKgCIDSkrI5DSrtgWAYdDuQDsDxnuEtYuWFqji9rORHRAEQnHoUDIMuBbk4izoBkBqmYQwiIiIiIlKm5d75iDSCE/5lCQSHLQILNnyoeAMdmpZPUG4xpRYzST1CmzrEBpffoQtOs8XVC6FLURaOgv8NYxAREREREQElEERcDAxXDwRnSQT+ZFc6tUG3g2W9D1K6h1JqbfmjgJxe3uRHlvdCOEKwvRD/3FAMw0R+gI1iU46HIxQRERERkeZACQSRX2X52LBZnOC0YNhCCKhk/oOg7ELC0vNxmuBor3APRNk48iLLeyEU4JedQZeCfJzFHQHINh/2cHQiIiIiItIcKIEg8qvy3gcUhwLmSuc/KJ/74GRUCCX+1iaMrnEZXt7kR/5vRYbowgzXMIZTFiUQRERERERECQQRl/IEgt0Wickw8CPbbb9vgY0OyWXd+Y+eE9HU4TW6vMhonGYL3sUFdEo/ik9uBIZhosicSY5DwxhERERERNq6eiUQbr31Vv79739TXFzcUPGIeMxJ//L5D8JpZy/AYnK47e92MB0TkBEZSH47Pw9E2LhO74XQLvUoXfPzcBZHAnDQftCToYmIiIiISDNQrwTC8ePHuf/++xkxYgTz5s1j3bp1GIbRULGJNBmb2UmWjw0Apy2McFu+237vklI6H8sCWmfvg3J5kdE4LWW9EPol78JRWDaM4YBNCQQRERERkbauXgmEL7/8kvfff5/Jkyfz448/MnPmTC644AKeeOIJ9uzZ01AxijS6DL9iMIGp1BccfoSX5Lntjz6cicVhkNvOj1PhgR6KsvEZXt7kRXYBIOr4Xqw54RgGnHScIM+ZV83RIiIiIiLSmtV7DoQBAwbw0EMP8f333/Pyyy8zfPhw3nvvPSZPnswVV1zBK6+8wokTJxoiVpFGc9KvbBhOaUnZygphtv/dLJtLnXQ5lAH82vugsrUdW5H8iCicFi+8iwsZemgPzpKyHhe/2H7xcGQiIsCePZCTU/ZVREREmlSDTaJoNpsZNWoUTz31FN999x3jx4/n4MGD/P3vf2fMmDHMmDGD7777rqFOJ9Kg0vzLEghOWxi+DhsBjhLXvk7HT2G1OSjy9yatcztPhdhkynohlM2F0O/wFpwFZd8fsB3wZFgiImWCgiA4uOyriIiINCmvhqxs8+bNfPrpp3z55Zfk5ORwzjnncPXVV+Pl5cVHH33E7Nmzuf3227n77rsb8rQi9ZbmmkAxjPCSPE7vYxB1JBOAYz3DMcytu/dBufzIaILSkvAryqfX4RKOhUGKI4V8Z371B4uIiIiISKtU7wTCwYMH+fTTT/nss89ITU0lLCyMSZMmcdVVV9GnTx9Xud/+9rc8/PDDvPPOO0ogSLNSbHGQ42MHwGkLJcyW6toXmF1EcE4xTrOJE13beyrEJmdYvMiLjKZd6hFG7N7G4f6dsfhmcMB+ABNtI4kiIiIiIiLu6pVAuOqqq9i/fz9Wq5WxY8fypz/9iVGjRmE2Vz4yYtiwYXzwwQf1OaVIg0v/df4D7AHg9CH8tPkPyldeSO8YjN3aoB12mr2CiCiCThwjNPsEwSf7UNAtg332fcQR5+nQRKQte+YZyM0tG8Zw772ejkZERKRNqdcdUXBwMI8++iiXXXYZgYHVz0w/duxYvv766/qcUqTBlc9/UGoLw2I4aW8rAMDkcNIxKRuAlG5tp/dBOaeXN4VhHQnMSGH41uOs7moiw5lBoXehp0MTkbbsmWcgORmiopRAEBERaWL1mkTxiSeeYOLEiVUmD4qLi0lJSXH97OfnR1RUVH1OKdLg0n7tgWCUhNHBUYgFA4CIE7lYbQ6Kfb3JjGybk3Xl/zqZYuyxg1jyylaoSA9M92RIIiIiIiLiIfVKIIwdO5avvvqqyv3ffPMNY8eOrc8pRBqdawJFWxidHQWu7Z2PnQIgtWtIq1+6sSqlvv5kdY0FoPf+stcpLTANwzA8GZaIiIiIiHhAvYYwVHcTYbfbq5wPQaQ5KPQqJc9aCoCzJJTOpccAMNtKCDtZNhdCStdQj8XnaXu7ZmHtEs4Fx/Zx/k8H2JfYhRLvEr4u+pp2Xu5LWg73G+6hKEVEREREpCnUOoGQn59Pbm6u6+fs7Gy3YQrlcnNzWblyJREREfWLUKQRlU+g6LQFg+FNlKOALCAg6wQm4FSYP0WBPh6NsaHt7ZpVq/IZncMojAzBPy2b9id8ye6cT5Yzi3a0q/5gERERERFpNWqdQHj99dd58cUXATCZTPztb3/jb3/7W6VlDcNg7ty59QpQpDGd9C9PIITSzllCoGEnyzDwzzwBtO3eBy4mEyfPjaHH55sYvDWFrzsHk+XMopvRDbNJPYxERERERNqKWicQzj//fPz9/TEMg6eeeorLL7+cfv36uZUxmUz4+fnRr18/4uPjGyxYkYZWPoGisySMzqVl8x9YC3LxLimi1GImLUpP2QFOxXUhas0OzjmUzbe2UEqtpeQ4c2hvaXurU4iIiIiItFW1TiAkJCSQkJAAQFFRERdffDGxsbENHphIYzMMwzWBomELo7MjH4CAzFQA0qLa4fCyeCy+5sSwmEkb3JvoNTvoebCYg329yHRkKoEgIiIiItKG1Kv/8Z133qnkgbRYBUYBhd4ODMOE09aezqUFGI5S/E6VLVOo4QvuMgb2pNTLi4E7MgA45TyFw3B4OCoREREREWkqteqB8MILL2AymZg9ezZms5kXXnih2mNMJhNz5sypc4AijeVk6UkADHs7vJxmIpyFkH4Ss9OB3ceP7DB/D0fYvDh8raT3707HbQfxz4PCICennKcIt4R7OjQRaUsSE6FLF9AkzSIiIk2uTgmEWbNmYbValUCQFu2koyyB4CwJo4OjAAvgPFG2okhhWEcwmTwYXfOUPrgXHbcdpO/uHDYPa0eGI0MJBBFpWp9+6ukIRERE2qxaJRD27t171p9FWpK00jSgbAWGzo4CjKJCyDmFARSGdgAKPRpfc1TSPpCkqI7E7M1g87B25DhzsBt2vE3eng5NREREREQamdZgkzbJMAy3HgidSgswfu19UBIcisPq68nwmrUj/SIIySkl4qQNgExHpocjEhERERGRptDgCYSioiI+/PBD3nnnHZKTkxu6epEGkevMpdgoxjDMGLYQOpfmw8my9loQ1tHD0TVv2ZFBZIZGELO3bNnLDEeGhyMSEREREZGmUOtlHE/3hz/8ge3bt/PZZ58BYLPZmDp1KgcOHAAgKCiIN954g759+9Y/UpEGlOYoG75g2EIIdDoIzDqBUVICXt4UtdOY/rMymUgeHMM5P65n3agQCswFFDuLPR2ViLQVV14J6ellkyhqPgQREZEmVa8eCBs3buTiiy92/fzZZ59x4MABnn76aT777DPCw8NrNNGiSFMrX4HBWRJGJ8f/hi8Q2RHMGtlTnZJ+kTiMAKKPlyUOMpzqhSAiTWTLFtiwoeyriIiINKl63SllZGQQFRXl+nn16tX079+fK664gt69ezN16lS2b99e7yBFGppr/gNbKF2KsiCjrEeCqWPU2Q6Tcl5mDvXpyzn7yiaazHRkYhiGh4MSEREREZHGVK8Egp+fH3l5eQCUlpayadMmRo4c6dofEBDg2i/SXBiG8b8VGErC6JF6AAwnBARhCgr2cHQtR35iNF0Pl+Bld1JsFLuGhYiIiIiISOtUrwRCv379eP/999m9ezcvvfQSBQUFjBkzxrX/2LFjhIWF1TtIkYaU7czGhg3DaQFbMO1OHgXAFKnJE2vDp72ZYx1j6H64CIC9Ni3rKiIiIiLSmtUrgTB37lyysrKYMmUKL7zwApdccgkDBgxw7f/qq69ITEysd5AiDck1/4GtPV0KszBlZ5XtUAKhVkwmSBkYS8yvwxj2l+zDaTg9HJWIiIiIiDSWeq3CEB8fzxdffMGWLVsIDg5m6NChrn25ubnccMMNbttEmoPy+Q8MWxgDkneVbQxqh8nXz4NRtUzWnr54/dgenyIHhX5FHC89Tjfvbp4OS0REREREGkG9EggAoaGhjBs3rsL24OBgfvvb39a3epEG978VGELpkroV0PCFuvKzlnKgdzy9D2xi14Ag9pXsVQJBRERERKSVqncCASA/P5+UlBRyc3MrnYn93HPPbYjTiNSb03CS7kgHwC/XB/+cX5cfjOjgwahatoJ+nei32sGuAXCw5ABjAsbiZWqQjxYREREREWlG6vVX/qlTp3jsscf4z3/+g8PhqLDfMAxMJhN79uypz2lEGkyWM4tSSjGcXvQ9mlS2sV17TD6+ng2sBQtvX8QJn34E5R4lLxgO2Q8RY43xdFgiIiIiItLA6pVAePjhh/n222+ZNm0aQ4YMIThYS+BJ8/a/4Qth9EstWzXApN4H9eJtcfJL3zh679/D1iHB7M37mZgwJRBEpJHcey/k5oL+5hAREWly9Uog/Pjjj/z2t7/lgQceaKh4RBpV+QSK3gX+hGamlG1UAqHegjqBz94IoISjpFDsLMbXrF4dItII7r3X0xGIiIi0WfVaxtHX15eoqKiGikWk0aWVpgEQlVJQtiEkFJPVx4MRtQ5hgQUcCR9KWLoNpxn2F+7ydEgiIiIiItLA6pVAuPLKK1m9enVDxSLSqByGwzWBYr/9xwGtvtBQLGaDgN69iTpS9vO+nG2eDEdERERERBpBvYYwjB8/nv/+97/MnDmT6667jo4dO2KxWCqU69evX31OI9IgMhwZOHFCqTc9Uo6B2QzhGr7QUPqF+rK9tC8YB0jxzyfHkU07S4inwxKR1iYvDwwDTCYICvJ0NCIiIm1KvRIIN9xwg+v7devWVdivVRikOUlzlA1fCMy2YAJMvWIxeXt7NqhWpFuQN6s6JdIpZTepUVb2nVjP0KjLPB2WiLQ2ffpAcjJERUFSkqejERERaVPqlUBYuHBhQ8Uh0ujKV2DokpwLgLnfIIxTmZ4MqVUxm0ycExFE0cEwiMpjr/MA5xqXYjKZPB2aiIiIiIg0gHolECZNmtRQcYg0uvIEQrfjORgWC6bYfhgbvvdwVK1Lv1AfPvEeipftK04FmknO3kt0+z6eDktERERERBpAvSZRPF1aWhp79+6lsLCwoaoUaTClRimZzrLeBpFpNky94zD5+nk4qtano58X5vZRdDpWNhfK9swfPRyRiIiIiIg0lHonEFavXs2ll17K6NGjmTRpEj///DMAWVlZXH311Xz11Vf1DlKkvtId6RgY+BY6Cch3YOmf4OmQWiWTyUS/UB+KCvsC8EtIPgUl2Z4NSkREREREGkS9EgjffPMNv//972nfvj1z5szBMAzXvtDQUDp06MDy5cvrHaRIfZUPX+hwsgSnxRtTTF8PR9R69W3vQ1LQAMLSSnFaTOxO+sbTIYmIiIiISAOoVwLhxRdfZMiQIbz77rvceOONFfYPGjRIKzBIs3DSUZZAiDxZQlGPWExWHw9H1Hq197HQKcCKT3onAHZaj+F0Oj0clYiIiIiI1Fe9JlE8cOAA8+fPr3J/eHg4mZma5V48r7wHQkSaDe9EDV9oDBuKNri+9w9qR1LOcHyKPyE30Mz3xz/EN7Kra/9wv+GeCFFEREREROqhXj0Q/Pz8KCoqqnL/8ePHCQkJqXW9b7/9NmPGjCE+Pp5rr72W7du3n7X8F198waWXXkp8fDwTJ05kzZo1bvsNw2DRokWMHDmSAQMGMGPGDI4cOeJW5vbbb+fCCy8kPj6ekSNHcv/993Py5Mlaxy7Nj82wccpxCoCQTAiI6+fhiFq/yKB8ir2DCE8qm6gyozjFwxGJiIiIiEh91SuBMGzYMFasWEFpaWmFfenp6bz//vuMHDmyVnWuXLmShQsXMmfOHD7++GPi4uKYOXNmlT0ZtmzZwn333cc111zDihUrGDt2LHPmzGH//v2uMq+88gpvvvkmf/7zn3n//ffx8/Nj5syZlJSUuMoMHz6c5557jlWrVrF48WKOHz/O3XffXavYpXlKK00DEwTmlZIffg4mb29Ph9TqWb0chAYUkm0v6+2REg6O/BwPRyUiIiIiIvVRrwTC3LlzOXHiBNdccw3vvfceJpOJtWvX8uyzzzJx4kQMw2DOnDm1qnPp0qVMnTqVKVOm0Lt3bx555BF8fX356KOPKi2/bNkyRo0axa233kqvXr2YO3cuffv25a233gLKeh8sW7aM2bNnM27cOOLi4njyySdJS0tj9erVrnpmzJjBoEGDiIqKIjExkVmzZrFt2zbsdnvdXyBpFk6WngAg8qSN4riBHo6m7egQnEemXy/CTzgxzCby0g55OiQRaQ0++QTWrSv7KiIiIk2qXnMg9OzZk3feeYe//vWvLFq0CMMwWLJkCQBDhw7lT3/6E9HR0TWuz2azsWvXLm677TbXNrPZzIgRI9i6dWulx2zbto0ZM2a4bRs5cqQrOZCUlER6ejojRoxw7Q8KCmLgwIFs3bqVyy+/vEKd2dnZ/Pvf/yYhIQHvWj6tdjgcOByOWh0jjetk7mGwQvsMB37DznH7/VQ2uZ9rMRGjwq62yYCjebaybw2DAnyx5dsxmUxVHtIlwIvQgDzMpkjI7gEdj3K8XRFBNjsmL4veI9KslbdPtdNmatCg/32v31GdqZ1LW6B2Lm1BU7fveiUQAM455xxef/11cnJyOHr0KIZh0KVLF0JDQ2td16lTp3A4HISFhbltDwsL49Chyp9eZmRkEB4eXqF8RkYGUDaUonxbVWXKPfXUU7z99tsUFRUxaNAgXnrppVpfw+7du2t9jDSulA6pYAW7PZK0w/vJPC0z0OHEiQrl802+ANjstiaLsTnLz8+vsK2goOCsx5zILwYgwCuYZOtQ2hUeojDAQvbBnZS078C2U9saI1SRBrVjxw5PhyDS6NTOpS1QOxdpOHVOINhsNj755BN+/PFHjh07RkFBAQEBAXTr1o1Ro0ZxxRVXYLVaGzLWRjdz5kyuueYaUlJSeOGFF5g3bx7/7//9v7M+aT1T3759W9x1t2ZFziLW5q8FoCSoLxcMch/C4MxJq3CMLb9sTg+rtxIIAIGBga7vDcNwvdfP9r7oGBACgLXAzs5kKyEn2nGiZz75gUV06dCRQd0GNXLUInXncDjYsWMH8fHxWCwWT4cj0ijUzqUtUDuXtsBmszXpQ+w6JRD27dvHHXfcQUpKCoZhEBQUhL+/P1lZWezevZtVq1bx0ksv8c9//pNevXrVuN727dtjsVgqTJiYmZlZoZdBufDw8Ao9CU4vHxER4doWGRnpViYuLs7tuNDQUEJDQ+nRowe9evVi9OjRbNu2jYSEmi/7Z7FY9AHVjGSm7wUfCM4pxdK9b8XfjbniNCCu++Ka541aN9Pp35b9YDKZzvr6mH99XUMDi/G2lHKydDgYX3Ei0kLXrAwsXfQekeZPn+fN1GefQVER+PnBFVd4OpoWT+1c2gK1c2nNmrpt13oSxYKCAmbPnk1mZib33HMPa9as4b///a/b17lz55KWlsbtt99OYWFhjeu2Wq3069eP9evXu7Y5nU7Wr19f5U38oEGD2LBhg9u2devWMejXMZLR0dFERES41Zmfn8/PP/981sRA+dh4m01PoVuyExl7AfDL8aFjsL+Ho2l7zCaIDM6n0KsDkSfLPm6yTx3xbFAi0rLdfjtMnVr2VURERJpUrXsgLF++nNTUVF5//XWGDRtWYX+HDh247bbbGDBgALfccgsff/wxN954Y43rv/nmm5k3bx79+/dnwIABvPHGGxQVFTF58mQAHnjgATp06MB9990HwPTp05k2bRqvvfYao0ePZuXKlezcuZNHH30UKHtSOn36dP75z3/SrVs3oqOjWbRoEZGRkYwbNw6An3/+mR07djB48GCCg4M5duwYixYtomvXrrXqfSDNi2EYpDnSAAs2Rwc6B9R7yg+poaP5/0u8Gd5ZQAiFeX2h4y6ORjhYe+QEhk8QAKM6BXgoShERERERqY1a31F99913nH/++ZUmD0533nnnMWLECL755ptaJRAmTJhAVlYWixcvJj09nT59+vDqq6+6hiSkpqa6ukcDJCYm8vTTT/Pcc8/xzDPP0L17d1588UViYmJcZWbNmkVRURELFiwgNzeXwYMH8+qrr+Lj4wOAr68v//nPf3j++ecpLCwkIiKCUaNGcccdd2g+g5Ys/SRp7csmTMzxiSHUR13XPMHHWoyPdxHpRjxh+TsoDDRjT/4Br6gJng5NRERERERqodYJhP379zNt2rQalR0+fDjLli2rdVA33XQTN910U6X73nzzzQrbLrvsMi677LIq6zOZTNx9993cfffdle6PjY2tU5zSvOUf2Ep+Xy8wINwnqlaTYUrDCgrMouRUFH7pkRQGZpAadJwuhhNMtR5FJSIiIiIiHlLrv95zcnJcExNWJzw8nJycnFoHJdIQTmTsAcC70IfoAD8PR9O2BfjnYjaXcoLzMDkM0iMtWNK2eTosERERERGphVonEGw2G15eNeu4YLFYsNvttQ5KpL6MrAzSfPIBsJVG0tnf28MRtW1mk0FQQDalpmBC08qGDp1ybvNsUCIiIiIiUit1mlUuOTmZXbt2VVsuKSmpLtWL1Jtzzw5SO5fdqNpLO2oCxWYgKOAUOXlh5BYPAjZxJKqUhOwT0KnmS72KiIiIiIjn1OmuatGiRSxatKjacoZhaNy5eETpvu2cvKJsAsxAZyf8vDTW3tO8vez4++aRW9ybiNxN5AebsZ36AVACQURERESkJah1AmHhwoWNEYdIgzFyszlZeoJS745QaiXKGubpkFq04JTD//vBAF+7DWuuFeqQGzQZeRxiGJbMLhB8nONhGdi//gzvsVc0XMAiIiIiItIoap1AmDRpUmPEIdJgnHt3khLlC4CjpAPRAZr/oLkIJAMf8jlhHkZg6TGywr05cSCJLp4OTERajsBACAoq+yoiIiJNSv26pdUx9mwnJaps/gNHcQc6K4HQbJhMEGY6CoYPwWlBAOwMz8EwnB6OTERajL17ITe37KuIiIg0KSUQpFUxCvJxHDtEaqeyBIKpJJIIP4uHo5LThZKEl9NBumMYAAd7+lB4cLuHoxIRERERkeoogSCtirFvJ+kR3pRazRgOKx28w7FoIs9mxWIqpVthOnZHB4KyLTgtJnan/ejpsEREREREpBpKIEir4tyzwzV8wVkSSVSA1cMRSWV6558ETBQW9AVgZ+cinGmpng1KRERERETOSgkEaTWM4iKMQwdcEyg6iyPp7F+nlUqlkYWUFhJRkkt+aV+8bZAb4s2Rfas9HZaItAT33w+33lr2VURERJqUEgjSahj7d+M0HKSWr8BQHEnnACUQmqve+SfA8ML7VEcAtgemYBTmezgqEWn23n0Xliwp+yoiIiJNSndX0mo49+wgM9wbm9WE4fTG3wgjyNs9R7ahaIP7MR3SK9ST097RqHFKmeiiLHwdNrJsQ/A1/s3R7r6k71xD5NDLPR2aiIiIiIhUQj0QpFUwbCUYB/eeNnwhgih/KyZNoNhsmTHolX8Sw9GOkFPtAfjJtBvDoQSOiIiIiEhzpASCtArGwX1QaielexDw6/wHGr7Q7PUqOInZMEgvPBeAAz29ydm30cNRiYiIiIhIZZRAkFbBuXc7BpDSuWzVBUdJBzoHeHs2KKmWn9POOfZTlJZGEnrKD8NsYmvBfz0dloiIiIiIVEIJBGnxjNJSjP17yArzpsTLieH0gpJQOvqpB0JLkGgrm4ciuyABgN3dnBQk7fdkSCIiIiIiUgklEKTFMw4fgJJiUnq2A8BZEk6EnzdWi+Y/aAmiHPlE+FooKO1O+1wLpd5mfj75rafDEhERERGRMyiBIC2ec/d2AFLOCSv7ubgDURq+0GKYgMQI37LvcvsBsL1TAbbsNI/GJSIiIiIi7pRAkBbNcDow9u0sm/8gtGz2fmdxJN0ClUBoSfq198XHYiKltB9B+VDiZ2Hnsf94OiwRERERETmNBolLi2YcPQRFhWR3DKLIbMNwWnCWhNEtSAmElsRqMREf6sPm9GJCTvUkL/AQW9unMcBWhJfVz9PhiUhzcvnlkJUFoaGejkRERKTNUQJBWjRjzw4AUhK6Abk4S8Lp5O+Dn5c617Q0ieF+bE4v5oBjCO2LDpIfaGHfoa/oF3elp0MTkebk//0/T0cgIiLSZukuS1osw3Di/DWBkNrFHwBnSSTd1fugRQr1tdA9yBsDb6IzOgGwxeeX/8/encdJVtX3/3/de2vv6n2d7p59X3o2BtlmAAERgxtuX1FA/OISvuabaPCrvyRGBU0wxBhADRqIBJHEkCgaFTAiqzKAwAyz72t3z/TeXVVd+73n90f19EwzCwPMdHVPv5+PR3Fv3Xvu7c+tuXRXfepzzsFz3SJHJiIiIiIioASCjGOmdR8kYphAkLZwHCiMf6AEwvh1Vm0IgJ3ueQSyHr0VNrt3P1ncoEREREREBFACQcYxs7kw+0JsyVwGzSDG2Di5Gs3AMI7NLAtQFrAZpITpXYX+zS+Z9RhjihyZiIiIiIgogSDjkjFmuPvCwfkNAHiZaqaUhPHZVjFDkzfBtiyWVReqELrNSmzXcKDGom3f80WOTETGjBUroLm5sBQREZFRpUEUZXw62Ab9veDz01bjQf5Q94VAsSOTN2lJdYjfHUzSmi9nSWeUbZMGeS75LM2pEx93bvjc0QlQRIrr4EFoayt2FCIiIhOSKhBkXDpUfWDNmkebe6CwLV2v8Q/OABG/zYLKIAApcx4YQ1uDg9d5sMiRiYiIiIhMbEogyLh0KIGQWDSbuIljjEXYq6Mm5BQ5MjkVzquPYAHbsvU0dRS6pHTF9xQ1JhERERGRiU4JBBl3TFcHdHeA7dA+NVrYlq1iejSCZWn8gzNBVchhUVWhCiGTXQLA3klg+nqKGZaIiIiIyISmBIKMO8PdF2bMps3qBMBN1zNd3RfOKOc3FKoQ9nuLqOsyGNuit2dnscMSEREREZmwlECQccfbUkgg2PNbaM0VBtLSAIpnnsqgQ0t1oQqBxDwA9tS7mES8iFGJiIiIiExcSiDIuGL6euBAK1gWg3OmEzMDGGNRbTVQ4tftfKY5vz6ChWEvy6ns9cj7bWIHtxU7LBERERGRCUmfuGRc8bZsAMCaOoP2QD8AJlvJ9Gi0iFHJ6VIRdGioiIFlExyYDsDumiwmnS5yZCIiIiIiE48SCDKumM3rALDmtdCaawUK3Rc0/sGZa2p1HxaG3ZxHadwlE7JJtm0tdlgiIiIiIhOOr9gBiJwsE49h9u8BCuMf7Mv9d2F7po7m6PETCM8cGBxeb/dlR+wry7qnPlA5pUL+PJMqYrT3lxPtaiRe2sHu8kEW5nJYfiWORCac226DZBIikWJHIiIiMuEogSDjxqHBE62mKSSjfmIDfRgDjb4m/LambzyTTanu48BAGXusC6lI/geDUYd063bC0xcUOzQRGW0f+UixIxAREZmw1IVBxg1zaPrG+S2059sL23IVzND4B2e8kD/PpPIBPCtA5cFqAHZF+/FcVZCIiIiIiIwWVSDIuGCSg5g9OwGw5y9mf66QTPDSdUyv1fSN49m+eJbWI7qZvNqhbidOuBOrv4x95mJKB/+LRKnD4IFtlDbPH61QRUREREQmNFUgyLhgtm4E40H9JKyqGvZmCwMo+nL11IWdIkcno8Hn5CmN9pFzItS1VgGwJ6IqBJEJZ+tW2LixsBQREZFRpQSCjAve0OwL9vzFpLwUMXoBmOJvxrI0/sFEUV7ag2157HIuIRp3SUZs4gf0IUJkQrn0Uli0qLAUERGRUaUEgox5Jp3C7NwGgL1g8fD4B162jJnRsmKGJqPM5+RprIiR9UWobq0BYE90ANfNFTkyEREREZEzn8ZAkDHPbN0Ings19Vi1DexNPAWAl65n+iRN4zcebZnSO+J5zPfSSR87pbqP9v4ydvovoWrgP4iV+4gd2AqzV53qMEVERERE5AiqQJAxb7j7woLFAOzNFcY/iHgNlAY0/sFEE/C5NFUOkPOFqGxrAGBPWZxUPlHkyEREREREzmxKIMiYZjJpzI5CH3d7wRIyJkPM9AAwLdBUzNCkiCZX9WFbHjsCb6WyN082YPPSvoeLHZaIiIiIyBlNCQQZ08y2TeDmoboW6hoK4x9YBi9XyuxoZbHDkyIJ+DyaKgfI+wKUHSgkkl6JtjOYHShyZCIiIiIiZy4lEGRMO3L2Bcuy2JneD4BJ1zElqvEPJrLJVX04tsf20EXUdOXI+y3+0KoqBBERERGR00UJBBmzTDaD2b4ZODz+wb6h8Q/KmUTA0fSNE1mhCqEf1/FT0jEVgA2lHcQyfUWOTERERETkzKQEgoxZZvtmyOehshoamsiaLHG6AZgRmFzk6GQsmFzVj2N7bIusouFgHtdn8ULbr4odloiIiIjIGUkJBBmzvE2HZ1+wLIvWXNvQ+AclzC3V+AcCfsejubIfz/YR7ZoLwObybvrTXUWOTERERETkzOMrdgAix2Jy2eHuC9aCJQCsT20rPM800lCrW1cKmqv6ae0rZ2PJ2Uxv20J7k5/ftv0nTY3Lj3vMueFzRzFCETml/vAHcF1wNI2viIjIaFMFgoxJZscWyGWhogprUjN5k2e/uwuAejML29L4B1LgdzwmV/Xj2Q7+nkIVQltFlnQ6VuTIROS0mDQJmpsLSxERERlVSiDImDTcfWFo9oW9ub24VhaTDzMvovEPZKTmygF8tsvW0nOYvC+HsS26+rYXOywRERERkTOKEggy5phcDrNtEwDW0OwLmzNbAcgPTmV6aaBoscnY5HM8ZtT1YCybfM9CMIb2yjyplGZkEBERERE5VZRAkDHH7NwK2QyUVWA1TSFrsuzO7wagJDeDiqD6vcrRJpXHKAun2FWxgqm7XQA6VYUgcub553+Gb32rsBQREZFRpQSCjDne5pHdF3bnduORx8uVMj3UUOToZKyyLJhT3wUWxBJvwfIMB6sMmb6DxQ5NRE6lW26Bm24qLEVERGRUKYEgY4rJ5zFbNwJgLSx0X9iaLXRfcNV9QV5DNJRlclU/7WXzadpT+PV2YHB3kaMSERERETkzKIEgY4rZtQ0yaSgtw2qeStpLsze3FwBvcBpTo/4iRyhj3dSaXoK+HJ25i7BdQ0eNTa5NSQQRERERkTdLCQQZU0Z2X7DZkduBh4eXrWBSoJqQT7esnJjPNsyq72Yg2ETtvggA+712jOsWOTIRERERkfFNn8ZkzDBuHrNlA3B49oVt2W1AofvCtFJVH8jJqYkOUh0d5ID3VpycoavWh7t7a7HDEhEREREZ15RAkDHD7N4B6RSUlGJNns6gN0hrvhUAd3Aa0zT+gZwky4JZdV3kfOVUtNUAsL20DyudLnJkIiIiIiLjlxIIMmZ4m14BwJ7fgmXbbM9ux2DwMtX4vVIaS3xFjlDGk3Agz7SaXtrMWwklPWLlPhJ7NxY7LBERERGRcUsJBBkTjOse0X1hCXC4+0I+MY0ppX4cyypafDI+NVf1Ew5C4MBsALZOymL19Rc3KBERERGRcUoJBBkTzJ4dkEpCJIo1dToxN8YB9wAYcJNTmK7xD+QNsC2Y09BFh3MOZT2QC9h0dW0udlgiIiIiIuOSasJlTPA2rAUOdV9w2JYuVB946TpwI0zZ/DzuxswbOndzPDu8npjS+6ZjlfGlIpKmoTxOuudsqP4Du5otOva9TP2U5cUOTUTeiDlzoLwc6uuLHYmIiMiEowoEKTqTz2OGpm+0Fi0Djui+MDidci9DpffGkgciADNqu0kEZlLdFsDYFs8kn8LTtI4i49Pjj8PGjYWliIiIjCpVIEjRmZ1bIJOG0jKsqdPpdXvpcrvAWLjJySzI9nCi0Q+er+864fkHKvVBcaIL+Dxm1nWzr+2tOPlHaau32bnjf5g99x3FDk1EREREZNxQAkGKztuwBgB74VIsyx6uPnBTk8ALsjDXU8zwZBSUte8+6bZewHnd57enzaKhPM6BgSbK91fRPb2P3wU3M23wAvwlZa/7fCIiIiIiE5G6MEhRmWwGs3UTUOi+YIxha3YrAO7gNJrzcSq97IlOIRPMQNY96ceRLAvm1HfRxlsJJQ2xcoe1m/+zSFchIiIiIjL+qAJBisps2wS5LFRWYzVOpsvtot/rB+PgJptZlG0rdogyjh1KIsQSh5JQWUpLI1id82DaVl5sjsHLL5KeNB+AVZNKihSpiJy0j34UuruhpgYeeKDY0YiIiEwoSiBIUQ13X1i0DMuyDndfSDbh92zm5vqKGZ6cgSrKujiQXEFZ/04GK/L0ZJ6gND8TzxcodmgicjKeegra2qCpqdiRiIiITDjqwiBFY1JJzPYtANiLlo7ovpAfnMrsXD8BvGKGKGcg2zbMKAswkFgJwLY5fkq3/E+RoxIRERERGfvGZALhgQce4JJLLqGlpYUPfvCDrFu37oTtH3nkEa644gpaWlp417vexVNPPTVivzGGO+64g5UrV7J48WKuv/569uzZM7y/tbWVv/zLv+SSSy5h8eLFXHbZZdx5551ks+p7fzqZLevBc6GuAatuEgfcAyRMAuP58VJNLMpq8EQ5PaqCNlFnMqGeKoxtsbV+D+Hu9mKHJSIiIiIypo25BMLDDz/Mrbfeymc+8xkeeugh5s2bxw033EBPz7E/TL788svcdNNNfOADH+BnP/sZl156KZ/5zGfYtm3bcJu7776b+++/n69+9as8+OCDhMNhbrjhBjKZDAC7du3CGMMtt9zCr371K/7iL/6CH//4x/zjP/7jqFzzROVtWAuAvXAZwOHBE5PNlPn8THHjxQpNznCWZTGzzE88tQrbhfbJIfw7/xvjacpPEREREZHjGXMJhHvvvZcPfehDvP/972fWrFncfPPNhEIhfvKTnxyz/Q9/+ENWrVrFJz7xCWbOnMlnP/tZFixYwI9+9COgUH3wwx/+kBtvvJHLLruMefPmcdttt9HZ2cljjz0GwIUXXsitt97KypUrmTx5Mpdeein/+3//b/7nf1TWfLqYRByzeztQ6L7gGY/t2cJzd3Aai6qCWMUMUM54QcdmVrQKr282AC8vMeSef+o1jhIRERERmbjG1CCK2WyWjRs38ulPf3p4m23bnH/++axZs+aYx6xdu5brr79+xLaVK1cOJwdaW1vp6uri/PPPH95fWlrKkiVLWLNmDVdeeeUxzxuPxykvL3/d1+C6Lq6rbzFfi9mwBoyBxsl45ZXsy+4lZVIYN4iXamDBFD+ed3LjHxhjXqPBKQhYgMMvpQGs8fS6vipW4xU2VAVs+rMrSGR2E6uAtRt/z/LuRViV1aMfo4wZh36H63f52GQDFoX/rT39G71hus9lItB9LhPBaN/fYyqB0NfXh+u6VFePfPNeXV3Nrl27jnlMd3c3NTU1R7Xv7u4GoKura3jb8dq82t69e/nRj37EF7/4xdd9DZs2bXrdx0xEM194lhKgvaKW7rVr2Va7DUrBHZxChcmyd/N66g8ePKlzDUbTJ9yftcbUbX5GyOXG1/ggiURixPOD/YfvrTDQxzLsuj/w0oooTT+7j4NL3wqWamAmuvXr1xc7BDmGllyOAJDL5Vi/dm2xwxn3dJ/LRKD7XOTU0SerV+no6OATn/gEV1xxBR/60Ide9/ELFiwgENB0cCdiBvowv+wCLJoueweTSqO8EH8BAHdwKuc0V7G4ehLeQOdJnW9v9NiJoEPcrGZyOFUMheSB3x8YV11MotHoiOcNwYYRzyvytWxNbSUbjrFpToa3Oh7W4rNGM0QZQ1zXZf369bS0tOA4TrHDkVex/X4A/H4/S5cuLW4w45juc5kIdJ/LRJDNZkf1S+wxlUCorKzEcZyjBkzs6ek5qsrgkJqamqMqCY5sX1tbO7ytrq5uRJt58+aNOK6jo4PrrruOZcuW8bWvfe0NXYPjOPoF9RrczesLJfBTZ+CrrGJndidZsph8GDtbx/zqEI5jg31yQ3RYr/VN8Xj6pDvGHeq2YA3/Z5x4VayWPXJDJOCjNncBnTzC5oVRFvz0YRrnLMAqGZl4kIlFv8/HNgv073MK6D6XiUD3uZzJRvveHlODKAYCARYuXMjq1auHt3mex+rVq1m2bNkxj1m6dCnPPffciG3PPvvs8LcSzc3N1NbWjjhnIpHglVdeGXHOQ8mDhQsXcuutt2Kf5IdXef28DYXxLKxFhdd/W7YwY0Z+cCpzKkKEHL32Mvqa/U2UZqdhbIvV54RJ/+LB1x5fQ0RG3yc/CZ/7XGEpIiIio2pMVSAAfPzjH+eLX/wiixYtYvHixdx3332kUine9773AfCFL3yB+vp6brrpJgCuu+46rr32Wn7wgx9w0UUX8fDDD7NhwwZuueUWoPDt9HXXXcddd93F1KlTaW5u5o477qCuro7LLrsMKCQPrr32WhobG/niF79Ib2/vcDyHKhjk1DDdnXCwDWwbe0ELOZNjV64wvoU7OI2WycEiRygT2XuqL+aBgR/SNjnEji17mP/y8/jOOrfYYYnIkb7ylWJHICIiMmGNuQTCH/3RH9Hb28udd95JV1cX8+fP55577hnuknDgwIER1QHLly/nm9/8Jrfffjvf+ta3mDZtGt/97neZM2fOcJtPfvKTpFIpvvzlLxOLxTjrrLO45557CAYLH1Z///vfs3fvXvbu3cuFF144Ip6tW7eOwlVPHMPVBzPmYEWi7MpuJU8eLxelxNQwtdRf5AhlIqv2lbM0cA5r8qv5/apKGn/831RNnY5VU1/s0EREREREim7MJRAArrnmGq655ppj7rv//vuP2vaOd7yDd7zjHcc9n2VZ/Nmf/Rl/9md/dsz973vf+4YrHOT0McYMJxDsV3VfcAensbwqhK2R76XIVkZXsKNvO/FQN79fVcqlD95P5FOfxfKNyV+XIiIiIiKjRp3NZfR0tENPFzg+rHmLSHpJ9uT2AIXZFxZVqfuCFJ9t2byr7HIwFrtnRthXMUDmN78sdlgiIiIiIkWnr9Rk1Hjrh7ovzJmPFQzxSmo1Hh5epprGQA3VId2OcnqUte8e8bx538Ax27mlhSlYq4Dl9dW83NDNMxdV0vSj3+OkBrGrjj0bDIBz8dtPWbwicgLNzdDWBk1N0Npa7GhEREQmFFUgyKgwxuBtXAsUui/kTI51mXUA5AYW0FIVKmJ0Ikc7p7Oa8nSIVMThdxdWktu6CZPNFDssEREREZGiUQJBRoXZtwsG+iAQxJo9n42ZjaRNGi9XipVqZl5FoNghiozgGIvL99WDge3zSmhrhNTWLZraUUREREQmLNWMy0l75sDg62q/alLJ8Lq39kUArAWL8XwOLydfBiA/MJ855SFCPuWyZOypT4VZ2lXF2rpenrqkmg//qJ1ceyuBpsnFDk1EREREZNQpgSBvWrvvpWNufy5VqCqwcnmWbHoZB7CXnM227HbiXhzjhnAHZ9AyQ90XZOw652ANu8oTxKJZVq+sZNUTW/HKy7GjZcUOTURERERkVOlrXzntKra342TzUFEFU6bxUqaQcMjH5hL1+ZlW6i9yhCLH5zc2l+5vAGDToigHmvyktmzGuPkiRyYiIiIiMrpUgSDH9FzquaO2tfuyb+hc1Rv2AGAvPot97n663W4s4yMfn82imiC2Zb2ZUEVOu6bBCC3dFayv6efxy2q4+kft9G/fQcXcuVi6f0VERERkglACQU4rfzxF2d4OAOwlK3gp/RQAudgs8IIsqg4WMzyZoLZM6T3m9gMB56ht53TUAnDegVr2lCWIl8Hq8yu48Ol99JWUUjW56bTGKiIiIiIyVqgLg5xWVZv2YhlINFXTUZanNd+KZWxysfk0RnzUhJTDkvEh4Dlc3FroyrBhSSkHGgKU79rIQG9/cQMTERERERklSiDI6WMM1Rv2AtCzaBovpgszMTip6eBGWFSl6gMZX6bGo8zrLQML/ucdjbgOBDevZTCZLnZoIiIiIiKnnRIIctpEDvYR7onh+Wza5tayM7cTgETffBwLFlQqgSDjz8r2eiI5h8FSjycvbCaQz5LbtI5s4vVNcyoib9CPfgSPPlpYioiIyKhS/bicNtUbC9UH/bOaaHO6wIVofgqpXDmzKwKEfG88f7UvfnhAx4FK903HKgIwkD36XjryXjtk4Y4a/jC/g+2LbGZtr2Baaz8d/3Yfk274NI5z9DgKInIKXXxxsSMQERGZsJRAkNPCcj2qNu8DoH1xM91uOwCJnnkAtFSFihabyJs1qaeExq4S2msHefpt1Uy6P0H9gZ1s+M+fMnDhO445M8OqSSVFiFRERERE5NRRFwY5LRr2duBLZcmWhNje6GEwVFBPKlVLic9iepm/2CGKvCmLdlXjz9kkSnM88bY5ACzY+hzOhpeLHJmIiIiIyOmhCgQ5LaZuLVQfdCyeQqfXCYAdXwTAoqoQ9jG+oRUZi4435SNA9UCIgzVJds1K8OSF87n46c0sf/6XPFtRQ2Dy1FGMUmQCefJJyGQgGFR3BhERkVGmCgQ55QKpDA17OwDY2FKKi0uQMG099QC0VGvwRDkzlCb9lCcCYMHmJWm2zJ6Fz3NZ/sR/kO3tKXZ4Imema66BK64oLEVERGRUKYEgp1zz9jZszxCbVMH+UD8ATnIOYDGzzE9NSIUvcmawsKjtDRPKOBjL5dnLw3RW1RJNJ2h55H5y8XixQxQREREROWWUQJBT7lD3hXXnTyJHDj8B+rrmAnBOXaSYoYmccjYWjV0l2MZP2hngsQ/OJBYpo3KwlzmP/AgvnS52iCIiIiIip4QSCHJKlfbGqOwawLUttjfmAQimZ+AZPw0RH5Ojqj6QM4/Ps6lwp2MZm/5gO49/6FxSgTD1/QeY+sgDePlcsUMUEREREXnTlECQU2rK1v0ArH1LAykrg4NDX/dCAM6pCx9zejuRM0GAEqbkVwLQXraFZ666nKwToKlrL42//k/yrlvkCEVERERE3hwlEOTU8QxTtrVigA0tha4Kkdxk8vkQIX+OuRWB4sYncppVu3OozS8AYFfdBp57xzvJ2w7T2raw5z9+jOt5RY5QREREROSNUz25nDJ1rV2EB9PsnVrCYDiHhUWsuwWA5sp+bGvScY995sDgiOftvpcoqzx4/B9WeUpCFjnlmvPnkrJ7SdgH2T5tG6GL3s3ZTzzE1O0vs+1nJcy96j2axlRERERExiVVIMgpc2jwxBcuqAYg6k4inSnDZ7s0VMSKGZrIqLGwmZ69lICJkrFjbFvQwfpzrwRg1vpnWP/IY3jGFDlKEREREZHXTwkEOSUC6SyNuw7QU+WnqwYwkOpdBEBj5QA+Wx+YZOLwE2ZG9jIs4xBzWtm+3Kb3vMsBWPCHR1nz+O+VRBARERGRcUcJBDklJm/dj+N6vHB+ofrA71YRH6zFsgxNlQNFjk5k9EVMDVNzqwDo8L1C/0XTGFh6AQAtv3uItb99RkkEERERERlXNAaCvHnGMH3jHuKlDnumFwZKzPUXZl4oifRzMJ0CYG/i6eOfQ3einIGqvFkk8z10+tbzWPIxPnTlBxnI5yjf8AItv/856zyXxW+7WGMiiLwera3FjkBERGTCUgWCvGnVB3oo60/w8lnlGAt8bhmpxGQAyqM9RY5OpLia8mdT6jaRJ88vB39F5L3von/p+QAsXP1LNj76GEaVCCIiIiIyDuh7X3nTpm/cSzJss3lhFAAvPh+wiITiBPzZ4gYnUmQWNtNzb2WL9XNixPhx36+YdfbbqcnB7I3PMu+FR3khnSVzzkVYR1QirJpUUsSoRURERESOpgoEeVMC6SxNO9t5ZVkpngM+r4Rk/3QAykpVfSAC4CPEzNzbsI2PuNPO3sAzdJ//NrYuvhCA5eseJ/LsbzGeV+RIRURERESOTxUI8qZM2bKPnN+wfklZYUNiDgaHoD9FKJAsbnAiY0jYVDEtdzG7/L+l19mBG8hizr2EzY6P+WseZ8nGp9ng5YlfcDmWrdyuyHHdfDMMDEB5OXzlK8WORkREZELRu1R544xh+qa9bFhcSt5v4fPCDPbPBqC8tBuNCycyUoU3jRm5wvSOA84+dgQepffs89mwojDF46LNz1Lx9MOqRBA5kbvvhn/8x8JSRERERpUqEOQNq2nvITQ4yCvLmgCwUzPwPD8+J0skHC9ydCKjr9330km1q3Sn0+/sJWEfZFvgV8xafgXrbB+LX3iY+dteYGcug/eRj2D79CtaRERERMYOVSDIGzZ90x42LoqSCdk4Jshg73wAykt7VH0gcgIBSpmTvRKfCZGye9gW+CX9Sxex5rx342Exc/crdP7ge7gpdQMSERERkbFDX2/JGxJIZajffYDfXtcAgC89hbwbxLbzRCP9xQ1OpAjK2ne/rvZz9g3QGK5h9cIDpEID7OQ/Oc+exMYFFzB36wtUH9hN/z99i5KFLfhDIZyL336aIhcREREROTmqQJA3ZOqW/WyfGyYZ9WEbP6nehQCUlfRh25rTXuRkRFMBVq5rIpr0kwq5/G5xO1a1y5ZFK0mEopQl+siueZFUPFHsUEVERERElECQN8AYpmzew5oVhZkXgtlmMrkSLDzKor1FDk5kfAlnfVywrpHyRICs3+PZlna8ykESy1fSXVpLOJvEWfsC8fXrih2qiIiIiExwSiDI61bT3k1HvSFW7sc2Dqm+wtgH0ZIBHMctcnQi408w73D++kaqB0LkfYbnFh4kUz2AvXQF+2qn4fPyBH96H/3PPFnsUEVERERkAlMCQV63aRv38tKh6oP8JFLpSgDKoj3FDEtkXPO7NudubKChJ4LnGB6e1kZPVS/V8+ezdcoiLKDk8V/Q+/P/wnhK1ImIiIjI6FMCQV6XYDJDzvTRVx3A9iyyA4Xqg0goTsCfLXJ0IuOb49ms2FxPc2cUY8Fvph5gV20XM6Y18crSywAoXbuagXu/h0loqlQRERERGV1KIMjrMnnrXtacVQpAyKtnMFEDQHlpdzHDEjlj2Fgs21bL4q5CZc8zTZ08PaWN+e98Ky9f/CGyToCS1l0k7/oHvL27ihytSBFcdBFcfnlhKSIiIqNK0zjKyTOGUO8BOs8uw/bAap+EwSZCH7Xdm7CsYgcocmawsFjVXkck7/B8Qzdbq2K0xR/g0vMuZV31p5n+63+jOtFD7r67cC5/F845q7D0P6BMFA88UOwIREREJixVIMhJq23rZtPCAAAht4Y+b1Zhu7VLyQORU8zCYkVnDe/bMZXyjJ+ESfDzwZ+TmrGLvR/+JFsa52MbD/Prn5P/r/sxmXSxQxYRERGRM5wSCHLSKtr20N4cwvLAS87BJUCAQco5WOzQRM5Yk5JhPrxtOkuCSwBYn1nP5uiviL//Up5oeRuuZcOmV8jdfQemq6PI0YqIiIjImUwJBDkpgf5B9jXmACjNlBKPNQFQa+1W9YHIaeb3bC6OXMxV0auIWlEGvAH+YP03uUtC/PTCq4mHolg9neTuvh1vw5pihysiIiIiZyiNgTDBPXNg8Jjb230jZ1SYtmELe1eEwRhcdx55N4BDlir2j0aYIgJM8U/hmvJreDr5NJuym9hhXqFsaSX/VfV+Ln3mSaZ078X9yY/wdmzBueK9WKFwsUMWOfUuuQQ6OqC+Hh5/vNjRiIiITCiqQJDX5Mvm6CwvTBlXnogQG5gMQA17sC2vmKGJTDhBK8jbSt7Gu0reRcSKEDN9pCc/xePvbuHZuefhYWFeeZH89/4Bb+/OYocrcupt2wabNhWWIiIiMqpUgSCvqW7Xbl5eGALAdeaQzYWx8Ki29hY5MpGJ4fn6LuzUc0dtnxeYx57cHnq9XpLRday/tJxdzVfxztWPUzHQR/5f78I5/2Lst16B5dOvexERERF5c1SBICfmGWKBHrAsKgd8DMamAhAt6cdvZV/jYBE5nfyWn9mB2czyz8LBwfUNEF/4Mv/2gRWsmbkIC4P37BPk77kD03mg2OGKiIiIyDinr6TkhKrb2tg8MwiA8c0kmS4FDOXRXkgVNzaRM9m++OEE3UClSyxx/ITd1Gg1pXYpu3O76ff6oWIbq68IsHnvKt79mxeJdrST/edv0XZhC50rZoNlcW743FG4ChERERE5k6gCQU4ok2/Hcywq+y3iyZkAREJx/H5VH4iMJQErwNzAXOb65xKyQuBk6Z+xlx9+bCYvLJ6O7XpMfuIV5vz7kwR74sUOV0RERETGIVUgyHFFe/rYO9UBwGdPJjFYDkB5aU8xwxKRV9k7ojohQjnz8FvdJOwDmFCMFy+GTYsW8M5Hd1LT2s2Cf/0f3FVgr3wrlqM/AyIiIiJyclSBIMcV6N1NLmATjRv6c3MBm2AgSSiovgsiY5mFRYmppdZdQMSrBWORrEnw4EcbePjyqeQcg/fko+S//y28fbuLHa6IiIiIjBNKIMgxBZIpWiflASjJ1ZBIVAOqPhAZT2x8lHnN1LjzCLjlYBn2zDPce8M01i6rwuvuwL33O7i//C9MWolBERERETkx1a6eodwnf31S7ZrjxxnLILyFfWcF8Gc9MolGPOMQYJCG3nVY1ikMVGSC2jKld9R+lo8QVWYG6XyMAQ7i+Qd5dlWUV5aUc8lvDzD5pdV4WzfivOMqrPktWPqfXERERESOQQkEOZrn0lHnAVDXZbPPmwVArbVLyQORcSxEGUFK8dxButjLYFmWX1xVR/P+PKue6KTyP+/DmjUP5/J3Y9XWDx/3XOq5k/4Zmt1BTrsvfxkSCYhGix2JiIjIhKMEghwllztI2+QQGEPGm0GWCA4Zqmgtdmgi8iZZWMwoqeIK52J+3vN7BoKbaZ3s48fXNNLySpwVL2wldNffQ+NkrKkzsfx+vPqukz6/2zFw3H3OxW8/FZcgE92nPlXsCERERCYsjYEgIxnDnobCB4DaLkO3uwCAGvZiW14xIxORU2ibu475FaVMyb8Fk2rE2LBuWSn3X9/M+pYSvPZ9ZF98ml3xzeCZYocrIiIiImOAEggygpXuZfvsQGE9W0+KCixcaqy9RY5MRE6HxlJYUTaVksQ5eNlyckGLZy6u4t+vaeRAg59pL+9n4W+2UNoZL3aoIiIiIlJkSiDICAcqOskHbEpjHj35xQBU0YrPOs5giyIy7vkcj0U1MKe7BtOzBOMGGaj08cv31vGL99SRsfPMe3IHM3+/i2A8XexwZaI7cABaWwtLERERGVUaA2ECef4Y/ZgHKt3h9VAizZ6mQk6pJFFKJw2AodbaNVohikgRVdr9nJWNsfvAYjpLY/jKtrF/aogff7SRRevinP38AC2PbqZrejXtCyeRC/uLHbJMRGefDW1t0NRUSCSIiIjIqFEFggwLx/qIlfvxZzz6c0sBKKeDoJUsbmAiMmocy2OWbxctg1ns9otxk80YG9YvLeVHH29i48Iotbt6aHl4I83r2nGy+WKHLCIiIiKjRAkEASCYzNLaWJijsbI/QL+ZDECttbOYYYlIkZTaMZbb62nsnUT24MV42XKyAYunL6niv66exEC5j0lbOlj88CYatnRg5TXIqoiIiMiZTl0YBICqg92svziC5RkGs4swOETopcTqL3ZoInKK7U0ce0yTsqx71LYpgV3UeCVsPXAeyWgX/spX6KqF//jIJFrWD3Lu73qYvK6d+u1dtC9owESqsWzlpkVERETOREogCIF0js6awreHFQM2He5sAOo09oHImFHWvrsoP3cg6wIxpvMc/YlGDiYvheqNOJFW1i0pYeucUi5+vIuZOweZ9tJ+TLALpkyHhiYlEkRERETOMHp3J0za2832uREAcukZuAQIkKCMjiJHJiJjhWVBpdXOXO8FarqqyHeeh8mHyYQ9fn1lNf/1wWkMlPkhk8Zs34x54XeY9v0YT10bRERERM4USiBMcP5snlhJhrzfJjIIA5kWAOqs3VhWkYMTkTHHtgy11j7mprZR3j4XNzYTY6BzkscD105h7UXzMIHgEYmEZzBt+zDe0d0jRERERGR8UQJhgmve2c2mRVEA7FQDWUrwkaESTY0lIsfnWC4N7GF2X4zIwUV42XJwcjy7JMm91y7g4JIFEAhCJoPZsQXz/O9wn3sak80UO3QREREReYOUQJjA7JyL5w0SL/Phy0FicCkA1dYebEtlxyLy2nxWjsm5VqYfCOHrm4bxbNIlPfxkVYr/eN8FJOcsgGAQshm8X/+c/O1fx33iUUwyUezQRUREROR1UgJhAqvb0cWmRSUAhAbLSZlqLFxq2FvkyERkvAlaGWbE+3nn9rkEk9VYtktP3Q5+cInDz9/+TuJzl0BVDaSSeE//hvw/fh334Z9i+nuLHbqIiIiInCTNwjBB2XmPQHcfbW+tA2NIxQtjH1SxH5+VK3J0IjJeOV0ZLu8qY/OkCDunHsQODNA69Q/cWzWHSOJtLO/aw5yNzxLtbsf7w+/xXlyNtWgJzvmXYDU0Fjt8GQ9++1vI58GntzAiIiKjbcxVIDzwwANccskltLS08MEPfpB169adsP0jjzzCFVdcQUtLC+9617t46qmnRuw3xnDHHXewcuVKFi9ezPXXX8+ePXtGtLnrrrv48Ic/zJIlS1ixYsWpvqQxqXZXN5vnF2ZeiCTDJNypgKHWKs5UcSJy5rCwWHDAz+UvNlLbXYllga9sG5m6X/DUpBA/XPkxfnfZtTBjDhgPs34N+e//A/n7v4e3bRPGqAuVnMDcubBwYWEpIiIio2pMJRAefvhhbr31Vj7zmc/w0EMPMW/ePG644QZ6enqO2f7ll1/mpptu4gMf+AA/+9nPuPTSS/nMZz7Dtm3bhtvcfffd3H///Xz1q1/lwQcfJBwOc8MNN5DJHB7IK5fLccUVV3D11Vef9mscCyzXo2JPF9uGpm7Mx+YAUM5BglaymKGJyBkkmHc4b0sl562fRDjtx/KlCNY9jVv9FM+XVnHX0g+w7n034i5YCpaF2bUd99//hfx3b8P9w+814KKIiIjIGDOmEgj33nsvH/rQh3j/+9/PrFmzuPnmmwmFQvzkJz85Zvsf/vCHrFq1ik984hPMnDmTz372syxYsIAf/ehHQKH64Ic//CE33ngjl112GfPmzeO2226js7OTxx57bPg8f/qnf8r111/PnDlzRuU6i61mTy87ZwTJ+20CGYdYZj6Aqg9E5E3bMqX3qEdPeYqmrgglXj0YcEpaCTX9gkx4C7/Jl/Ht2e/gdx/4MzJnXwjBEPR04T38U/L/+DXc3/wCM9BX7MsSEREREcbQGAjZbJaNGzfy6U9/enibbducf/75rFmz5pjHrF27luuvv37EtpUrVw4nB1pbW+nq6uL8888f3l9aWsqSJUtYs2YNV1555Sm/Dtd1cd3iz3fueUeXABtjwDPUbe3g6Q/WAGDHp2DwEaGPEvrAjHakMp6YI5aW7hV5HWxjUeo2EqKSmLOPnJ0kUP0iTukeBrvewvOZSp6fdAGzp5/Lys5NVL2yGvp68J59Em/10zBvIdaK82HKDCzLOq2xHvodPhZ+l8vRrH//d0gmIRLBTJDKwdNB97lMBLrPZSIY7ft7zCQQ+vr6cF2X6urqEdurq6vZtWvXMY/p7u6mpqbmqPbd3d0AdHV1DW87XptTbdOmTaflvK9X/cGDR20bjKapb4txoN4hUerDyUMssRSASncbWZMd5ShlvMrldK/I65dIFKZuDNCIHRwgH+rFDXQTanyEwMAsBvqXsT3tZ3tZC+EL5nJWxybm715DWfdB2Lwes3k96dJyeqbOpq9pOp7ff1rjXb9+/Wk9v7wxLTfdRKCzk2xdHevnzy92OOOe7nOZCHSfi5w6YyaBcKZYsGABgUCg2GHgDXQetW1vSRfTd+/n15eXAxBO1pMgQoAk1U43llX8uGVsMxSSB35/gNP7HbCciaLR6BHPSrmi7EqeTj/NrvwushXbqa3ooDJ1Nnu7G0gR4HeTlvJs41JWeL0s3/0yJVvXEooP0LThRZq2rYOWs7DOOg+rruGUxum6LuvXr6elpQXHcU7pueXNs4cSR36/n6VLlxY3mHFM97lMBLrPZSLIZrOj+iX2mEkgVFZW4jjOUQMm9vT0HFVlcEhNTc1RlQRHtq+trR3eVldXN6LNvHnzTmX4wxzHGRu/oOyjh7eo2dfHYNCjvSkEBhKxpYXt1m5Oc0WwnCEOdVuwhv8j8jq86p4p95fzLv+72JXdxRPJJ0iYGInwb5kybTKTcueyvTvCgWSeF6wqXphxGQ2zLuLirs00bnkBq6cLXlqNeWk1TJ2BveJ8rHktWKdwar8x8/tcjskC/fucArrPZSLQfS5nstG+t8fMIIqBQICFCxeyevXq4W2e57F69WqWLVt2zGOWLl3Kc889N2Lbs88+O/yNRHNzM7W1tSPOmUgkeOWVV457zjOV8VyaNhxg3ZJSAEpSZaTyNTjkqGJ/kaMTkYlsRmAG15Zfy9mhs3FwaHX386L9X0yZvIar5wRZWh3Cb8NBz8+Pqxdz5wU38OLbryMzeyFYNmbvLtyf/Ij8P96C++v/xnR3FPuSRERERM5IY6YCAeDjH/84X/ziF1m0aBGLFy/mvvvuI5VK8b73vQ+AL3zhC9TX13PTTTcBcN1113Httdfygx/8gIsuuoiHH36YDRs2cMsttwBgWRbXXXcdd911F1OnTqW5uZk77riDuro6LrvssuGf297ezsDAAO3t7biuy+bNmwGYMmUKJSUlo/wqnCbtrXiey/a5hetJxxYAUM1eHEsDy4hIcQWsAOeHz2dhYCG/S/2OHbkdrMusY6u1lXPqzqGkMs2W7iixwUpyuRBPBZt4an4TlbPOZXnbH5i7Yxvh5CDec0/hPfcU1pQZ2MvPwVqwBOs0j5UgIiIiMlGMqQTCH/3RH9Hb28udd95JV1cX8+fP55577hnuknDgwAHsI0rzly9fzje/+U1uv/12vvWtbzFt2jS++93vjpiO8ZOf/CSpVIovf/nLxGIxzjrrLO655x6CweBwmzvvvJOHHnpo+Pl73/teoDBN5DnnnHOar/r0M/kcZu8uNi2N4vosgtkA/ekZWHjUWHuKHZ6IyLByp5wro1fSmmvlqdRTdLvdPJ16mpAVIlzaSFNJH5lsmPhgJYPJMvr8dfx22pU8MfVyFgysZ8m2F6g/2Af7duHu24X3i/9ksKqeZPUkcpHocX/ulNLDY8B4nkf9wYOFsWRe1R3Mufjtp+3aRURERMY6yxijydhOAdd1Wbt2LS0tLWNiEEX3yV8Pr3u7t+O27ub+jzeRLHEI9synP76cOruNSawtXpAy/hjI5rIE/AGNgSCvW6xx+ojn/6v2whO294zHxuxGVqdWkzIpAAJeGWVeEz5CuK5DIllObLCSfP5wUnhafxtv2f0HJnXswpfNDG/PhqMkqxtIVtbh+Uf+nn51AqHj4EHqGxpGJK1BCYQxobkZ2tqgqQlaW4sdzbh16H3L0qVL1Tdczli6z2UiyGazrF+/ftTu8zFVgSCnnslkoHUvu2eESZY4+Fyb/vhiAJp9e3DzRQ5QRCasZw4MnkSrGcyhme2BR0lanWTtGN1WjIipJUoD5aW9lEV7SWVKiCcqSaZL2VPRxJ5lTURyKc5ufYW5rRuJ9ncRSCUItO6gvHUn6fIqBqsbSJdVH3PQWRERERE5mhIIZzizbyd4HuuXVwLgSzQCPirsbqJ2nIHihici8pocApR5TUSoJm63kbFjJK0uUlYvEVNDiVdLJDRIJDRIqO0ADMxlV0k9SX+Yp6afy9PT3sKMgXaW71tDQ+deAsk44YEewgM9uI6PZFU9ZvJkiJZiaUoaERERkeNSAuEMZpKDcKCNrlo/Bxp8YCAWWw5As7OnuMGJiLxOPkJUejPJmBhxu428lWbQ6mDQ6hxKJNQRsNLMi+1nYayV1nA126P19ATL2FnRzM6KZspzSRZ3bGVO2yZKew/i5LKUdrVhutqgJAr1jVBbX+xLlRNpaBi5FBERkVGjBMIZzOzZAcaw7i21AIRSVaTcUiJWnEq7u8jRichEU9a+e8Tz5n0nXwOVmNI74nkNQRJhm96yDJmgS9LqIml1kany09QVpTQVYGqqm6mpbvr8EXaUNLA3UsOAP8Izzct4rrGFmfGDLGnfQFV3K5H+HhhMYHZtg13bqYhEwbEwtfVY6uIwtrz4YrEjEBERmbCUQDhDmXgMujpIhm22Ty/8Mw/2D4194OxBVboiMp5ZWJSmAkRTfpLBPL3lGVKhPLFojieWt9LQE2F2awWViRCVuSRn9+9iycBe9pTUsqOkgbg/zJbyZraWNTIl2cPFmTZqO/ZgDrZDfIBgMg5b1mN2bMbUNWDVTYKyimJftoiIiEhRKYFwhjK7tgGw6dxJeDaUpaJ0ZJsIkKbOaS9ydCIip4aFRUnGT0mnn1QgT29ZhsFIjoM1SQ7WJKnpDzN7fwU1AyECxmVO4iCzEwc5EKpga7SRzlA5e0tqua+klillM3jLlA6m9LeR3LWdksE4ZDPQ3oppb4VgCDefx25ZBvWNGi9BREREJhwlEM5A3s6t0N+L61hsmBsAXLKxeQA0+vZhW5q5U0SKb8uruiW8WeGsj6ZuH01dUbY399NWm6C7IkV3RYqKeJAZ7eVM6ongeDaN6X4a0/30+kvYWtrI/nA1+3xl7POVUe5vYqpdw+xcnJJEL5HeDsL93diZNN6zT+A9+wTU1GMvWobdsgyrquaUXoeIiIjIWKUEwhnGGA/3tw8DsGvFZJIBl2DOT19qFjYuk5z9RY5QROT0Kk0FWL69jnn7KtnZNMC++jj9pRlentuJL2/R2B1lcmcpVbEgVblBzuvdzmJnL9uik9hVUs9AIMK6SYvY5maZnTjArPJaAm6W8EAPGKjevw26O/CefBTvyUexGidjLVqGvWgpVml5sS//zPfpT0NvL1RVwfe/X+xoREREJhQlEM4wZuMrcKAVHIf1C4NABn9sOuDQ4OzFb+WKHaKIyKiIZPy07Kphzv5K9jTE2F8fJxnKs68hzr6GOJGUj8mdpUzujFKSgWUDe1kYa2VnST3bSupJ+0OsL5/K5tJmpg92MscfIepmsJrqobsT03kQ+now7fsx7fvx/ue/obwSq34S1NRj+f1vKG7n4ref4lfiDPOrX0FbGzQ1FTsSERGRCUcJhDOIyeVwH38EgM6WKRyMZrA9i67kIjCGJmdvkSMUERl9wZzD3P2VzNlfQU9Zmv31cdqrB0mG82yd2sfWqX1UD4SY3FFKY08J8+LtNB7cTm/tVLaWNjIQKGF76SR2RBtoTvVwoddFY4Mfq6EJk81AV0chmRDrh4E+zEAfbN+MqawuDL5YU4vl6M+tiIiIjH96R3MG8X7/OPT1QGk56xdGgDgliQYG3TCz832Ew8lihygiUjQWFjWxMDWxMC07PQ5UD7K/rjBOQk95mp7yNOvdbhq6S6jZ52NqsotpqW46guVsLW3kYKiC/ZEaHqCGpnycszMdzGIAq2kKVtMUTDoFnQcxnQdgMAG93ZjebrBtTGUNVm09VNdg+d5YZYKIiIhIsSmBcIYwvd14v3scgPQ7rmB75WoAeuMtAKzIdNBWWrTwRETGFJ9nM7mrlMldpaQCeVrr4uyvS5CI5GirT9BWD5tyMep7IzT0Jrigr4+EHWVrdBL7IjW0+Upp85VS6aZZke1gYbYHfygMU6ZjTZmOGUwUqhI6D0A6BT2dmJ5OsKxCZUJtPVTXveFuDiIiIiLFoATCGcAYg/voz8DNY82Yw8ZpHl4aIqkyenK1TMonaHIHaSt2oCIiY1A462N2ayWzWivoj2bYV5egrSZOLuDRWp+gtT6B7UH1QJiGngNcmK5hq9XE2mAtfU6I34Sn8rtgI8uyXSzNdrGh7kDhxDPKwZQR7k9R1dpPZWs/4XhmuDLBszZiV1Rj1dQXujkEgsV9IURERERegxIIZwCzdSNm+2awHXjHu1mf+SUAydhCAM7OdKDZykVkong900PO21c1vG5hUZkIUREPMW19kFyDj47qJAerBxkM5+mqTNFVmWI9PdQm97AiVkouNZktZgYxO8SzoUZeCDZQk6ui2dlDxB4EyyJVGaGtMkJbSyOhgcPJhMhAujAIY18PbAdTXolVW4+JD2g2BxERERmTlEAY50wuW6g+AOzzL2Z76QDJZJJALsBAahrlXobZ+f6ixigiMt7YxqI6FqYmHmbBnioS4RwHq5J0VCXpK0vTFcnQFckA3ZRk1zMrUU1feho9uakcdCdz0J1Mld1Jk7OXCrsHayiLmy4P014epn3hJILxNEu25DFdHZCIDQ/AmP/WLViTp2HNX4w9vwWrouqEsYqIiIiMFiUQxjnvmd/CQB+UV2KvupS16YcK22OzAJuzMh3YxQ1RRGRcs7AoTQUobQswu62C2gqbPWWD7C6Ls790kMFAnsGqDqCDEu8FyFaSSU6jL9VIb24FYWuQRmcf9U47Pis/fN5MaQhrSm1hzIR0qjCbQ3cHxAYw+/dg9u8pTA1Z34g9ZwHW3IVYjc1Yln6ri4iISHEogTCOmZ4uvGefAMB5+3vosPvocDuwsYkPzido8rRke4ocpYjI2HXM7g4GsrksAX+WY/X/2jK09MV8TI2VkQrlSYRzJMN5cj4PQr34Q734eRmTj5BPNbI71cju1AXU2100OvsosRMjzmmFwjB5GtbkadjLz8XbvA6zeT1m3y7oaMfraIdnHoNoKdbs+dhzF2LNmIPlD5z6F0VERETkOJRAGKeMMbiP/BRcF2vWPKx5i1ib/DUAvvR08EIsyRwkgFfkSEVEzlw2FiVpPyVpP6bPkPN5DIbzDIZypEJ58CXxle7AV7oDY2x60rV0pSYRTk+myfSyuq4T2zIjz+nfDIv9sHg5TnIh5bsOULHzAJW7uyARx6x5AXfNC+DzYU2fjTVnYaFCoWyCjJtw9dXQ1weVlcWOREREZMJRAmGcMlvWY3ZuA8fBueK9DJpBtme3AxDrnY3PeJyV7ShylCIiE4eFRSDvEIg7VMaDeJYhGcyTL3EZCGbJ+DyccAdOuIM8sDtXwu5UA1UZH835GGEre9Q53UiQ3kXT6F00jXMCKzB7dmK2bcLbtgn6ezHbN2O2b8b7FTCpGXvOAuw5C2BSM5Z1hg6f+/d/X+wIREREJiwlEMYhk83gPvpzAOzz34pVXcv61Go8PJxsLSZbzbLsQaIm/xpnEhGR08U2FtG0H9J+KgiR9bkMhvIkwi6pUA7bPwj+nfQDfZ4PJ1VJWcpiupcjYPuPOp/l+LBmzoWZc7GveC90HsTbthGzbROmdR8caMU70Ir31P9AaRn27AVYcxZgTZ+lKSJFRETklFACYRzynn4MYv1QUYW96lLyJs/6zHoAUv1zCdgW52QOFjdIEREZIZB3CCQcKhPgWYZEME9/2Ec6ksJysnglXfSXwMuZLvxuJVVOOfWBUsJ26KhqAsuyoH4STv0kWHUZZjCO2bYZb9smzM6tEI/hvfwcvPwc2A7WlGlYM+diz5oH9Y1nbnWCiIiInFZKIIwzprsDb/VTADhXvAfLH2B7ZjMpk8JyI7jJyZzbECLc5xY5UhEROR7bWJSl/ZSlwfSFGfBX0BsOko3EsAMD5H29dNJLZw4cL0KFXclU/wEanIZjfvi3Skqxlr0Fe9lbMPlcoavD1o14O7dCX0/h+Z6deL99GEpKsWbOwZ41rzAQY0m0CK+AiIiIjEdKIIwjxvNw//tB8Fys2fOx5izEGMPazFoAsrHZ+CybvGfYFz+6L+1ApZIKIiJjjYVFRS5LRS6LN+CQbTyPjmyClN2FFerEtZP0kOTB+IP4TYQZ/pksDM2mydeEfYwpHS2fH2vWPJg1Dwcwvd14O7Zgdm7F7N4Bg3HMupdw171UOCBaBlXVWJU1UFaOZb/5aSKdi9/+ps9xXPPmQXs7NDbCli2v3V5EREROGSUQxhHv2Scx+/dAIIjzR+/Dsixac610up1gbPLxWUwt8eGzVZoqIjIe2ZbHpFKPSURwvWl0JuZyMJ0kZXdiR9rI2Um25tezNbEenwkxwz+D+aHZTPZNxrGcY57TqqrBectKeMtKTD6P+4v/xPR1Q28PDMYhEYNEDLNvNzg+TEUVVlU1VFZjhSOj/AqchEQC4vHCUkREREaVEgjjhOlox3viUQCcK96LVVGFMYbV6dUA5OMzKbHDTIron1RE5Ezg2IZJZWkmldnk3EaimbexLraHbms3TqSVvJNmW34T2xKbsI2fJnsai8JzmB6Yit86ehBGAMvnw6qswqqsghlgMhno68b09RQSCvkc9HRiejoBMKFwIZFQUQWVVVj+wGi+BCIiIjLG6NPmOGDyefIP/Vuh68LchVhLzwZgX34f7fl2MDa5gUVc1BAh5XpFjlZERE41v+OxrLqEZdULSeTms7k/xfr+ffTZhWSC50ux32xnf3I71qCPKjOZ+cHZLIxMJ2SHjnteKxiEhiashiaMMYVqhN4eTG83xAYgnYIDrZgDrQCYaClUVGFVVkN5BZajtxEiIiITif7yjwPek7+GjgMQKcF55wexLAtjDM+mngUgH5tDmR1laU2I1R3JIkcrIiKnU9Rvc3ZtCWfXzieWncPueJYt8VYOshsvvA/bN0iPtZvf5Xbzu36bEreJ6b6ZnBWdRYW/5LjntSwLSsuhtBxr6gxMPg8DfYXqhP5eGExAIg6JOKZ1L1gWpqwcq6IaKqsKx52C8RNERERk7FICYYzz9u3Ge/YJgELyIFoKwK7crsLYB56P3MBCLmuKaOwDEZEJpizgsKQ6zJLq2Rgzi4PJPBsT7ezO7yLh34PtjzHo288G9rM+/hRWpp7q6lImd4doGEwTMMcfXHdKaQCqa7GqawEw2Qz09WL6e6GvBzJpGOjHDPTD3p3gOJjySqzKakxHO9Q1YB1jkEcREREZv5RAGMNMNoP7s38HY7CWrMCe31LYbgyrU4WxD3KxuVT4IiyqDhYzVBEROUW8PTuOud3tGHjNY+uGHm8FMjSxpWQqWyoG6S7tgeAAhA7S03yQ7ibwMrX44w1U95VTN5inOpOgLJ/keB/5rUAQ6idh1U8qdHdIpwpTRPb3Ql9vYfyE3m5Mbzf5nVvJhUqINU4j1jidgcbppMtrYGgKylWTjl8JISIiImOXEghjmPc/vyh8y1NWgXPFe4e3b8tto8frwXh+8rH5rJocwTnGvOAiIjL+DGSPXRXwP5UHj9pWHjj2zAtHmpaHaX0+YnYd+/0WqUgSgnGcUBdeqIuuWujIVuImGzGpaYQyDlEGqPUPUGoP4Ldyw+c6p6NQjWBZFoQjEI5gNU7GGMPBzl6C8T5C8X78yTj+9CDVuzZSvWsjANlwlFjjdGKN0zHBhYXBGfW3S0REZFxRAmGM8rZvxnupUGXgvPfDWKFwYbvxeC71HAD5gfnUBCLMr1T1gYjIRHS8ZMOxuTSngbhDzikjFvaIRVxywRR2oA870AcVG3HdAL2pRrpTjbipFsImR6k1QKkdo9WxqHNTBBg5YK9lWeQipeQipSTqp9C27EKiXW2Ute+mrH03pR37CaQS1OxcT83O9eSf+W8oK8eaNgt72iys6bMKMz2IiIjImKYEwhhkkoO4//0gAPY5q7Cnzx7etzm7mX6vH+MGycfmsWpaBFvf4IiIyOvgd22qEzbVCR+u7WcwlCcRzpEMuXhOFl90D0T3YIyFm6mhN9VIV7KJndEKMFDtpal3k9S7SRrcJHXuyAF8jeMj3jCVeMNU2pZfjJXPUdq5fzihUNbZBrEBzLqXcNe9VDioogpr2szDCYWyihNeg8lkCoMMnybOxW8/becWEREZr5RAGIPch39amEqrpg770iuHt+dNnufTzxfWBxbSEAozp1xzcouIyBvneDZlyQBlyQAGQzrokgjlSEfypPwuTqgLJ9SFv/IVyIfJp5roT9fTk65nk1tdOIkxlEbSVOYSVGUHyWddSnz28OC+xucn1jiDWOMMAFbW+DH7dmP27MTs2YFp2w/9vZi1vbhr/1A4Z1XNUIXCzEJCIVpW2P697+G+uBqCqr4TEREZbUogjDHemucxG9eCZeNc9REsv39438bMRuJeHJMPk4/P5sIZEfUfFRGRU8bCIpzxEc74KE85ZGyXgWCO/mCWeDCH50vhK90BpYWBHn3ZEtx0PZnMJOLpOuL+WvZFaqE3C0DQsSjxWZT4bUp8NlG/RcC2sPwBrJlzYeZcAEwmPZRQ2FFIKhxoHR6Q0X250G2Pmrrh6gQTMFgBJdBFRERGmxIIY4jXuhf3Vz8BwL747diNk4f35UyOF9IvFNYHFtEcCTG91H/M84iIiLxZh8ZXCKb91OOn1jJUx8J0VSTpKU8zUJIlHxiEwC4C7ALAnwljJ6vJZBpJ5ZrIuBEyrqE3c3jMhICXp7Wtk1o3RZ2bpM5NUe2lcAD8AazZ82H6LOjvw/T3wUAvJOLQ3YnX3QkvPguAKYkWuj1UVEF55YiEu4iIiJweSiCMESYRw33wX8F1seYtwl51yYj96zLrSJokJl+CG5/JhbNKVH0gIiKjxjYW9X0R6vsiAGQdl97yNN3laXrKUwyUZMkFUxBsBVoJA6F0gMBgBSZTRTpXT9yrJ2v72WeXsc9XNnxux3hUe2lq3BS1booaX4ra2gjRmjoswORyMNBbSCj098JgYvhh2vYBYKKlRyQUKrB8SiiIiIicakogjAHGzeM+eB/EY1Bbj/Peq7GswzNxZ0yGF9MvApDrb2FaaYgpqj4QEZEiCrgODb0lNPSWAJBzXHrK0vSUFx790QzpUJZ0qBPoBLYQ8SCSDFOWLcdkqkjm6uh168hafjqdCJ1OZMTPCJr84aRCSSO19SnqNr2C3xeFTBJTVQb9fZAcLFQpJOKY1r0AmNKykQkFR295RERE3iz9NR0DvEd+htm/B4IhfP/r41jB0Ij9a9NrSZs0Xq4UNzGdC+dEjn0iERGRIvG7Dg19JTT0HUooePSWpektTdNfmqE/miHn90hEUyRIAQeBTQRdi0mpCOF0KXamknS2hn6vhj47TMby0eYrpc1XOvxz/s+XP0i48wDJ2npW//djVLtpalN9VPW24+vvIdfbiz+TKiTl4zHM/j0YLLIlpWSiFWRKK8hGyzG2A8CUUo2lICIicrKUQCgy76XVeC+tBiyc91+DVV07Yn/aS/Ny+mUA8v2LmVUeorFE1QciIjL6tkzpfd3HWEBlPEhFPEDO8cgEXbywR9KfZ9CXJ+8YOqODEB2kkFQAx7NoTgeJpiP4smXksxUk8zX0Uj583rxl84dgQ+FJZBpUL6PUy1KSTVIX76K5Zx91fQcoG+jCn00THIwRHIxBxz6MZZGNlJEpLcfU1UJZBZbjvPkXSERE5AynBEIReft34z78EAD2Je/Anj3/qDYvZV4iSxYvW4E7OJUL56n6QERExh8Li4DrEEg6lOcLH9YNhtn95XRGUnRG0nSG0/SEM+RtQ3ckTXckDRxOWoTyDj47BYDfyjLf2kYiV0OvHWXQDhC3A8RDAQ6GKlhXO3v4uNp4F7M6dzK5Zx+1ve2EsimCgwMEBwcwB/eBZWFKy6GistDloawcEREROZoSCEViYgOFcQ88F2vBYuyVlxzVZtAbZG16LQC5vsXMrwhSFx75T/Zc6rkRz9t9hamzEm/gWyIREZHRZGGxoyIGFJIDU+IlTI5HyDgeSV+elM8l5XdJ+fKkHY+0z8W1DAB5O8+eqX8AoCTroyETJJiNkEqW4ObLSbvVxKkk5QvRVVpLV2ktq2eeC8ZQnuxncs8+pnbtYXLPXkrSgxDrh1h/YTpJy6Jn+w7ik6YRa5xGvH4Knj/AqkklxXqpRERExgQlEIrA5POFGRcScahrwHnPh485o8KL6RfJk8fLVGNSzayapjcuIiIy/h2aIvJEHHxE8REFygMOLoa0z8VnCn8vbSx8rkXeMQwG8gwG8sAgVHcdPodrUZ8KEEiHsbMlePlyMvkqEqEaNkxZwoYpS0YkFJq79zGley+l6ThlHfso69hH09qn8SybeG0T+TlzsKfNxJo8DSsQPE2vjoiIyNilBEIRuI88VJh2KhQuDJp4jDchcS/O+sx6AHJ9S1hcHaIqpP6ZIiIy8QwnHLIWlldIIDiuxYy2clzLI+sfevhcsn6PnN8l53i4jiEWzUA0A/QDbYXzGKjK+AimQ/iyJXj+MjqjlexuvoAkf0RZKjacUJjcs4+yVIzyzv2Yzv24v/stnmXTW91I/6RpDDZOJz1pCuYYf8tVsSAiImcaJRBGmcnnMWtfAMvC+cC1WFU1x2z3QuoFXFzcdB2lXiOXNOlNiIiIyKs5xiactQlnR243GHK+kYmFrN8l6/PwHEMqlCcVSgAJoGP4uGjeJpgOcmBWI+35mazOLMU3APVtPUzp3D+cUKjpbqWmuxXW/w7XsumqnERX3VT6G6aSbJhCIBLGMwb7GBWGIiIi45USCKPM8vlwPngdBILYM+Ycs82AO8DGzEawwO1fwnunlxF07FGOVEREZPyysAjkHQJ5Bzg8e5HB4NpmOLGQOyKxkPN55H0e8WgK2AlBoASyVbB7Guw1AQLuAvypAKU9WeoO9DNt30Eauvpp6G2jobcNtjyLAbrK6nipqome+gb6G+uwKoNEgllKAllCgRz2EXmFc8Pnju6LIyIi8gYpgVAE9ryWE+5/MrEaYxnc1CQurp7OpIimbRQRETkVLCx8noUvYxPJjHwb5GEKCQWfS7yqnLyVxrUy5MlgLBfPypK220n7IV4G7dNh7fllWKaCcDpAab9LTWeCus4EFf39zD3YTWjPGiwgHiqlrbqZtqrJ7KlqIllbTjiYpySYpTyaoSroUBl0CDiqWBARkbFLCYQxZnt6N3u8rQDUZ89ixaRQkSMSERGZGGwsgjmHYM7BmAYoTPiAweCRxyVD1DSQsQZIWwNkrAEyVgxjeSTDaZJh6JgUZCOHx0MIZAwVfVkq+3KU97UxtX8PSzbkCQ9adJU20VbVzMtVcQ5UNpL3BSjxWVQGneGEwpEPJRdERKTYlEAYQ2JujEeTj4IFVmIOVzVPP+bsDCIiIhPV0w/9LZYBc5r/PJa17z7Oni5CQPnQM0MpOccbqlwodIfIDS3zPkM2aNHZEKSz4ehBFksSGcoGtlIzsJHpXS7kS0iZamL+RvZHJ7MuWgNHvA84MrlQEXQoD9iUBxzKgzZRn33Ue4ZXT/V8JM/zOFh5kEw6g23b6kYhIiInRQmEMSJv8vzXwC/wrCxeppr3lr+ViE/jHoiIiBzJLQkXO4QRLCwCrkPAdXj1cMfRgE3G55J2XNI+l7TPG153bcNg1Mdg1MeBpiOPGgAG8Gc30hRzCaX8mHyUpFdF3G6gLVNDazIKZmT3RseCskMJhaFlF1FC/jwhf46Az+Vkv5N45sDg63oNNNuEiMjEoQTCGPE/8SeJW90YN8gScznTy9R1QUREZDxLZD3IWjj4KME3IsHQ3ziZPBlcK0PEq6HS309/ppsYCRJBl1zAprfm0BcJh2aL2MehdwdOzsKXC+K6UdKmAuOWEMuXMJCNYFIlmHwEaBj+eRaGoD83nFAI+nJksh7hVIhwwMULacYIERF5bUogjAEb0pvY7m7EGChPrOLiqbXFDklEREROIxsfAXxgSpjkLmNV3eH0Qt7kGcj3E+vbS39sPwPpbgaIkwjkiUcdMmEH129w/WkgjY/uo3+AAeMFwQ3j5QsJhbwbJu5GiGUimGQY487g4H4/YPECPcMVDCnXI+RYBIceIcciYFvqVikiIkogFFtXvovHk48Xxj2ILeYDjfP0DYCIiMhxTLv/UXyJFPlomD3XXlHscE6Zo7sNhIF5UDIPpwSqgOp8jkh3F4G+VqxEOybbRc7ESEYgXuqQKPWRKHVIRH24PgvLyYCTwQn0H/fnGs/BuBFMPsKgGybhRjBWuLAtF4F8GOOGsbAJDCUTjkwsBB2LgaxLqd/W+xcRkQlACYQiyngZfhb/JcZycZONvKvifEoDTrHDEhERGbOm3/9rQp19pOsqz5gEQrvvpZNr6IPG2rMYrG08vM14hAZ6mdxzkMjeDsJ9XYT6uiDbR7LEYrDEKYy1UOKQiDoMRp3CeqmPbNDGsl0sOw7++HF/rDGAG8K4EVJuhGR+KMGQL8GkS9i4pQTcMGUBH2UBmzJ/YRyGsoAz9NymNGATdDS2k4jIeKcEQpEYY3g48WuSxPDyJSw0b2V2xdEjNIuIiMiZ5/izPJxY876BE+y1SFXWkaqsA89j8zyHst44pX1xGrvjRHcOUjIQw5/LA5DzHUowHJFYGBrYMV4WYDDqkAoBNuBLY/nSQO8xf7IxFmk3TCpfwsF8BJMtwSQLXSdMvgSTL8FnOQT9eUL+PEFfYTyGoD9P0Jcn4HO5ILpMSQYRkTFOCYQieTH9Evvc3RhjUzpwMZfOqC52SCIiInKmsG3iVWXEq8pGbjeGYCpLycAgvo4eqjN5orFBavoGmbw/RSgZ58iOCAZIRWwShyoZhhMNDrHyAIkyH4MlNp5tsHxJ8CWPG5JxfeTdEuL5woCPJhsZSjIUHi+43diWTcAujLngdyymDrQTNTlKvBwlJj+0zBE2eU5FqsG5+O2n4CwiIhOHEghF0Jpr5dnUs2CB6VvB+ydPx2er36CIiIic2JYpx64AOLbyozdZFplIkEw4SCIaoDMa5ciMgeV6hAdThBMpIvGhZSJFeDBN+WCa+q4MwWQSx/OGjzFAMmKTKPUNj8UQPzQew9C2dNjBcvJYzgAEjlNFYcCfdvBlAtjZIFYuxL5QCV4+St4rJWfqyToRXMcHxhA2LmGTJzyUUIiYPGFvaGkOL8NeYRnAG/Hj9sWztJ7klJWvd6rK1zMVpqbBFJHxRAmEUZbwEvwy8TBYhnxiOldULqMyqHEPREREpPiMY5MsKyFZVkLPcRsZ/JkcoWSGYCpNs+viS2fxpbKUprNUprM4bVl86SxOOokvncXL50hGrMOJheiRyYbC0nMscmGXXDgFpI75o8tSLiVxl5JBQygJoZRDIOXDyfixcwHIh8j6Qgz6g2R8AbK+IBl/iKwviOvzYzkOPp+NY9t4PpdsLIvftoYfPpvD6xanZOaJ5hcfP+F+tzTwps6vKgoRGU1KIIwi17j8Mv4wGVJ42QrmeqtYWBV67QNFRERExgrLIhcKEO5tI2vBLh8QBaIOhdkjwkcfYwy2a/DnXHw5F382T03cpaHHxZ9zcXIZ8o5HzmfIBCEdskiFLVIRm8ESm0TUIRu0SYcL1QxHJzdyQA7bTVAy6FKSyBONu1QkXKKJPCU9LtF4YT2S9LCAvO2Q8QXJ+oOFRIMvQNYfLGzzBcj4Q+T9QfKBEE+VhLFCEZxwGF8kgi8SIVgSJhLwE/ZZRHw2YacwU4WIyJlMCYRR9PvU7+nwDmA8P5GBi7l8ZkWxQxIREZEz1AkHajQQymUJxAIjujCcNpaF57PI+GwyYf9JH9YQcAq5gT7IG5c8LpNjIeL+LHF/jkQgTzzokgh5JIMGz7GIl/mIlx3/La7lGcJJl0jSI5J0iSRdwskUkeQgkUGXipRLeKCwPZgxJ3x5sk6AdCBEwh+ixx8k6w/hBsNEhhIP+UCYzkwey3bAsbEtG9u28NkWfjwCxj3511BEZAxQAmGUbM9uZ01mDQBu93lcNaVJWWoRERGRExjIvvoDtoOvq5RKoPJVezwMmYBLKpgnFcyTDuRpqxkk7/PIO4ceBmNbJKM+ktHX/vm2C4EshLIQTHsE0y6hZI5wKk8o7RFMe4QyeYLpGMG0S3nSI9Bv8Gc9HO/E53Ytm4w/RK8/SM4fIO8L4PkDGMcPfj+W48P2Ofh8Dn6nMA1m0Gfh+Hzg82FZozdjhfvkr0/r+dUNQ2T8UAJhFPS5fTw2+BgAC33LWDFtMRUa90BERETkdXs9A0nW9Y/sTmEw5B2DaxeSCYnqajxyeFaeoCklZyXJkSJvpXCtLJ4D6XDhQblNYU7Lk6ugsD3w5cCfNfhzhaRCIOsSyrgEst7Qc4M/l8efzRHIxgkMtfMnvOH1QNbgyx+uhDBDy6zjI+/4yb30B1xfgLw/iOcP4PqDmMBQMiIQZFp1FIIhrEAQAkEIFpaWPwCBAAwv/aOalBCR8UkJhFHwu9TvyJKlydfEJdGV2PrlLCIi8oYMzJ9KqqGKbGVpsUORccjCwu9a+F0bcuCZoWm0hz6VB83hmSsMHh75wsPK4+FicPHIEzE1uGTIWxlcK0Oew0tjFaomPBuyQcgGLQr9RGze6FtvyzP48gwlFA49DP6chz9rCOSy+LPpwrbsUPIh4+FPGNoPesPb/bnCum2O/XOMz491RFLhUJLBxAbAcQoPu7C0jlh/9b6j1h1HyQmRM4QSCKNgmn8aBsOlkUuVPBAREXkTXr7js8UOQSYICxuHAA6Bw1/7D2nMn3Xc4wweLjk8crhWjkTfr/Fsg2eZoSUjnvt84FqF7a5lcG0z9LywxAJjW+QCkAs4JHnzVaxO3gwlHrzhaohDzw9VS/hzafy5ZOF5aChZkRtaJo98PrJC4rivi2UdN9FgOg8OJyzarW48v6/wCDi4h9b9Dp7fhxvw4Qb8uEE/KyouwHL0cUZkNOn/uFHQEmyhJdgyYtup6kvm1XeNeF52VF9BERERETmWEw40eQIJTv648Gu83S4PHD8hYDicWDiUUIi5eTyb4QREqrwSz/IwQxUShcoJF2N5GOPh4WGswj6sQibE9Vm4Pos0p+iLLQO+nCl02cgd2f3CHU40BLKHEw6Fbd5Q9w2Dv3/L8L6yoXYnk5TI89/g80EwVOimMbQkGIRQGOMPUN8/gEn2k9+/G5zC+BHDy+H1N18hoXEcZKJQAmEc2BfPHnffQKUSBiIiIiJnIgsLx4BjDn+UzmdHfqy2y+qPqpA4HoN3OMHAUNJhKPngDT13jcEzh1oaDB5WPo0ZSlgYyyuMLmm5hWUhUPIBi3wA0qegQmIoWJx8YRwJX/5w4iGQ9QhkXIJZ94hkhCGQTRPIJglkDYGER6C30F2jLOuR37XxtQe1tB08n7/w8PvB58f4/IUBK30+LKewdHyFwS0tnw9raD+qgpAJRHe7iIiIiEiRHD3TxOljYWNhYx/5EeBYyYdXffVf1nXsiouygI1nQR5DzrKHH3nLIo9F3gbXsnCPqKIoPDw8e6gywj6ckDBWHsvOD8fg+guPDEeOI/HG2O7hQS0DWY9gxiOYyQ8NamkIZA6PLRHIpglkhpIR6cJ2J2/wuQbyBryjX7rB1U+THZq+MxcIDS3DuMEQbiCEFyxM8UkojBcKwdA6wTB2OITPdnBs8FkW23KbsW2DbXnYlik87MLSsTwsC6yhf6Nzw+e+4ddE5I1QAkFERETGjeV/djuBvjjZylKNhyDCG++GcSrEskd+re9i4xIEgq/rLDaxxpmFT+MGjGcOV0oc6oJhCnUThyoivKH9IVOBR46yoCFHlhxZ8iZLnsMPzyokJDwHsg5kQxaDOIDDyc6ocSxO3gwnFZy8wXGPWPcyWF4Gy/RjGYYftmfADOVLBg1WYmifZ/AsB89yMJaNZ/lwh567lg/XdnBt39C6D9f249o+PMfhJee3GMePbdnYloWDPbQONjaOZeHgw2f5iLlx7KH9hXbOUJuhYy2wLYNlGRYE5+LY4FgWPgscu7DfsazhRIdtgc+2cCxNTT+RKIEgIiIi40b55r2EOvtI11UWOxQROUXeaBJk3r5Xf5QJDD0KjIH4YJxgeYRtU/oKg1ceGqTSNiMHtxxad20zYoyJQw/zqs/Ih8aROH5H4zfLHXqc3E84FXUsxnPAOBjPz470GoznAy+A8fzg+TFmaOkdsTQB8Pw4+PERwDEB/LaDz7Lw2Ra+oWSDzy4kIgrbTmLdGjr2BOs2YCl5MeqUQBiDnn/VwIga50BEREREZKQtU3pP3MBANpcl4M8Vpu58E2+pDaZQJGGBsQoJhekHywoJh+HEw+Floe3QLBrAgepEocuDdUTXB8uAZ7A9D9szWCPWvcJzU1hapjD9pmUMeF5haQpnMnZhlgtjgWcBloVnHd7uOhY5v0XeN/QYXrdxfYc/gFt2IWlhOVlg8A29Ti6QN/arEg0BMEc8zx8vITGy3Wt1WbFgZILiiPVD1RI+m+HqiqOqJywL+4gqC/uI9r6h9oeqLA61d15djXHEeZ/rSBY625xEUmPVpJI39PqOBUogiIiIiIiInICFVRga4lAWAYimAic6ZIRkKPd6fyDHGo/SvGppux7+nItv6OHPecPrh7flh9a9oeeH91nG4PoOJxdyfptswCIbOGIZtMkGbHIBi0zAJhewyQQtcgF7RLtcoPCB37I8cDJYTub1XfOrXwLXwvZs8GysIx6HnmNsLM8Zel6onsh7Nq5rYRjaP5RVsYxdmITE2FgeWMYq7DuiiwmYoTYW1tC6PZy8McMJHRsPzFByx3jYQ/sZXi+0sQ6tG4PNEe2BlyyDhYfjFZaFfQytGyzjYTGUPBpaHjo/eDC0Hc/gWTYsuvJNvdavhxIIIiIiIiIir9NrVkCMAs+xyTo22dAbG8/Bdg8nHDIV9fgzefzZHP5MDn82RyCdozSXx5fM4xtKRDi5w+u+fB4nl8efc/EsyPkPJR9sssEj1l9HMiLvLyQijGNwnUNdOSYam5MdNNTxHM7bc1qDGUEJBBERERERkQloRALCHcD1QdoHRKDwUfEkPy4ag+0afHkXx/Vw8ocfh7pl+F2DCVUTdF3spIcT97BdD9stHGMPPSzPxbM8XNsrjEXhgFsoPBjqJsLwNtcBbD+uM7Tu5YbagWdbw0v30NJncG1reB/WUPcPhgpLrENFJtbQviMeQ1UoptBsuEtKoYLhyG4LI7swmEPbrMLYGkfuNyPaW0PFLUc8f/X+V7UzWIVqjFE0JhMIDzzwAP/yL/9CV1cX8+bN46//+q9ZvHjxcds/8sgj3HHHHbS1tTFt2jQ+//nPc9FFFw3vN8Zw55138p//+Z/EYjGWL1/OV7/6VaZNmzbcpr+/n6997Ws88cQT2LbN5Zdfzl/91V9RUjJ++6eIiIiIiIicdpaF57PI+k78YTbWOPW0hnHkgJxHfYdvgNfZk2SsKA8coz/LEOON7kCSYy6B8PDDD3Prrbdy8803s2TJEu677z5uuOEGHn30Uaqrq49q//LLL3PTTTfx53/+57z1rW/lF7/4BZ/5zGf46U9/ypw5cwC4++67uf/++/nGN75Bc3Mzd9xxBzfccAMPP/wwwWBhopnPf/7zdHV1ce+995LL5fjLv/xLvvzlL/MP//APp+S6nks9N7y+N5GlrPLg8RufvuFcRUREREREiqKY046OZwPZ43fjsDyb0CjGMrr1Difh3nvv5UMf+hDvf//7mTVrFjfffDOhUIif/OQnx2z/wx/+kFWrVvGJT3yCmTNn8tnPfpYFCxbwox/9CChUH/zwhz/kxhtv5LLLLmPevHncdtttdHZ28thjjwGwc+dOnnnmGb7+9a+zZMkSVqxYwZe+9CV+9atf0dHRMWrXLiIiIiIiIjJWjakKhGw2y8aNG/n0pz89vM22bc4//3zWrFlzzGPWrl3L9ddfP2LbypUrh5MDra2tdHV1cf755w/vLy0tZcmSJaxZs4Yrr7ySNWvWUFZWRktLy3Cb888/H9u2WbduHW9729teM3YzNI1KNnvs8gEv5w2vWy6j3ldF5FQo9No6NKptsaMROT10n49tbjiCW5LFDUf0t/RN0H0uE4Huc5kQzNCgk2Z0bvIxlUDo6+vDdd2juipUV1eza9euYx7T3d1NTU3NUe27u7sB6OrqGt52vDbd3d1UVVWN2O/z+SgvLx8+/rV4XiFBsHXr1mPuDxIcXp9JEFhwUucVGXNswHvNViLjm+7zMWvLT34+vF6rf6M3R/e5TAS6z2WCOPR59HQbUwmE8czn89HS0oJt21jW6A5kISIiIiIiIhOPMQbP8/D5Ruej/ZhKIFRWVuI4Dj09PSO29/T0HFVlcEhNTc1wJcGx2tfW1g5vq6urG9Fm3rx5w+fo7R05j2s+n2dgYGD4+Ndi2zaBQOCk2oqIiIiIiIiMN2Oq82AgEGDhwoWsXr16eJvneaxevZply5Yd85ilS5fy3HPPjdj27LPPsnTpUgCam5upra0dcc5EIsErr7wyfM5ly5YRi8XYsGHDcJvnnnsOz/NOOH2kiIiIiIiIyEQxphIIAB//+Md58MEHeeihh9i5cydf/epXSaVSvO997wPgC1/4woipFa+77jqeeeYZfvCDH7Bz506+/e1vs2HDBq655hoALMviuuuu46677uK3v/0tW7du5Qtf+AJ1dXVcdtllAMycOZNVq1bx13/916xbt46XXnqJr33ta1x55ZXU19eP/osgIiIiIiIiMsaMqS4MAH/0R39Eb28vd955J11dXcyfP5977rlnuEvCgQMHsO3DeY/ly5fzzW9+k9tvv51vfetbTJs2je9+97vMmTNnuM0nP/lJUqkUX/7yl4nFYpx11lncc889BIOHBzb85je/yde+9jU+9rGPYds2l19+OV/60pdG78JFRERERERExjDLjNZ8DyIiIiIiIiIybo25LgwiIiIiIiIiMvYogSAiIiIiIiIir0kJBBERERERERF5TUogiIiIiIiIiMhrUgLhFHjggQe45JJLaGlp4YMf/CDr1q0rdkgiJ+373/8+73//+1m2bBnnnXce/+f//B927do1ok0mk+Hmm2/mnHPOYdmyZfzf//t/6e7uHtGmvb2dT33qUyxZsoTzzjuPv/u7vyOfz4/mpYiclH/+539m7ty5/M3f/M3wNt3jcqbo6Ojg85//POeccw6LFy/mXe96F+vXrx/eb4zhjjvuYOXKlSxevJjrr7+ePXv2jDhHf38/N910E8uXL2fFihX85V/+JYODg6N8JSLH5rout99+O5dccgmLFy/msssu47vf/S5Hjguv+1zGmz/84Q/88R//MStXrmTu3Lk89thjI/afqnt6y5YtfOQjH6GlpYWLLrqIu++++3XHqgTCm/Twww9z66238pnPfIaHHnqIefPmccMNN9DT01Ps0EROygsvvMBHP/pRHnzwQe69917y+Tw33HADyWRyuM3f/u3f8sQTT3D77bdz//3309nZyZ/8yZ8M73ddl09/+tPkcjl+/OMf841vfIOHHnqIO++8sxiXJHJc69at48c//jFz584dsV33uJwJBgYGuPrqq/H7/dx999386le/4otf/CLl5eXDbe6++27uv/9+vvrVr/Lggw8SDoe54YYbyGQyw20+//nPs2PHDu69916+973v8eKLL/LlL3+5GJckcpS7776bf//3f+fLX/4yDz/8MJ///Oe55557uP/++0e00X0u40kymWTu3Ll85StfOeb+U3FPJxIJbrjhBhobG/npT3/KF77wBb7zne/wH//xH68vWCNvygc+8AFz8803Dz93XdesXLnSfP/73y9iVCJvXE9Pj5kzZ4554YUXjDHGxGIxs3DhQvPII48Mt9mxY4eZM2eOWbNmjTHGmCeffNLMmzfPdHV1Dbf5t3/7N7N8+XKTyWRGNX6R40kkEubyyy83v//9780111xjvv71rxtjdI/LmePv//7vzdVXX33c/Z7nmQsuuMDcc889w9tisZhZtGiR+eUvf2mMOXzvr1u3brjNU089ZebOnWsOHjx4+oIXOUmf+tSnzF/8xV+M2PYnf/In5qabbjLG6D6X8W/OnDnmN7/5zfDzU3VPP/DAA+bss88e8b7l7//+783b3/721xWfKhDehGw2y8aNGzn//POHt9m2zfnnn8+aNWuKGJnIGxePxwGGv7HasGEDuVxuxH0+c+ZMGhsbWbt2LQBr165lzpw51NTUDLdZuXIliUSCHTt2jF7wIidwyy23cNFFF424l0H3uJw5Hn/8cRYtWsSf/umfct555/He976XBx98cHh/a2srXV1dI+710tJSlixZMvy+Zc2aNZSVldHS0jLc5vzzz8e2bXXRlDFh2bJlPPfcc+zevRsolGS/9NJLXHjhhYDucznznKp7eu3ataxYsYJAIDDcZuXKlezevZuBgYGTjsf3Zi9oIuvr68N1Xaqrq0dsr66uPqoPuch44Hkef/u3f8vy5cuZM2cOAN3d3fj9fsrKyka0ra6upqura7jNkR+sgOHnh9qIFNOvfvUrNm3axH/9138dtU/3uJwp9u/fz7//+7/z8Y9/nD/+4z9m/fr1fP3rX8fv93PVVVcN36vHet9yaMyP7u5uqqqqRuz3+XyUl5frXpcx4VOf+hSJRIJ3vOMdOI6D67p87nOf493vfjeA7nM545yqe7q7u5vm5uYRbQ69l+nu7h7R3e1ElEAQkWE333wz27dv59/+7d+KHYrIKXPgwAH+5m/+hh/84AcEg8FihyNy2hhjWLRoEX/+538OwIIFC9i+fTs//vGPueqqq4ocncip8cgjj/CLX/yCf/iHf2DWrFls3ryZW2+9lbq6Ot3nIqNAXRjehMrKShzHOWrAxJ6enqO+qRIZ62655RaefPJJ7rvvPhoaGoa319TUkMvliMViI9r39PRQW1s73ObVI9Yfen6ojUixbNy4kZ6eHt73vvexYMECFixYwAsvvMD999/PggULdI/LGaO2tpaZM2eO2DZjxgza29uH9wMnfN9SU1NDb2/viP35fJ6BgQHd6zIm3HbbbXzqU5/iyiuvZO7cubz3ve/lYx/7GN///vcB3edy5jlV9/SJ3su8ns+uSiC8CYFAgIULF7J69erhbZ7nsXr1apYtW1bEyEROnjGGW265hd/85jfcd999TJ48ecT+RYsW4ff7R9znu3btor29naVLlwKwdOlStm3bNuIX27PPPks0GmXWrFmjch0ix3Puuefyi1/8gp/97GfDj0WLFvGud71reF33uJwJli9fPtwv/JA9e/bQ1NQEQHNzM7W1tSPu9UQiwSuvvDL8vmXZsmXEYjE2bNgw3Oa5557D8zwWL148ClchcmLpdBrLskZscxxneBpH3edypjlV9/TSpUt58cUXyeVyw22effZZpk+fftLdF0BdGN60j3/843zxi19k0aJFLF68mPvuu49UKsX73ve+YocmclJuvvlmfvnLX/JP//RPlJSUDPeTKi0tJRQKUVpayvvf/36+8Y1vUF5eTjQa5etf/zrLli0b/nC1cuVKZs2axRe+8AX+3//7f3R1dXH77bfz0Y9+dMRALSLFEI1Gh8f0OCQSiVBRUTG8Xfe4nAk+9rGPcfXVV/O9732Pd7zjHaxbt44HH3yQW265BQDLsrjuuuu46667mDp1Ks3Nzdxxxx3U1dVx2WWXAYUBRFetWsVf//Vfc/PNN5PL5fja177GlVdeSX19fTEvTwSAt771rXzve9+jsbFxuAvDvffey/vf/35A97mMT4ODg+zbt2/4eWtrK5s3b6a8vJzGxsZTck+/613v4rvf/S5/9Vd/xSc/+Um2b9/OD3/4Q/7iL/7idcVqmUPpOnnDfvSjH/Ev//IvdHV1MX/+fL70pS+xZMmSYoclclLmzp17zO233nrrcCIsk8nwjW98g1/96ldks1lWrlzJV77ylRFlfm1tbXz1q1/lhRdeIBwOc9VVV3HTTTfh8ylPKWPPtddey7x58/irv/orQPe4nDmeeOIJvvWtb7Fnzx6am5v5+Mc/zoc+9KHh/cYY7rzzTh588EFisRhnnXUWX/nKV5g+ffpwm/7+fr72ta/x+OOPY9s2l19+OV/60pcoKSkpxiWJjJBIJLjjjjt47LHH6Onpoa6ujiuvvJLPfOYzwwld3ecy3jz//PNcd911R22/6qqr+MY3vnHK7uktW7Zwyy23sH79eiorK7nmmmv41Kc+9bpiVQJBRERERERERF6TxkAQERERERERkdekBIKIiIiIiIiIvCYlEERERERERETkNSmBICIiIiIiIiKvSQkEEREREREREXlNSiCIiIiIiIiIyGtSAkFEREREREREXpMSCCIiIiIiIiLympRAEBERmeCef/555s6dy6OPPlrsUE5Kd3c3f/qnf8o555zD3Llz+dd//ddih/S6XXLJJfx//9//V+wwREREXhclEEREREbBT3/6U+bOnUtLSwsdHR1H7b/22mt55zvfWYTIxp9bb72VZ555hk996lPcdtttrFq16qg2ruuyfPlybrzxxqP2/eu//itz587li1/84lH77rjjDubOncvu3btPS+wiIiLjmRIIIiIioyibzfLP//zPxQ5jXHvuuee49NJLueGGG3jPe97DzJkzj2rjOA5Lly5lzZo1R+17+eWX8fl8vPzyy8fcV11dzfTp009L7CIiIuOZEggiIiKjaP78+Tz44IPHrEI40yWTyVNynp6eHsrKyl6z3fLly+nr62Pnzp0jtr/88stcccUV7Nu3j66uruHt+XyedevWsXz58jcd46m6VhERkbFECQQREZFR9OlPfxrP87j77rtP2K61tZW5c+fy05/+9Kh9c+fO5dvf/vbw829/+9vDZfef//znOeusszj33HO5/fbbMcZw4MABbrzxRpYvX84FF1zAD37wg2P+TM/z+Na3vsUFF1zA0qVL+eM//mMOHDhwVLtXXnmFG264gbPOOoslS5ZwzTXX8NJLL41ocyimHTt2cNNNN3H22WfzkY985ITXvH//fv70T/+Ut7zlLSxZsoQPfehDPPnkk8P7D3UDMcbwwAMPMHfuXObOnXvc85111lkAIyoN9u/fT1dXF9dccw3BYHDEvs2bN5NMJoePA1i9ejUf+chHWLp0KStWrODGG288KiFxoms1xvBP//RPXHjhhSxZsoRrr72W7du3HxVrLpfjO9/5DpdffjktLS2cc845XH311fz+978/4WsmIiIympRAEBERGUXNzc285z3vOS1VCJ/73OcwxnDTTTexZMkS7rrrLu677z4+/vGPU19fz+c//3mmTJnC3/3d3/GHP/zhqOPvuusunnzyST75yU9y7bXX8uyzz3L99deTTqeH26xevZqPfvSjDA4O8id/8id87nOfIxaL8bGPfYx169Yddc4/+7M/I5VK8bnPfY4PfvCDx429u7ubD3/4w/zud7/j6quv5nOf+xyZTIYbb7yR3/zmNwCcffbZ3HbbbQBccMEF3HbbbcPPj2Xp0qX4fL4RyY2XXnqJSCRCS0sLixYtGpFAOLR+KIHw7LPP8olPfIKenh7+5E/+hOuvv541a9Zw9dVX09raelLXescdd3DHHXcwb948vvCFLzB58mT+9//+30dVKHznO9/hO9/5Dueccw5f/vKX+eM//mMaGxvZuHHjca9PRERktPmKHYCIiMhEc+ONN/Lzn/+cu+++my996Uun7LyLFy/mlltuAeB//a//xSWXXMI3vvEN/vzP/5xPfepTALzzne9k1apV/OQnP+Hss88ecfzAwAAPP/ww0WgUgAULFvDZz36WBx98kOuuuw5jDF/96lc555xzuOeee7AsC4APf/jDXHnlldx+++1HVTfMmzePf/iHf3jN2P/5n/+Z7u5uHnjgAVasWAHABz/4Qd797ndz6623cumllzJ58mQmT57MF77wBaZNm8Z73vOeE54zHA4zf/78EQmEl19+mZaWFnw+H8uWLeP5558f3vfSSy8RDodZsGABALfddhvl5eX8x3/8BxUVFQBcdtllXHXVVXz729/m7/7u7054rb29vdxzzz1cfPHFfO973xt+vf7xH/+R733veyOOffLJJ7nooov42te+9pqvlYiISLGoAkFERGSUTZ48mXe/+908+OCDdHZ2nrLzfuADHxhedxyHRYsWYYwZsb2srIzp06ezf///397dhETZ9XEc/3Vrmfk+ZYGVI6WIJEOTWMwstF1BKaXVRClBYdoL4SZbtIigTbRTiBIRhCG0oiyIkigwmEDSjIS0jFSejF5oJCenmjCfhcyFV+Pd2Nx2+8Dz/YCL83Jd55zZ+b/O+Z//hDy/bds2I3ggSZs3b1Zqaqra29slTW7xHxwcVFFRkUZGRuT1euX1euX3++VwOPTo0SP9+PHD9M7du3fPaO7t7e2y2WxG8ECS4uLi5HK5NDw8rJcvX87sR/hJXl6eKddBd3e37Ha7pMkcCb29vfry5YukyeCCzWZTdHS03r9/r97eXm3fvt0IHkiTQQKn02n8Jr9a68OHD/X9+3eVlZUZwQNJ2rdvX8iziYmJ6u/v1+DgYETrBADg30AAAQCAOXD48GGNj4/P6o0MaWlppnJCQoJiYmJksVhC6kdHR0Oet1qtpvK8efNktVo1PDwsScY/tydOnJDD4TD9XblyRYFAQD6fz/SOFStWzGjub968mfbmg1WrVhntkZiaB2F0dFT9/f1GkkS73W4kTgzmRgj2D4433ZxWr16tkZGRkGMIP681+I6MjAxTvcViUVJSkqnu2LFj8vl82rRpk4qKinT27Fn19fVFtGYAAP4UjjAAADAHpu5CCB4vmGrqF+upxsfH//adf/0V+l0gKipq2r4TExMznGnoMzU1NcrJyZm2z6JFi0zlmJiY3x5nNgUDAl1dXVq4cKEkGTsQLBaLMjIy1NXVZSSLnJpA8Xf9k7Xm5+fr7t27unfvnjwej65evaqmpiadPn36l7kjAAD4NxFAAABgjhw6dEg3b96c9kaG4Bfqn3cKRPolfiaGhoZM5YmJCQ0NDRk3HaxcuVKSFB8fL6fTOatjp6WlaWBgIKT+1atXRnskFi9ebAQJYmNjlZmZaboC0m636/Hjx3r79q2ioqK0du1a03h/N6eUlJSQYMl0a5Imd24EfztpMjfCp0+fQvonJyertLRUpaWlGhsbU1lZmerq6gggAAD+Z3CEAQCAOZKenq7i4mK1tLQYZ/SD4uPjlZKSos7OTlP9pUuX/th8Wltb9fnzZ6N8584dffjwQQUFBZKk3Nxcpaenq7GxUWNjYyHPe73eiMcuLCzU06dP1d3dbdT5/X5dvnxZy5cvV2ZmZsTvXrdunfr6+uTxeIzdB0F2u11PnjxRV1eXsrOzjRwQS5cuVU5OjlpbW01BnBcvXsjj8aiwsDDsuE6nU/Pnz5fb7Tbt+GhqagrpOzIyYirHxcUpPT1dgUDgt9YKAMCfxA4EAADmUFVVlW7cuKGBgQFlZWWZ2nbu3Kn6+nqdPHlSubm56uzsnPaL+GxJSkrSnj17VFJSoo8fP6qpqUlWq1W7du2SNHlE4syZM6qoqNDWrVtVUlKiZcuW6d27d+ro6FB8fHzI7QIzdfDgQd26dcu4QjIpKUmtra16/fq16urqpj2eMVN5eXm6du2aenp6tHfvXlOb3W6Xz+eTz+dTeXm5qa2mpkYVFRVyuVzasWOHvn79KrfbrYSEBB09ejTsuBaLRfv379fFixdVWVmpwsJCPXv2TA8ePFBKSoqp75YtW7R+/XqtWbNGycnJ6unpUVtbm8rKyiJeNwAAs40AAgAAc8hqtaq4uFjXr18PaTty5Ii8Xq/a2tp0+/ZtFRQUqKGhQQ6H44/MpaqqSs+fP1d9fb3GxsbkcDh06tQpxcbGGn02bNiglpYWnT9/Xm63W36/X6mpqbLZbHK5XBGPvWTJEjU3N+vcuXNyu9369u2bsrOzdeHCBW3cuPEfrWtqXoOfdyBkZWUpMTFRo6OjRnLFIKfTqYaGBtXW1qq2tlbR0dHKz8/X8ePHTUcSfqW6uloLFixQc3OzOjo6ZLPZ1NjYqMrKSlO/8vJy3b9/Xx6PR4FAQGlpaaqurtaBAwciXDUAALNv3kQkWZQAAAAAAMD/FXIgAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsAggAAAAAACAsP4L6TnxuvsEub4AAAAASUVORK5CYII=\n","text/plain":["<Figure size 1200x700 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["sns.set_style(\"whitegrid\")\n","plt.figure(figsize=(12, 7))\n","\n","sns.histplot(df_train['word_count'], color='skyblue', label='Train', kde=True, stat=\"density\", linewidth=0)\n","sns.histplot(df_val['word_count'], color='salmon', label='Validation', kde=True, stat=\"density\", linewidth=0)\n","sns.histplot(df_test['word_count'], color='lightgreen', label='Test', kde=True, stat=\"density\", linewidth=0)\n","\n","plt.axvline(x=512, color='red', linestyle='--', linewidth=2, label='Truncation Threshold (512)')\n","\n","plt.title('Distribution of Review Lengths (Word Count)', fontsize=16)\n","plt.xlabel('Number of Words', fontsize=12)\n","plt.ylabel('Density', fontsize=12)\n","plt.legend()\n","plt.xlim(0, 1000)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RcX1As9rd64N"},"source":["# Section 2: Kim CNN - Baseline Model"]},{"cell_type":"markdown","metadata":{"id":"x0Z1AfLPgH5a"},"source":["### Experiment 1: Kim CNN (Static)\n","\n","This first experiment establishes our primary baseline, replicating the \"static\" model from Kim (2014).\n","\n","We are using the `configs/cnn_baseline.yaml` file, which sets **`freeze_embed: true`**. The model will use the pre-trained 300d GloVe embeddings *without* updating them. Key parameters include `dropout: 0.5`, `batch_size: 50`, and `learning_rate: 1.0` for the Adadelta optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151215,"status":"ok","timestamp":1761363219700,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"_7uzOrzigJte","outputId":"1f96fa92-6ce7-403c-960b-7d8f9dd4fe65"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 03:31:14.401016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 03:31:14.418874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761363074.440101   36040 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761363074.446508   36040 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761363074.463123   36040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761363074.463158   36040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761363074.463161   36040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761363074.463164   36040 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 03:31:14.468100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_baseline.yaml\n","[INFO] No seed provided, running with system randomness.\n","[INFO] Using device: cuda\n","[INFO] Saving best model to: outputs/models/cnn_baseline.pt\n","[INFO] Saving metrics to: outputs/metrics/cnn_baseline_metrics.json\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 20626.67it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:14, 27084.55it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 360,902 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:09<00:00, 48.46it/s]\n","Train Loss: 0.4687 | Train Acc: 0.7735\n","Evaluating: 100% 50/50 [00:00<00:00, 106.49it/s]\n","Val/Test Loss: 0.3985 | Acc: 0.8148 | F1: 0.8398\n","[INFO] New best model saved to outputs/models/cnn_baseline.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:06<00:00, 64.46it/s]\n","Train Loss: 0.3650 | Train Acc: 0.8424\n","Evaluating: 100% 50/50 [00:00<00:00, 105.65it/s]\n","Val/Test Loss: 0.3010 | Acc: 0.8696 | F1: 0.8759\n","[INFO] New best model saved to outputs/models/cnn_baseline.pt\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:06<00:00, 64.74it/s]\n","Train Loss: 0.3106 | Train Acc: 0.8705\n","Evaluating: 100% 50/50 [00:00<00:00, 106.66it/s]\n","Val/Test Loss: 0.3255 | Acc: 0.8640 | F1: 0.8541\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:06<00:00, 70.36it/s]\n","Train Loss: 0.2776 | Train Acc: 0.8868\n","Evaluating: 100% 50/50 [00:00<00:00, 105.53it/s]\n","Val/Test Loss: 0.2862 | Acc: 0.8812 | F1: 0.8867\n","[INFO] New best model saved to outputs/models/cnn_baseline.pt\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:06<00:00, 64.86it/s]\n","Train Loss: 0.2369 | Train Acc: 0.9033\n","Evaluating: 100% 50/50 [00:00<00:00, 107.55it/s]\n","Val/Test Loss: 0.3893 | Acc: 0.8492 | F1: 0.8648\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:06<00:00, 64.50it/s]\n","Train Loss: 0.2061 | Train Acc: 0.9187\n","Evaluating: 100% 50/50 [00:00<00:00, 106.61it/s]\n","Val/Test Loss: 0.3135 | Acc: 0.8688 | F1: 0.8750\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:06<00:00, 70.56it/s]\n","Train Loss: 0.1757 | Train Acc: 0.9300\n","Evaluating: 100% 50/50 [00:00<00:00, 107.80it/s]\n","Val/Test Loss: 0.3195 | Acc: 0.8756 | F1: 0.8732\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:05<00:00, 78.19it/s]\n","Train Loss: 0.1523 | Train Acc: 0.9401\n","Evaluating: 100% 50/50 [00:00<00:00, 107.27it/s]\n","Val/Test Loss: 0.3185 | Acc: 0.8792 | F1: 0.8795\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:06<00:00, 64.42it/s]\n","Train Loss: 0.1365 | Train Acc: 0.9500\n","Evaluating: 100% 50/50 [00:00<00:00, 106.43it/s]\n","Val/Test Loss: 0.3379 | Acc: 0.8772 | F1: 0.8769\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:05<00:00, 77.87it/s]\n","Train Loss: 0.1267 | Train Acc: 0.9518\n","Evaluating: 100% 50/50 [00:00<00:00, 107.24it/s]\n","Val/Test Loss: 0.4542 | Acc: 0.8472 | F1: 0.8313\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:05<00:00, 77.80it/s]\n","Train Loss: 0.1187 | Train Acc: 0.9556\n","Evaluating: 100% 50/50 [00:00<00:00, 104.67it/s]\n","Val/Test Loss: 0.4419 | Acc: 0.8620 | F1: 0.8524\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:05<00:00, 78.13it/s]\n","Train Loss: 0.1029 | Train Acc: 0.9631\n","Evaluating: 100% 50/50 [00:00<00:00, 107.05it/s]\n","Val/Test Loss: 0.3903 | Acc: 0.8748 | F1: 0.8732\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:06<00:00, 64.94it/s]\n","Train Loss: 0.0894 | Train Acc: 0.9689\n","Evaluating: 100% 50/50 [00:00<00:00, 107.18it/s]\n","Val/Test Loss: 0.3974 | Acc: 0.8792 | F1: 0.8814\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:05<00:00, 78.00it/s]\n","Train Loss: 0.0876 | Train Acc: 0.9688\n","Evaluating: 100% 50/50 [00:00<00:00, 107.65it/s]\n","Val/Test Loss: 0.3966 | Acc: 0.8788 | F1: 0.8794\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:05<00:00, 77.54it/s]\n","Train Loss: 0.0800 | Train Acc: 0.9725\n","Evaluating: 100% 50/50 [00:00<00:00, 105.58it/s]\n","Val/Test Loss: 0.4210 | Acc: 0.8760 | F1: 0.8745\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","[INFO] Loaded best model from outputs/models/cnn_baseline.pt\n","Evaluating: 100% 500/500 [00:05<00:00, 87.38it/s] \n","Val/Test Loss: 0.2979 | Acc: 0.8729 | F1: 0.8776\n","===== Final Test Results (Random Seed) =====\n","Test Loss: 0.2979 | Test Acc: 0.8729 | Test F1: 0.8776\n","[INFO] All metrics saved to outputs/metrics/cnn_baseline_metrics.json\n"]}],"source":["!python run_experiment.py --config configs/cnn_baseline.yaml"]},{"cell_type":"markdown","metadata":{"id":"52xWMFo9jabg"},"source":["### Experiment 2: Kim CNN (Non-Static)\n","\n","To fully address our original methodology, we will now run the second CNN baseline: the \"non-static\" (or fine-tuned) version.\n","\n","This experiment uses the `configs/cnn_non_static.yaml` file, which sets **`freeze_embed: false`**. Unlike our first baseline, the model will now update the 300-dimensional GloVe embedding weights during training. This will significantly increase the number of trainable parameters and test if fine-tuning the embeddings provides a performance boost."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3672697,"status":"ok","timestamp":1761366892410,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"HKyaCTc1jc21","outputId":"0497c720-31af-4b39-b687-c7775b59e5a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 03:33:45.585371: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 03:33:45.602490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761363225.623540   36743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761363225.629889   36743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761363225.645835   36743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761363225.645867   36743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761363225.645870   36743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761363225.645873   36743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 03:33:45.650549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_non_static.yaml\n","[INFO] No seed provided, running with system randomness.\n","[INFO] Using device: cuda\n","[INFO] Saving best model to: outputs/models/cnn_non_static.pt\n","[INFO] Saving metrics to: outputs/metrics/cnn_non_static_metrics.json\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 20549.59it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 38668.84it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 36,940,502 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [04:33<00:00,  1.64it/s]\n","Train Loss: 0.4613 | Train Acc: 0.7766\n","Evaluating: 100% 50/50 [00:00<00:00, 107.50it/s]\n","Val/Test Loss: 0.3111 | Acc: 0.8636 | F1: 0.8709\n","[INFO] New best model saved to outputs/models/cnn_non_static.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [08:07<00:00,  1.08s/it]\n","Train Loss: 0.3439 | Train Acc: 0.8524\n","Evaluating: 100% 50/50 [00:00<00:00, 104.42it/s]\n","Val/Test Loss: 0.2961 | Acc: 0.8772 | F1: 0.8845\n","[INFO] New best model saved to outputs/models/cnn_non_static.pt\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [04:19<00:00,  1.73it/s]\n","Train Loss: 0.2900 | Train Acc: 0.8821\n","Evaluating: 100% 50/50 [00:00<00:00, 106.70it/s]\n","Val/Test Loss: 0.3645 | Acc: 0.8460 | F1: 0.8273\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [10:11<00:00,  1.36s/it]\n","Train Loss: 0.2424 | Train Acc: 0.9028\n","Evaluating: 100% 50/50 [00:00<00:00, 105.98it/s]\n","Val/Test Loss: 0.2845 | Acc: 0.8852 | F1: 0.8821\n","[INFO] New best model saved to outputs/models/cnn_non_static.pt\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:15<00:00, 29.15it/s]\n","Train Loss: 0.1941 | Train Acc: 0.9246\n","Evaluating: 100% 50/50 [00:00<00:00, 106.31it/s]\n","Val/Test Loss: 0.2940 | Acc: 0.8852 | F1: 0.8828\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [06:25<00:00,  1.17it/s]\n","Train Loss: 0.1610 | Train Acc: 0.9383\n","Evaluating: 100% 50/50 [00:00<00:00, 107.81it/s]\n","Val/Test Loss: 0.3064 | Acc: 0.8884 | F1: 0.8905\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:15<00:00, 29.27it/s]\n","Train Loss: 0.1358 | Train Acc: 0.9505\n","Evaluating: 100% 50/50 [00:00<00:00, 108.10it/s]\n","Val/Test Loss: 0.3360 | Acc: 0.8820 | F1: 0.8798\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [02:18<00:00,  3.25it/s]\n","Train Loss: 0.1100 | Train Acc: 0.9596\n","Evaluating: 100% 50/50 [00:00<00:00, 108.87it/s]\n","Val/Test Loss: 0.3422 | Acc: 0.8928 | F1: 0.8940\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:15<00:00, 29.26it/s]\n","Train Loss: 0.0995 | Train Acc: 0.9636\n","Evaluating: 100% 50/50 [00:00<00:00, 106.53it/s]\n","Val/Test Loss: 0.3856 | Acc: 0.8876 | F1: 0.8920\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [04:05<00:00,  1.83it/s]\n","Train Loss: 0.0811 | Train Acc: 0.9715\n","Evaluating: 100% 50/50 [00:00<00:00, 107.53it/s]\n","Val/Test Loss: 0.4063 | Acc: 0.8880 | F1: 0.8908\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [06:28<00:00,  1.16it/s]\n","Train Loss: 0.0744 | Train Acc: 0.9737\n","Evaluating: 100% 50/50 [00:00<00:00, 106.46it/s]\n","Val/Test Loss: 0.4311 | Acc: 0.8876 | F1: 0.8912\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:15<00:00, 29.21it/s]\n","Train Loss: 0.0652 | Train Acc: 0.9768\n","Evaluating: 100% 50/50 [00:00<00:00, 106.24it/s]\n","Val/Test Loss: 0.4730 | Acc: 0.8748 | F1: 0.8673\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [04:36<00:00,  1.63it/s]\n","Train Loss: 0.0580 | Train Acc: 0.9808\n","Evaluating: 100% 50/50 [00:00<00:00, 105.10it/s]\n","Val/Test Loss: 0.4674 | Acc: 0.8844 | F1: 0.8829\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [02:18<00:00,  3.26it/s]\n","Train Loss: 0.0561 | Train Acc: 0.9807\n","Evaluating: 100% 50/50 [00:00<00:00, 107.85it/s]\n","Val/Test Loss: 0.4723 | Acc: 0.8860 | F1: 0.8885\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [05:58<00:00,  1.25it/s]\n","Train Loss: 0.0447 | Train Acc: 0.9847\n","Evaluating: 100% 50/50 [00:00<00:00, 105.49it/s]\n","Val/Test Loss: 0.4807 | Acc: 0.8856 | F1: 0.8871\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","[INFO] Loaded best model from outputs/models/cnn_non_static.pt\n","Evaluating: 100% 500/500 [00:05<00:00, 87.68it/s] \n","Val/Test Loss: 0.3038 | Acc: 0.8748 | F1: 0.8696\n","===== Final Test Results (Random Seed) =====\n","Test Loss: 0.3038 | Test Acc: 0.8748 | Test F1: 0.8696\n","[INFO] All metrics saved to outputs/metrics/cnn_non_static_metrics.json\n"]}],"source":["!python run_experiment.py --config configs/cnn_non_static.yaml"]},{"cell_type":"markdown","metadata":{"id":"OZ_iftDT_Ujp"},"source":["### Conclusion: Kim CNN Baselines\n","\n","We have successfully run both planned baseline experiments using the Kim (2014) CNN architecture, adapted for the IMDb dataset, using system randomness. These runs establish our baseline performance before the main seeded experiments.\n","\n","1.  **Experiment 1: Static Embeddings (`cnn_baseline.yaml`)**\n","    * **Trainable Parameters:** 360,902\n","    * **Best Validation Loss:** 0.2862 (Epoch 4)\n","    * **Final Test Accuracy:** **87.29%**\n","    * **Final Test F1 Score:** **87.76%**\n","\n","2.  **Experiment 2: Non-Static Embeddings (`cnn_non_static.yaml`)**\n","    * **Trainable Parameters:** 36,940,502\n","    * **Best Validation Loss:** 0.2845 (Epoch 4)\n","    * **Final Test Accuracy:** **87.48%**\n","    * **Final Test F1 Score:** **86.96%**\n","\n","---\n","\n","#### Key Findings\n","\n","* **Performance:** In this specific run, the results were very close. The \"static\" model achieved a slightly higher F1 score (87.76% vs. 86.96%), while the \"non-static\" model achieved a slightly higher accuracy (87.48% vs. 87.29%). This highlights the run-to-run variability when seeds are not controlled.\n","* **Efficiency:** The static model remained significantly more efficient, training much faster per epoch (~6-9 sec vs. ~15-20+ sec for non-static in this run) due to the vastly fewer trainable parameters.\n","* **Overall Baseline:** Both models performed well, confirming the Kim CNN architecture is suitable for this task. The baseline performance is established in the **~87.3% - 87.5% accuracy** range and **~87.0% - 87.8% F1 score** range. This sets a clear benchmark for the subsequent BERT experiments."]},{"cell_type":"markdown","metadata":{"id":"UHm20O_JYRnJ"},"source":["# Section 3: BERT-based-uncased Layer-Freezing Experiment (with random Seeding)"]},{"cell_type":"markdown","metadata":{"id":"yRa1G6o708gM"},"source":["### Experiment 1: BERT Full Fine-Tune\n","\n","Now that we have established our Kim CNN baselines (achieving **~87.3% - 87.5% Accuracy / ~87.0% - 87.8% F1 Score** without explicit seeding), it's time to run our first BERT experiment.\n","\n","This will use the `configs/bert_full_finetune.yaml` config file. Our `run_experiment.py` script will:\n","\n","1.  Identify the `model: type: \"bert\"` from the config.\n","2.  Load the pre-trained `bert-base-uncased` model and its tokenizer from Hugging Face.\n","3.  Initialize the `SentimentDataset`, which will now use the BERT tokenizer to create `input_ids` and `attention_mask`.\n","4.  Fine-tune **all 110 million parameters** of the model (no layers are frozen).\n","5.  Use the hyperparameters from the Devlin et al. (2019) paper (e.g., `batch_size: 16`, specific `learning_rate`).\n","6.  Save the best model to `outputs/models/bert_full_finetune.pt` and metrics to `outputs/metrics/bert_full_finetune_metrics.json`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4582260,"status":"ok","timestamp":1761371474673,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"35yFnY3h1BZ1","outputId":"663e51dc-62b3-430d-e202-6ae1b428db91"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 04:34:58.301251: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 04:34:58.319391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761366898.341173   52487 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761366898.347887   52487 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761366898.364529   52487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761366898.364558   52487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761366898.364561   52487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761366898.364563   52487 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 04:34:58.369452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_full_finetune.yaml\n","[INFO] No seed provided, running with system randomness.\n","[INFO] Using device: cuda\n","[INFO] Saving best model to: outputs/models/bert_full_finetune.pt\n","[INFO] Saving metrics to: outputs/metrics/bert_full_finetune_metrics.json\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Trainable parameters: 109,483,778 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [16:24<00:00,  1.43it/s]\n","Train Loss: 0.2751 | Train Acc: 0.8817\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.1697 | Acc: 0.9332 | F1: 0.9344\n","[INFO] New best model saved to outputs/models/bert_full_finetune.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [16:24<00:00,  1.43it/s]\n","Train Loss: 0.1234 | Train Acc: 0.9568\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.1649 | Acc: 0.9404 | F1: 0.9407\n","[INFO] New best model saved to outputs/models/bert_full_finetune.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [16:24<00:00,  1.43it/s]\n","Train Loss: 0.0525 | Train Acc: 0.9838\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2246 | Acc: 0.9368 | F1: 0.9387\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [16:25<00:00,  1.43it/s]\n","Train Loss: 0.0222 | Train Acc: 0.9946\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2298 | Acc: 0.9444 | F1: 0.9447\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","[INFO] Loaded best model from outputs/models/bert_full_finetune.pt\n","Evaluating: 100% 1563/1563 [07:17<00:00,  3.57it/s]\n","Val/Test Loss: 0.1630 | Acc: 0.9414 | F1: 0.9416\n","===== Final Test Results (Random Seed) =====\n","Test Loss: 0.1630 | Test Acc: 0.9414 | Test F1: 0.9416\n","[INFO] All metrics saved to outputs/metrics/bert_full_finetune_metrics.json\n"]}],"source":["!python run_experiment.py --config configs/bert_full_finetune.yaml"]},{"cell_type":"markdown","metadata":{"id":"yApyan4sImWW"},"source":["### Experiment 2: BERT with 4 Frozen Layers\n","\n","We now have our two key baselines from the non-seeded runs: the Kim CNN (**~87.5% Acc / ~87.8% F1**) and the fully fine-tuned BERT (**94.14% Acc / 94.16% F1**).\n","\n","This next experiment tests our first hypothesis (from Sun et al., 2019). We will freeze the BERT embedding layer and the first 4 encoder layers using the `configs/bert_frozen_4.yaml` file. This means we will only be fine-tuning the last 8 layers of the model, which should train faster and be more computationally efficient.\n","\n","The question is: How much accuracy will we lose (if any) by training only ~60% of the model?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3760965,"status":"ok","timestamp":1761375235644,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"cE4FYitOIn3R","outputId":"86429cf9-9eac-4b48-aea3-75137b087e23"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 05:51:20.616864: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 05:51:20.634254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761371480.655934   72049 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761371480.662475   72049 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761371480.678837   72049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761371480.678869   72049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761371480.678872   72049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761371480.678874   72049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 05:51:20.683795: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_4.yaml\n","[INFO] No seed provided, running with system randomness.\n","[INFO] Using device: cuda\n","[INFO] Saving best model to: outputs/models/bert_frozen_4.pt\n","[INFO] Saving metrics to: outputs/metrics/bert_frozen_4_metrics.json\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 4 encoder layers.\n","[INFO] Trainable parameters: 57,295,106 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [13:00<00:00,  1.80it/s]\n","Train Loss: 0.2870 | Train Acc: 0.8723\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.1817 | Acc: 0.9292 | F1: 0.9299\n","[INFO] New best model saved to outputs/models/bert_frozen_4.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [13:00<00:00,  1.80it/s]\n","Train Loss: 0.1435 | Train Acc: 0.9473\n","Evaluating: 100% 157/157 [00:43<00:00,  3.59it/s]\n","Val/Test Loss: 0.1976 | Acc: 0.9316 | F1: 0.9315\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [13:00<00:00,  1.80it/s]\n","Train Loss: 0.0739 | Train Acc: 0.9762\n","Evaluating: 100% 157/157 [00:43<00:00,  3.59it/s]\n","Val/Test Loss: 0.2148 | Acc: 0.9376 | F1: 0.9376\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [13:00<00:00,  1.80it/s]\n","Train Loss: 0.0380 | Train Acc: 0.9896\n","Evaluating: 100% 157/157 [00:43<00:00,  3.59it/s]\n","Val/Test Loss: 0.2417 | Acc: 0.9352 | F1: 0.9361\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","[INFO] Loaded best model from outputs/models/bert_frozen_4.pt\n","Evaluating: 100% 1563/1563 [07:16<00:00,  3.58it/s]\n","Val/Test Loss: 0.1701 | Acc: 0.9349 | F1: 0.9356\n","===== Final Test Results (Random Seed) =====\n","Test Loss: 0.1701 | Test Acc: 0.9349 | Test F1: 0.9356\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_4_metrics.json\n"]}],"source":["!python run_experiment.py --config configs/bert_frozen_4.yaml"]},{"cell_type":"markdown","metadata":{"id":"A_K90Am-byHs"},"source":["### Experiment 3: BERT with 8 Frozen Layers\n","\n","The results from the first two BERT experiments (run without specific seeds) show:\n","* **Full Fine-Tune:** 94.14% Accuracy / 94.16% F1 Score\n","* **Frozen 4 Layers:** 93.49% Accuracy / 93.56% F1 Score\n","\n","In this particular run, freezing the first 4 layers resulted in a slight performance decrease compared to the full fine-tune, though it was significantly faster (~13 min/epoch vs. ~16.5 min/epoch). This differs slightly from our initial seeded run, highlighting the effect of run-to-run variability.\n","\n","Now we push the layer-freezing hypothesis further using the `configs/bert_frozen_8.yaml` file. We will freeze the embedding layer and the first 8 encoder layers, fine-tuning only the final 4 \"task-specific\" layers. This will be even faster, but will the performance drop further?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGv925orb0K4","outputId":"97c3e273-8c95-4233-f76e-dab57f1029eb","executionInfo":{"status":"ok","timestamp":1761378260334,"user_tz":300,"elapsed":3024687,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 06:54:01.512771: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 06:54:01.531162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761375241.552739   88105 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761375241.559602   88105 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761375241.576810   88105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761375241.576849   88105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761375241.576852   88105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761375241.576855   88105 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 06:54:01.581940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_8.yaml\n","[INFO] No seed provided, running with system randomness.\n","[INFO] Using device: cuda\n","[INFO] Saving best model to: outputs/models/bert_frozen_8.pt\n","[INFO] Saving metrics to: outputs/metrics/bert_frozen_8_metrics.json\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 8 encoder layers.\n","[INFO] Trainable parameters: 28,943,618 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [09:55<00:00,  2.36it/s]\n","Train Loss: 0.3007 | Train Acc: 0.8647\n","Evaluating: 100% 157/157 [00:43<00:00,  3.59it/s]\n","Val/Test Loss: 0.2463 | Acc: 0.9032 | F1: 0.9096\n","[INFO] New best model saved to outputs/models/bert_frozen_8.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [09:55<00:00,  2.36it/s]\n","Train Loss: 0.1863 | Train Acc: 0.9283\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.1980 | Acc: 0.9312 | F1: 0.9301\n","[INFO] New best model saved to outputs/models/bert_frozen_8.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [09:55<00:00,  2.36it/s]\n","Train Loss: 0.1440 | Train Acc: 0.9460\n","Evaluating: 100% 157/157 [00:43<00:00,  3.59it/s]\n","Val/Test Loss: 0.1889 | Acc: 0.9364 | F1: 0.9361\n","[INFO] New best model saved to outputs/models/bert_frozen_8.pt\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [09:55<00:00,  2.36it/s]\n","Train Loss: 0.1108 | Train Acc: 0.9605\n","Evaluating: 100% 157/157 [00:43<00:00,  3.59it/s]\n","Val/Test Loss: 0.1962 | Acc: 0.9352 | F1: 0.9359\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","[INFO] Loaded best model from outputs/models/bert_frozen_8.pt\n","Evaluating: 100% 1563/1563 [07:16<00:00,  3.58it/s]\n","Val/Test Loss: 0.1773 | Acc: 0.9365 | F1: 0.9362\n","===== Final Test Results (Random Seed) =====\n","Test Loss: 0.1773 | Test Acc: 0.9365 | Test F1: 0.9362\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_8_metrics.json\n"]}],"source":["!python run_experiment.py --config configs/bert_frozen_8.yaml"]},{"cell_type":"markdown","metadata":{"id":"elekP8Imk2gR"},"source":["### Experiment 4: BERT with 11 Frozen Layers\n","\n","We have now seen the trend across the first three non-seeded BERT runs:\n","* **Full Fine-Tune (0 Frozen):** 94.14% Acc / 94.16% F1\n","* **Frozen 4 Layers:** 93.49% Acc / 93.56% F1 (~13 min/epoch)\n","* **Frozen 8 Layers:** 93.65% Acc / 93.62% F1 (~10 min/epoch)\n","\n","Interestingly, in these non-seeded runs, freezing 8 layers performed slightly better than freezing 4, while both were slightly below the full fine-tune but significantly faster. This reinforces the efficiency gains of layer freezing.\n","\n","This is the final and most extreme test using `configs/bert_frozen_11.yaml`. We will freeze all encoder layers *except the last one* (`freeze_layers: 11`). We are now fine-tuning only the single most task-specific layer. This will be the fastest run, but we expect a more significant performance drop. This will help us find the ultimate balance between efficiency and accuracy for these initial runs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZZB3aGAV-2k","outputId":"24de1ebe-ff30-4ba0-97f6-153162db8fd4","executionInfo":{"status":"ok","timestamp":1761394376481,"user_tz":300,"elapsed":2479494,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 11:31:58.115577: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 11:31:58.133197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761391918.154970    3535 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761391918.161469    3535 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761391918.178105    3535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761391918.178145    3535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761391918.178149    3535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761391918.178155    3535 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 11:31:58.183165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_11.yaml\n","[INFO] No seed provided, running with system randomness.\n","[INFO] Using device: cuda\n","[INFO] Saving best model to: outputs/models/bert_frozen_11.pt\n","[INFO] Saving metrics to: outputs/metrics/bert_frozen_11_metrics.json\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 244kB/s]\n","vocab.txt: 100% 232k/232k [00:00<00:00, 542kB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 1.08MB/s]\n","config.json: 100% 570/570 [00:00<00:00, 5.14MB/s]\n","model.safetensors: 100% 440M/440M [00:01<00:00, 240MB/s]\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 11 encoder layers.\n","[INFO] Trainable parameters: 7,680,002 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [07:27<00:00,  3.15it/s]\n","Train Loss: 0.3612 | Train Acc: 0.8282\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.2312 | Acc: 0.9108 | F1: 0.9122\n","[INFO] New best model saved to outputs/models/bert_frozen_11.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [07:29<00:00,  3.13it/s]\n","Train Loss: 0.2282 | Train Acc: 0.9100\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2132 | Acc: 0.9160 | F1: 0.9147\n","[INFO] New best model saved to outputs/models/bert_frozen_11.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [07:28<00:00,  3.13it/s]\n","Train Loss: 0.2160 | Train Acc: 0.9148\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2097 | Acc: 0.9220 | F1: 0.9232\n","[INFO] New best model saved to outputs/models/bert_frozen_11.pt\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [07:29<00:00,  3.13it/s]\n","Train Loss: 0.2058 | Train Acc: 0.9204\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2071 | Acc: 0.9216 | F1: 0.9222\n","[INFO] New best model saved to outputs/models/bert_frozen_11.pt\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","[INFO] Loaded best model from outputs/models/bert_frozen_11.pt\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.62it/s]\n","Val/Test Loss: 0.2023 | Acc: 0.9216 | F1: 0.9225\n","===== Final Test Results (Random Seed) =====\n","Test Loss: 0.2023 | Test Acc: 0.9216 | Test F1: 0.9225\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_11_metrics.json\n"]}],"source":["!python run_experiment.py --config configs/bert_frozen_11.yaml"]},{"cell_type":"markdown","metadata":{"id":"l0VbxLYeWPIm"},"source":["### 5. Conclusion: Layer-Freezing Experiment Results (Non-Seeded Runs)\n","\n","All initial experiments (run without specific seeds) for our first hypothesis are now complete. We have successfully trained the Kim CNN baselines and all four BERT variants. The `run_experiment.py` script saved the best-performing model (based on validation loss) for each run.\n","\n","Here are the final test set results for the four BERT models from these initial runs:\n","\n","* **`bert_full_finetune` (0 Frozen):**\n","    * Test Accuracy: **94.14%**\n","    * Test F1 Score: **94.16%**\n","    * Key Hyperparams: `batch_size: 16`, `epochs: 4`, `lr: 0.00002`\n","\n","* **`bert_frozen_4` (4 Frozen):**\n","    * Test Accuracy: **93.49%**\n","    * Test F1 Score: **93.56%**\n","    * Key Hyperparams: `batch_size: 16`, `epochs: 4`, `lr: 0.00002`\n","\n","* **`bert_frozen_8` (8 Frozen):**\n","    * Test Accuracy: **93.65%**\n","    * Test F1 Score: **93.62%**\n","    * Key Hyperparams: `batch_size: 16`, `epochs: 4`, `lr: 0.00002`\n","\n","* **`bert_frozen_11` (11 Frozen):**\n","    * Test Accuracy: **92.16%**\n","    * Test F1 Score: **92.25%**\n","    * Key Hyperparams: `batch_size: 16`, `epochs: 4`, `lr: 0.00002`\n","\n","---\n","\n","### Key Findings (Initial Runs)\n","\n","These results provide a first look at the trends and support our hypotheses:\n","\n","1.  **BERT's Superiority:** All BERT models significantly outperformed the Kim CNN baselines (~87.3% - 87.5% Acc / ~87.0% - 87.8% F1), with the best BERT model achieving over 94% accuracy.\n","\n","2.  **Layer Freezing Impact:**\n","    * In these specific runs, the **full fine-tune** achieved the highest accuracy and F1 score.\n","    * Freezing 4 or 8 layers resulted in only a minor performance decrease (~0.5% - 0.7%) while offering substantial speed improvements (as observed by faster epoch times). Freezing 8 layers slightly outperformed freezing 4 in this instance.\n","    * Freezing 11 layers led to a more noticeable drop (~ 2% from the peak) but still maintained high performance (~92.2%) with the fastest training time.\n","\n","3.  **Efficiency vs. Accuracy Trade-off:** The results clearly show that layer freezing provides significant computational savings (faster training) for a relatively small cost in performance, especially when freezing up to 8 layers.\n","\n","These initial findings are promising. The next section, \"Statistical Validation Runs (with Seeding)\", will provide more robust results by averaging over multiple runs to account for variability. We are now ready to move on to our second hypothesis: **Probability Calibration**."]},{"cell_type":"markdown","metadata":{"id":"HCMlunkauXcH"},"source":["# Section 4: Statistical Validation Runs (with Seeding)\n","\n","We have now resolved all discrepancies in our methodology:\n","1.  **`max_length`:** Justified.\n","2.  **CNN Baselines:** We have configs for both static and non-static.\n","3.  **Metrics:** Our `evaluator.py` script now calculates F1, Precision, and Recall.\n","4.  **Seeding:** Our `run_experiment.py` script now accepts a `--seed` argument for reproducibility.\n","\n","The following cells will re-run all 6 of our core experiments. As discussed, we will run each experiment 3 times with different seeds (`42`, `123`, `2025`) to gather the data needed to analyze variance for our final report.\n","\n","This will generate 18 total metric files (6 experiments x 3 seeds)."]},{"cell_type":"markdown","metadata":{"id":"DdssyfkSuqnZ"},"source":["### 4.1 Kim CNN Baselines"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1928415,"status":"ok","timestamp":1761319893945,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"},"user_tz":300},"id":"4x25Ey4ouzJ9","outputId":"32ff06f9-6af3-4267-9a41-1a777c60b764"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-10-24 14:59:30.943696: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 14:59:30.961246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761317970.982678   16524 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761317970.989203   16524 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761317971.005707   16524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761317971.005733   16524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761317971.005736   16524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761317971.005739   16524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 14:59:31.010540: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_baseline.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 21589.32it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 39375.41it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 360,902 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:14<00:00, 31.51it/s]\n","Train Loss: 0.4732 | Train Acc: 0.7703\n","Evaluating: 100% 50/50 [00:00<00:00, 96.27it/s]\n","Val/Test Loss: 0.3387 | Acc: 0.8512 | F1: 0.8430\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed42.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.24it/s]\n","Train Loss: 0.3584 | Train Acc: 0.8456\n","Evaluating: 100% 50/50 [00:00<00:00, 97.34it/s]\n","Val/Test Loss: 0.3130 | Acc: 0.8628 | F1: 0.8718\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed42.pt\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.32it/s]\n","Train Loss: 0.3120 | Train Acc: 0.8687\n","Evaluating: 100% 50/50 [00:00<00:00, 96.98it/s]\n","Val/Test Loss: 0.2920 | Acc: 0.8756 | F1: 0.8735\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed42.pt\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.26it/s]\n","Train Loss: 0.2688 | Train Acc: 0.8883\n","Evaluating: 100% 50/50 [00:00<00:00, 96.71it/s]\n","Val/Test Loss: 0.3583 | Acc: 0.8488 | F1: 0.8314\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.26it/s]\n","Train Loss: 0.2368 | Train Acc: 0.9044\n","Evaluating: 100% 50/50 [00:00<00:00, 96.76it/s]\n","Val/Test Loss: 0.3178 | Acc: 0.8656 | F1: 0.8581\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.25it/s]\n","Train Loss: 0.2049 | Train Acc: 0.9192\n","Evaluating: 100% 50/50 [00:00<00:00, 96.20it/s]\n","Val/Test Loss: 0.3912 | Acc: 0.8524 | F1: 0.8385\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.21it/s]\n","Train Loss: 0.1815 | Train Acc: 0.9291\n","Evaluating: 100% 50/50 [00:00<00:00, 95.79it/s]\n","Val/Test Loss: 0.2950 | Acc: 0.8852 | F1: 0.8853\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.18it/s]\n","Train Loss: 0.1608 | Train Acc: 0.9385\n","Evaluating: 100% 50/50 [00:00<00:00, 96.06it/s]\n","Val/Test Loss: 0.3372 | Acc: 0.8716 | F1: 0.8774\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.17it/s]\n","Train Loss: 0.1344 | Train Acc: 0.9496\n","Evaluating: 100% 50/50 [00:00<00:00, 94.30it/s]\n","Val/Test Loss: 0.3358 | Acc: 0.8848 | F1: 0.8871\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.15it/s]\n","Train Loss: 0.1260 | Train Acc: 0.9524\n","Evaluating: 100% 50/50 [00:00<00:00, 95.96it/s]\n","Val/Test Loss: 0.3523 | Acc: 0.8816 | F1: 0.8805\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.23it/s]\n","Train Loss: 0.1179 | Train Acc: 0.9565\n","Evaluating: 100% 50/50 [00:00<00:00, 96.27it/s]\n","Val/Test Loss: 0.3676 | Acc: 0.8804 | F1: 0.8848\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.21it/s]\n","Train Loss: 0.1060 | Train Acc: 0.9620\n","Evaluating: 100% 50/50 [00:00<00:00, 96.18it/s]\n","Val/Test Loss: 0.4132 | Acc: 0.8632 | F1: 0.8543\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.24it/s]\n","Train Loss: 0.0949 | Train Acc: 0.9648\n","Evaluating: 100% 50/50 [00:00<00:00, 96.43it/s]\n","Val/Test Loss: 0.4420 | Acc: 0.8740 | F1: 0.8816\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.15it/s]\n","Train Loss: 0.0889 | Train Acc: 0.9688\n","Evaluating: 100% 50/50 [00:00<00:00, 95.54it/s]\n","Val/Test Loss: 0.4114 | Acc: 0.8776 | F1: 0.8831\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.20it/s]\n","Train Loss: 0.0837 | Train Acc: 0.9708\n","Evaluating: 100% 50/50 [00:00<00:00, 96.25it/s]\n","Val/Test Loss: 0.4310 | Acc: 0.8724 | F1: 0.8675\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 500/500 [00:05<00:00, 96.76it/s]\n","Val/Test Loss: 0.3096 | Acc: 0.8676 | F1: 0.8641\n","===== Final Test Results (Seed 42) =====\n","Test Loss: 0.3096 | Test Acc: 0.8676 | Test F1: 0.8641\n","[INFO] All metrics saved to outputs/metrics/cnn_baseline_seed42_metrics.json\n","2025-10-24 15:03:39.407037: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 15:03:39.424726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761318219.446032   17643 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761318219.452533   17643 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761318219.468872   17643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318219.468899   17643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318219.468902   17643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318219.468905   17643 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 15:03:39.473838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_baseline.yaml\n","[INFO] Set random seed to 123\n","[INFO] Using device: cuda\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 22067.87it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 37871.80it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 360,902 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:14<00:00, 31.36it/s]\n","Train Loss: 0.4659 | Train Acc: 0.7775\n","Evaluating: 100% 50/50 [00:00<00:00, 94.47it/s]\n","Val/Test Loss: 0.3179 | Acc: 0.8708 | F1: 0.8719\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed123.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.08it/s]\n","Train Loss: 0.3599 | Train Acc: 0.8453\n","Evaluating: 100% 50/50 [00:00<00:00, 95.16it/s]\n","Val/Test Loss: 0.3225 | Acc: 0.8612 | F1: 0.8728\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.08it/s]\n","Train Loss: 0.3121 | Train Acc: 0.8706\n","Evaluating: 100% 50/50 [00:00<00:00, 95.63it/s]\n","Val/Test Loss: 0.3954 | Acc: 0.8388 | F1: 0.8583\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.18it/s]\n","Train Loss: 0.2695 | Train Acc: 0.8878\n","Evaluating: 100% 50/50 [00:00<00:00, 96.46it/s]\n","Val/Test Loss: 0.3194 | Acc: 0.8656 | F1: 0.8549\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.25it/s]\n","Train Loss: 0.2340 | Train Acc: 0.9064\n","Evaluating: 100% 50/50 [00:00<00:00, 96.41it/s]\n","Val/Test Loss: 0.2932 | Acc: 0.8788 | F1: 0.8822\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed123.pt\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.18it/s]\n","Train Loss: 0.2001 | Train Acc: 0.9214\n","Evaluating: 100% 50/50 [00:00<00:00, 96.32it/s]\n","Val/Test Loss: 0.3327 | Acc: 0.8700 | F1: 0.8751\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.22it/s]\n","Train Loss: 0.1753 | Train Acc: 0.9321\n","Evaluating: 100% 50/50 [00:00<00:00, 96.46it/s]\n","Val/Test Loss: 0.3134 | Acc: 0.8848 | F1: 0.8825\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.19it/s]\n","Train Loss: 0.1554 | Train Acc: 0.9397\n","Evaluating: 100% 50/50 [00:00<00:00, 96.08it/s]\n","Val/Test Loss: 0.3532 | Acc: 0.8736 | F1: 0.8697\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.21it/s]\n","Train Loss: 0.1399 | Train Acc: 0.9478\n","Evaluating: 100% 50/50 [00:00<00:00, 95.31it/s]\n","Val/Test Loss: 0.3637 | Acc: 0.8772 | F1: 0.8735\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.14it/s]\n","Train Loss: 0.1264 | Train Acc: 0.9537\n","Evaluating: 100% 50/50 [00:00<00:00, 95.06it/s]\n","Val/Test Loss: 0.4664 | Acc: 0.8608 | F1: 0.8722\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.18it/s]\n","Train Loss: 0.1147 | Train Acc: 0.9579\n","Evaluating: 100% 50/50 [00:00<00:00, 96.12it/s]\n","Val/Test Loss: 0.4032 | Acc: 0.8684 | F1: 0.8748\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.17it/s]\n","Train Loss: 0.1022 | Train Acc: 0.9637\n","Evaluating: 100% 50/50 [00:00<00:00, 96.16it/s]\n","Val/Test Loss: 0.3787 | Acc: 0.8752 | F1: 0.8748\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.20it/s]\n","Train Loss: 0.0892 | Train Acc: 0.9678\n","Evaluating: 100% 50/50 [00:00<00:00, 95.51it/s]\n","Val/Test Loss: 0.4235 | Acc: 0.8768 | F1: 0.8827\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.17it/s]\n","Train Loss: 0.0901 | Train Acc: 0.9667\n","Evaluating: 100% 50/50 [00:00<00:00, 96.02it/s]\n","Val/Test Loss: 0.4317 | Acc: 0.8760 | F1: 0.8787\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.13it/s]\n","Train Loss: 0.0819 | Train Acc: 0.9704\n","Evaluating: 100% 50/50 [00:00<00:00, 95.94it/s]\n","Val/Test Loss: 0.4394 | Acc: 0.8748 | F1: 0.8715\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 500/500 [00:05<00:00, 97.05it/s]\n","Val/Test Loss: 0.3066 | Acc: 0.8741 | F1: 0.8762\n","===== Final Test Results (Seed 123) =====\n","Test Loss: 0.3066 | Test Acc: 0.8741 | Test F1: 0.8762\n","[INFO] All metrics saved to outputs/metrics/cnn_baseline_seed123_metrics.json\n","2025-10-24 15:07:48.342662: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 15:07:48.360076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761318468.381451   18749 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761318468.388031   18749 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761318468.404259   18749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318468.404287   18749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318468.404290   18749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318468.404293   18749 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 15:07:48.409114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_baseline.yaml\n","[INFO] Set random seed to 2025\n","[INFO] Using device: cuda\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 21354.33it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 39257.86it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 360,902 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:14<00:00, 31.36it/s]\n","Train Loss: 0.4707 | Train Acc: 0.7723\n","Evaluating: 100% 50/50 [00:00<00:00, 95.66it/s]\n","Val/Test Loss: 0.3131 | Acc: 0.8592 | F1: 0.8647\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed2025.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.07it/s]\n","Train Loss: 0.3605 | Train Acc: 0.8438\n","Evaluating: 100% 50/50 [00:00<00:00, 95.16it/s]\n","Val/Test Loss: 0.3193 | Acc: 0.8616 | F1: 0.8511\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.05it/s]\n","Train Loss: 0.3069 | Train Acc: 0.8729\n","Evaluating: 100% 50/50 [00:00<00:00, 95.71it/s]\n","Val/Test Loss: 0.2953 | Acc: 0.8820 | F1: 0.8892\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed2025.pt\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.13it/s]\n","Train Loss: 0.2651 | Train Acc: 0.8909\n","Evaluating: 100% 50/50 [00:00<00:00, 95.93it/s]\n","Val/Test Loss: 0.3212 | Acc: 0.8668 | F1: 0.8777\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.19it/s]\n","Train Loss: 0.2340 | Train Acc: 0.9067\n","Evaluating: 100% 50/50 [00:00<00:00, 95.51it/s]\n","Val/Test Loss: 0.2874 | Acc: 0.8796 | F1: 0.8778\n","[INFO] New best model saved to outputs/models/cnn_baseline_seed2025.pt\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.14it/s]\n","Train Loss: 0.2000 | Train Acc: 0.9235\n","Evaluating: 100% 50/50 [00:00<00:00, 95.69it/s]\n","Val/Test Loss: 0.2900 | Acc: 0.8888 | F1: 0.8922\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.17it/s]\n","Train Loss: 0.1770 | Train Acc: 0.9313\n","Evaluating: 100% 50/50 [00:00<00:00, 94.27it/s]\n","Val/Test Loss: 0.3559 | Acc: 0.8708 | F1: 0.8613\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.16it/s]\n","Train Loss: 0.1594 | Train Acc: 0.9403\n","Evaluating: 100% 50/50 [00:00<00:00, 95.49it/s]\n","Val/Test Loss: 0.3241 | Acc: 0.8876 | F1: 0.8877\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:14<00:00, 32.11it/s]\n","Train Loss: 0.1356 | Train Acc: 0.9471\n","Evaluating: 100% 50/50 [00:00<00:00, 95.66it/s]\n","Val/Test Loss: 0.3433 | Acc: 0.8856 | F1: 0.8883\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.19it/s]\n","Train Loss: 0.1199 | Train Acc: 0.9545\n","Evaluating: 100% 50/50 [00:00<00:00, 95.77it/s]\n","Val/Test Loss: 0.4044 | Acc: 0.8688 | F1: 0.8784\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.17it/s]\n","Train Loss: 0.1113 | Train Acc: 0.9586\n","Evaluating: 100% 50/50 [00:00<00:00, 95.68it/s]\n","Val/Test Loss: 0.4067 | Acc: 0.8764 | F1: 0.8838\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.21it/s]\n","Train Loss: 0.0982 | Train Acc: 0.9652\n","Evaluating: 100% 50/50 [00:00<00:00, 95.89it/s]\n","Val/Test Loss: 0.3821 | Acc: 0.8832 | F1: 0.8870\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.18it/s]\n","Train Loss: 0.0944 | Train Acc: 0.9660\n","Evaluating: 100% 50/50 [00:00<00:00, 96.35it/s]\n","Val/Test Loss: 0.4330 | Acc: 0.8676 | F1: 0.8593\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.21it/s]\n","Train Loss: 0.0914 | Train Acc: 0.9675\n","Evaluating: 100% 50/50 [00:00<00:00, 95.90it/s]\n","Val/Test Loss: 0.3923 | Acc: 0.8816 | F1: 0.8807\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:13<00:00, 32.16it/s]\n","Train Loss: 0.0802 | Train Acc: 0.9716\n","Evaluating: 100% 50/50 [00:00<00:00, 95.96it/s]\n","Val/Test Loss: 0.4087 | Acc: 0.8904 | F1: 0.8933\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 500/500 [00:05<00:00, 96.45it/s]\n","Val/Test Loss: 0.3134 | Acc: 0.8733 | F1: 0.8697\n","===== Final Test Results (Seed 2025) =====\n","Test Loss: 0.3134 | Test Acc: 0.8733 | Test F1: 0.8697\n","[INFO] All metrics saved to outputs/metrics/cnn_baseline_seed2025_metrics.json\n","2025-10-24 15:11:57.263727: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 15:11:57.281346: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761318717.302538   19845 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761318717.309011   19845 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761318717.325235   19845 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318717.325261   19845 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318717.325265   19845 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761318717.325267   19845 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 15:11:57.330097: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_non_static.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 21052.65it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 37101.39it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 36,940,502 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:24<00:00, 18.73it/s]\n","Train Loss: 0.4668 | Train Acc: 0.7745\n","Evaluating: 100% 50/50 [00:00<00:00, 95.09it/s]\n","Val/Test Loss: 0.3361 | Acc: 0.8508 | F1: 0.8405\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed42.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.03it/s]\n","Train Loss: 0.3406 | Train Acc: 0.8518\n","Evaluating: 100% 50/50 [00:00<00:00, 95.41it/s]\n","Val/Test Loss: 0.2978 | Acc: 0.8728 | F1: 0.8805\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed42.pt\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.03it/s]\n","Train Loss: 0.2885 | Train Acc: 0.8824\n","Evaluating: 100% 50/50 [00:00<00:00, 95.21it/s]\n","Val/Test Loss: 0.2898 | Acc: 0.8780 | F1: 0.8739\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed42.pt\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.04it/s]\n","Train Loss: 0.2410 | Train Acc: 0.9010\n","Evaluating: 100% 50/50 [00:00<00:00, 95.68it/s]\n","Val/Test Loss: 0.4257 | Acc: 0.8300 | F1: 0.8033\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.04it/s]\n","Train Loss: 0.2064 | Train Acc: 0.9200\n","Evaluating: 100% 50/50 [00:00<00:00, 95.59it/s]\n","Val/Test Loss: 0.2963 | Acc: 0.8828 | F1: 0.8821\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.03it/s]\n","Train Loss: 0.1645 | Train Acc: 0.9363\n","Evaluating: 100% 50/50 [00:00<00:00, 95.54it/s]\n","Val/Test Loss: 0.4120 | Acc: 0.8532 | F1: 0.8375\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.03it/s]\n","Train Loss: 0.1389 | Train Acc: 0.9482\n","Evaluating: 100% 50/50 [00:00<00:00, 95.13it/s]\n","Val/Test Loss: 0.3195 | Acc: 0.8892 | F1: 0.8903\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.1173 | Train Acc: 0.9559\n","Evaluating: 100% 50/50 [00:00<00:00, 95.40it/s]\n","Val/Test Loss: 0.3417 | Acc: 0.8880 | F1: 0.8917\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.04it/s]\n","Train Loss: 0.0982 | Train Acc: 0.9635\n","Evaluating: 100% 50/50 [00:00<00:00, 95.90it/s]\n","Val/Test Loss: 0.3799 | Acc: 0.8872 | F1: 0.8919\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.05it/s]\n","Train Loss: 0.0855 | Train Acc: 0.9712\n","Evaluating: 100% 50/50 [00:00<00:00, 95.71it/s]\n","Val/Test Loss: 0.4122 | Acc: 0.8848 | F1: 0.8901\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.0754 | Train Acc: 0.9719\n","Evaluating: 100% 50/50 [00:00<00:00, 95.39it/s]\n","Val/Test Loss: 0.4460 | Acc: 0.8772 | F1: 0.8710\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.0660 | Train Acc: 0.9773\n","Evaluating: 100% 50/50 [00:00<00:00, 94.85it/s]\n","Val/Test Loss: 0.4269 | Acc: 0.8792 | F1: 0.8761\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.05it/s]\n","Train Loss: 0.0613 | Train Acc: 0.9782\n","Evaluating: 100% 50/50 [00:00<00:00, 95.04it/s]\n","Val/Test Loss: 0.4469 | Acc: 0.8888 | F1: 0.8870\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.0560 | Train Acc: 0.9816\n","Evaluating: 100% 50/50 [00:00<00:00, 95.59it/s]\n","Val/Test Loss: 0.4653 | Acc: 0.8876 | F1: 0.8913\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.04it/s]\n","Train Loss: 0.0472 | Train Acc: 0.9841\n","Evaluating: 100% 50/50 [00:00<00:00, 94.71it/s]\n","Val/Test Loss: 0.4725 | Acc: 0.8908 | F1: 0.8901\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 500/500 [00:05<00:00, 95.72it/s]\n","Val/Test Loss: 0.2983 | Acc: 0.8734 | F1: 0.8683\n","===== Final Test Results (Seed 42) =====\n","Test Loss: 0.2983 | Test Acc: 0.8734 | Test F1: 0.8683\n","[INFO] All metrics saved to outputs/metrics/cnn_non_static_seed42_metrics.json\n","2025-10-24 15:18:31.484217: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 15:18:31.501948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761319111.523184   21509 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761319111.529696   21509 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761319111.545902   21509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761319111.545930   21509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761319111.545933   21509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761319111.545935   21509 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 15:18:31.550846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_non_static.yaml\n","[INFO] Set random seed to 123\n","[INFO] Using device: cuda\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 21985.52it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 38113.52it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 36,940,502 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:24<00:00, 18.64it/s]\n","Train Loss: 0.4632 | Train Acc: 0.7784\n","Evaluating: 100% 50/50 [00:00<00:00, 93.47it/s]\n","Val/Test Loss: 0.3118 | Acc: 0.8696 | F1: 0.8691\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed123.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:23<00:00, 18.83it/s]\n","Train Loss: 0.3466 | Train Acc: 0.8531\n","Evaluating: 100% 50/50 [00:00<00:00, 93.24it/s]\n","Val/Test Loss: 0.2992 | Acc: 0.8792 | F1: 0.8863\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed123.pt\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:23<00:00, 18.86it/s]\n","Train Loss: 0.2903 | Train Acc: 0.8802\n","Evaluating: 100% 50/50 [00:00<00:00, 94.12it/s]\n","Val/Test Loss: 0.3621 | Acc: 0.8512 | F1: 0.8665\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.02it/s]\n","Train Loss: 0.2434 | Train Acc: 0.9015\n","Evaluating: 100% 50/50 [00:00<00:00, 95.89it/s]\n","Val/Test Loss: 0.2951 | Acc: 0.8832 | F1: 0.8792\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed123.pt\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:23<00:00, 18.98it/s]\n","Train Loss: 0.1998 | Train Acc: 0.9209\n","Evaluating: 100% 50/50 [00:00<00:00, 95.33it/s]\n","Val/Test Loss: 0.3088 | Acc: 0.8800 | F1: 0.8773\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.03it/s]\n","Train Loss: 0.1675 | Train Acc: 0.9376\n","Evaluating: 100% 50/50 [00:00<00:00, 96.14it/s]\n","Val/Test Loss: 0.3245 | Acc: 0.8848 | F1: 0.8906\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.1360 | Train Acc: 0.9484\n","Evaluating: 100% 50/50 [00:00<00:00, 95.33it/s]\n","Val/Test Loss: 0.3360 | Acc: 0.8768 | F1: 0.8729\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.1161 | Train Acc: 0.9564\n","Evaluating: 100% 50/50 [00:00<00:00, 95.67it/s]\n","Val/Test Loss: 0.3470 | Acc: 0.8808 | F1: 0.8787\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.08it/s]\n","Train Loss: 0.0970 | Train Acc: 0.9642\n","Evaluating: 100% 50/50 [00:00<00:00, 95.21it/s]\n","Val/Test Loss: 0.4039 | Acc: 0.8672 | F1: 0.8588\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.0860 | Train Acc: 0.9692\n","Evaluating: 100% 50/50 [00:00<00:00, 95.85it/s]\n","Val/Test Loss: 0.4227 | Acc: 0.8824 | F1: 0.8868\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.0817 | Train Acc: 0.9703\n","Evaluating: 100% 50/50 [00:00<00:00, 96.31it/s]\n","Val/Test Loss: 0.4108 | Acc: 0.8876 | F1: 0.8877\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.05it/s]\n","Train Loss: 0.0634 | Train Acc: 0.9775\n","Evaluating: 100% 50/50 [00:00<00:00, 95.77it/s]\n","Val/Test Loss: 0.4612 | Acc: 0.8796 | F1: 0.8832\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.0545 | Train Acc: 0.9810\n","Evaluating: 100% 50/50 [00:00<00:00, 95.82it/s]\n","Val/Test Loss: 0.5237 | Acc: 0.8852 | F1: 0.8910\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.04it/s]\n","Train Loss: 0.0515 | Train Acc: 0.9829\n","Evaluating: 100% 50/50 [00:00<00:00, 95.98it/s]\n","Val/Test Loss: 0.5265 | Acc: 0.8880 | F1: 0.8930\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.0462 | Train Acc: 0.9842\n","Evaluating: 100% 50/50 [00:00<00:00, 95.76it/s]\n","Val/Test Loss: 0.4746 | Acc: 0.8892 | F1: 0.8906\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 500/500 [00:05<00:00, 96.51it/s]\n","Val/Test Loss: 0.3119 | Acc: 0.8727 | F1: 0.8671\n","===== Final Test Results (Seed 123) =====\n","Test Loss: 0.3119 | Test Acc: 0.8727 | Test F1: 0.8671\n","[INFO] All metrics saved to outputs/metrics/cnn_non_static_seed123_metrics.json\n","2025-10-24 15:25:05.925179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 15:25:05.942650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761319505.963897   23188 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761319505.970495   23188 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761319505.987075   23188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761319505.987102   23188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761319505.987105   23188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761319505.987107   23188 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 15:25:05.991989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/cnn_non_static.yaml\n","[INFO] Set random seed to 2025\n","[INFO] Using device: cuda\n","[INFO] Initializing Kim CNN pipeline...\n","[INFO] Building vocabulary from data/processed/train_clean.csv...\n","Building Vocab: 100% 22500/22500 [00:01<00:00, 21872.71it/s]\n","[INFO] Vocabulary built. Total size: 121932 tokens.\n","[INFO] Loading GloVe embeddings from data/embeddings/glove.6B.300d.txt...\n","Loading GloVe: 400000it [00:10, 37569.32it/s]\n","[INFO] GloVe embeddings loaded.\n","[INFO] 60570 / 121932 words found in GloVe vocab.\n","[INFO] Trainable parameters: 36,940,502 / 36,940,502\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/15 =====\n","Training: 100% 450/450 [00:24<00:00, 18.73it/s]\n","Train Loss: 0.4658 | Train Acc: 0.7731\n","Evaluating: 100% 50/50 [00:00<00:00, 95.82it/s]\n","Val/Test Loss: 0.3098 | Acc: 0.8628 | F1: 0.8649\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed2025.pt\n","\n","===== Epoch 2/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.01it/s]\n","Train Loss: 0.3455 | Train Acc: 0.8508\n","Evaluating: 100% 50/50 [00:00<00:00, 95.29it/s]\n","Val/Test Loss: 0.3236 | Acc: 0.8588 | F1: 0.8470\n","\n","===== Epoch 3/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.2884 | Train Acc: 0.8804\n","Evaluating: 100% 50/50 [00:00<00:00, 95.84it/s]\n","Val/Test Loss: 0.2843 | Acc: 0.8812 | F1: 0.8871\n","[INFO] New best model saved to outputs/models/cnn_non_static_seed2025.pt\n","\n","===== Epoch 4/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.2409 | Train Acc: 0.9031\n","Evaluating: 100% 50/50 [00:00<00:00, 96.16it/s]\n","Val/Test Loss: 0.2876 | Acc: 0.8832 | F1: 0.8886\n","\n","===== Epoch 5/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.2037 | Train Acc: 0.9190\n","Evaluating: 100% 50/50 [00:00<00:00, 95.79it/s]\n","Val/Test Loss: 0.2895 | Acc: 0.8876 | F1: 0.8854\n","\n","===== Epoch 6/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.1634 | Train Acc: 0.9378\n","Evaluating: 100% 50/50 [00:00<00:00, 95.48it/s]\n","Val/Test Loss: 0.3062 | Acc: 0.8852 | F1: 0.8847\n","\n","===== Epoch 7/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.1410 | Train Acc: 0.9473\n","Evaluating: 100% 50/50 [00:00<00:00, 96.43it/s]\n","Val/Test Loss: 0.4008 | Acc: 0.8632 | F1: 0.8518\n","\n","===== Epoch 8/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.1151 | Train Acc: 0.9565\n","Evaluating: 100% 50/50 [00:00<00:00, 95.96it/s]\n","Val/Test Loss: 0.3436 | Acc: 0.8916 | F1: 0.8933\n","\n","===== Epoch 9/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.0945 | Train Acc: 0.9663\n","Evaluating: 100% 50/50 [00:00<00:00, 96.17it/s]\n","Val/Test Loss: 0.3893 | Acc: 0.8860 | F1: 0.8902\n","\n","===== Epoch 10/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.0845 | Train Acc: 0.9695\n","Evaluating: 100% 50/50 [00:00<00:00, 96.33it/s]\n","Val/Test Loss: 0.3700 | Acc: 0.8924 | F1: 0.8941\n","\n","===== Epoch 11/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.08it/s]\n","Train Loss: 0.0721 | Train Acc: 0.9748\n","Evaluating: 100% 50/50 [00:00<00:00, 96.10it/s]\n","Val/Test Loss: 0.3976 | Acc: 0.8936 | F1: 0.8967\n","\n","===== Epoch 12/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.0645 | Train Acc: 0.9793\n","Evaluating: 100% 50/50 [00:00<00:00, 95.04it/s]\n","Val/Test Loss: 0.3933 | Acc: 0.8944 | F1: 0.8950\n","\n","===== Epoch 13/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.05it/s]\n","Train Loss: 0.0549 | Train Acc: 0.9808\n","Evaluating: 100% 50/50 [00:00<00:00, 95.86it/s]\n","Val/Test Loss: 0.4325 | Acc: 0.8876 | F1: 0.8864\n","\n","===== Epoch 14/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.07it/s]\n","Train Loss: 0.0520 | Train Acc: 0.9823\n","Evaluating: 100% 50/50 [00:00<00:00, 95.48it/s]\n","Val/Test Loss: 0.4462 | Acc: 0.8876 | F1: 0.8872\n","\n","===== Epoch 15/15 =====\n","Training: 100% 450/450 [00:23<00:00, 19.06it/s]\n","Train Loss: 0.0518 | Train Acc: 0.9824\n","Evaluating: 100% 50/50 [00:00<00:00, 96.12it/s]\n","Val/Test Loss: 0.4644 | Acc: 0.8940 | F1: 0.8943\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 500/500 [00:05<00:00, 96.48it/s]\n","Val/Test Loss: 0.2918 | Acc: 0.8765 | F1: 0.8822\n","===== Final Test Results (Seed 2025) =====\n","Test Loss: 0.2918 | Test Acc: 0.8765 | Test F1: 0.8822\n","[INFO] All metrics saved to outputs/metrics/cnn_non_static_seed2025_metrics.json\n"]}],"source":["# --- Run 1: CNN Static (Seed 42, 123, 2025) ---\n","!python run_experiment.py --config configs/cnn_baseline.yaml --seed 42\n","!python run_experiment.py --config configs/cnn_baseline.yaml --seed 123\n","!python run_experiment.py --config configs/cnn_baseline.yaml --seed 2025\n","\n","# --- Run 2: CNN Non-Static (Seed 42, 123, 2025) ---\n","!python run_experiment.py --config configs/cnn_non_static.yaml --seed 42\n","!python run_experiment.py --config configs/cnn_non_static.yaml --seed 123\n","!python run_experiment.py --config configs/cnn_non_static.yaml --seed 2025"]},{"cell_type":"markdown","metadata":{"id":"Z222zcd44Mi6"},"source":["### 4.2 BERT Layer-Freezing Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3sdmi9be4L2g","outputId":"a69aed5e-f358-463f-f342-ddad08e0345f"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-10-24 15:41:03.828123: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 15:41:03.846472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761320463.868680   27169 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761320463.875483   27169 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761320463.892582   27169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761320463.892613   27169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761320463.892616   27169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761320463.892618   27169 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 15:41:03.897588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_full_finetune.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 280kB/s]\n","vocab.txt: 100% 232k/232k [00:00<00:00, 33.8MB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 1.12MB/s]\n","config.json: 100% 570/570 [00:00<00:00, 4.42MB/s]\n","model.safetensors: 100% 440M/440M [00:01<00:00, 244MB/s]\n","[INFO] Trainable parameters: 109,483,778 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [16:08<00:00,  1.45it/s]\n","Train Loss: 0.2875 | Train Acc: 0.8747\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1774 | Acc: 0.9332 | F1: 0.9332\n","[INFO] New best model saved to outputs/models/bert_full_finetune_seed42.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.1287 | Train Acc: 0.9551\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1850 | Acc: 0.9380 | F1: 0.9392\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.0555 | Train Acc: 0.9843\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.2270 | Acc: 0.9336 | F1: 0.9355\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.0253 | Train Acc: 0.9942\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2180 | Acc: 0.9428 | F1: 0.9430\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.62it/s]\n","Val/Test Loss: 0.1678 | Acc: 0.9364 | F1: 0.9361\n","===== Final Test Results (Seed 42) =====\n","Test Loss: 0.1678 | Test Acc: 0.9364 | Test F1: 0.9361\n","[INFO] All metrics saved to outputs/metrics/bert_full_finetune_seed42_metrics.json\n","2025-10-24 16:56:07.057751: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 16:56:07.075120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761324967.096367   46164 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761324967.102902   46164 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761324967.119279   46164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761324967.119308   46164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761324967.119311   46164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761324967.119313   46164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 16:56:07.124152: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_full_finetune.yaml\n","[INFO] Set random seed to 123\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Trainable parameters: 109,483,778 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [16:11<00:00,  1.45it/s]\n","Train Loss: 0.2846 | Train Acc: 0.8751\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1825 | Acc: 0.9268 | F1: 0.9279\n","[INFO] New best model saved to outputs/models/bert_full_finetune_seed123.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [16:10<00:00,  1.45it/s]\n","Train Loss: 0.1282 | Train Acc: 0.9555\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.1681 | Acc: 0.9404 | F1: 0.9408\n","[INFO] New best model saved to outputs/models/bert_full_finetune_seed123.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [16:10<00:00,  1.45it/s]\n","Train Loss: 0.0515 | Train Acc: 0.9847\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2032 | Acc: 0.9448 | F1: 0.9447\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [16:10<00:00,  1.45it/s]\n","Train Loss: 0.0234 | Train Acc: 0.9948\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.2403 | Acc: 0.9424 | F1: 0.9424\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:13<00:00,  3.61it/s]\n","Val/Test Loss: 0.1580 | Acc: 0.9424 | F1: 0.9429\n","===== Final Test Results (Seed 123) =====\n","Test Loss: 0.1580 | Test Acc: 0.9424 | Test F1: 0.9429\n","[INFO] All metrics saved to outputs/metrics/bert_full_finetune_seed123_metrics.json\n","2025-10-24 18:11:13.098807: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 18:11:13.116619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761329473.137775   65402 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761329473.144197   65402 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761329473.160518   65402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761329473.160546   65402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761329473.160548   65402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761329473.160552   65402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 18:11:13.166165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_full_finetune.yaml\n","[INFO] Set random seed to 2025\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Trainable parameters: 109,483,778 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.2875 | Train Acc: 0.8750\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1780 | Acc: 0.9296 | F1: 0.9288\n","[INFO] New best model saved to outputs/models/bert_full_finetune_seed2025.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.1216 | Train Acc: 0.9576\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1694 | Acc: 0.9364 | F1: 0.9355\n","[INFO] New best model saved to outputs/models/bert_full_finetune_seed2025.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.0492 | Train Acc: 0.9852\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1952 | Acc: 0.9432 | F1: 0.9433\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [16:09<00:00,  1.45it/s]\n","Train Loss: 0.0220 | Train Acc: 0.9953\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.2257 | Acc: 0.9456 | F1: 0.9456\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.62it/s]\n","Val/Test Loss: 0.1638 | Acc: 0.9404 | F1: 0.9398\n","===== Final Test Results (Seed 2025) =====\n","Test Loss: 0.1638 | Test Acc: 0.9404 | Test F1: 0.9398\n","[INFO] All metrics saved to outputs/metrics/bert_full_finetune_seed2025_metrics.json\n","2025-10-24 19:26:11.883533: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 19:26:11.901373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761333971.922768   84067 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761333971.929331   84067 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761333971.945740   84067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761333971.945767   84067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761333971.945771   84067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761333971.945775   84067 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 19:26:11.950714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_4.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 4 encoder layers.\n","[INFO] Trainable parameters: 57,295,106 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [12:49<00:00,  1.83it/s]\n","Train Loss: 0.2908 | Train Acc: 0.8738\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1814 | Acc: 0.9300 | F1: 0.9297\n","[INFO] New best model saved to outputs/models/bert_frozen_4_seed42.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.1514 | Train Acc: 0.9464\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1917 | Acc: 0.9276 | F1: 0.9299\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.0818 | Train Acc: 0.9744\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1984 | Acc: 0.9380 | F1: 0.9384\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.0455 | Train Acc: 0.9873\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.2056 | Acc: 0.9448 | F1: 0.9448\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.63it/s]\n","Val/Test Loss: 0.1735 | Acc: 0.9328 | F1: 0.9323\n","===== Final Test Results (Seed 42) =====\n","Test Loss: 0.1735 | Test Acc: 0.9328 | Test F1: 0.9323\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_4_seed42_metrics.json\n","2025-10-24 20:27:48.472136: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 20:27:48.489979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761337668.511302   99291 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761337668.517885   99291 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761337668.534306   99291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761337668.534333   99291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761337668.534337   99291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761337668.534339   99291 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 20:27:48.539285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_4.yaml\n","[INFO] Set random seed to 123\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 4 encoder layers.\n","[INFO] Trainable parameters: 57,295,106 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [12:49<00:00,  1.83it/s]\n","Train Loss: 0.2893 | Train Acc: 0.8722\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1826 | Acc: 0.9276 | F1: 0.9284\n","[INFO] New best model saved to outputs/models/bert_frozen_4_seed123.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.1495 | Train Acc: 0.9465\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1876 | Acc: 0.9356 | F1: 0.9343\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.0782 | Train Acc: 0.9749\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1876 | Acc: 0.9372 | F1: 0.9373\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.0390 | Train Acc: 0.9890\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.2212 | Acc: 0.9388 | F1: 0.9391\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.63it/s]\n","Val/Test Loss: 0.1748 | Acc: 0.9322 | F1: 0.9329\n","===== Final Test Results (Seed 123) =====\n","Test Loss: 0.1748 | Test Acc: 0.9322 | Test F1: 0.9329\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_4_seed123_metrics.json\n","2025-10-24 21:29:22.943739: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 21:29:22.961409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761341362.982810  114375 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761341362.989442  114375 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761341363.005688  114375 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761341363.005717  114375 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761341363.005720  114375 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761341363.005722  114375 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 21:29:23.010622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_4.yaml\n","[INFO] Set random seed to 2025\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 4 encoder layers.\n","[INFO] Trainable parameters: 57,295,106 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [12:49<00:00,  1.83it/s]\n","Train Loss: 0.2927 | Train Acc: 0.8723\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1820 | Acc: 0.9260 | F1: 0.9253\n","[INFO] New best model saved to outputs/models/bert_frozen_4_seed2025.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [12:49<00:00,  1.83it/s]\n","Train Loss: 0.1449 | Train Acc: 0.9476\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1776 | Acc: 0.9380 | F1: 0.9374\n","[INFO] New best model saved to outputs/models/bert_frozen_4_seed2025.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.0752 | Train Acc: 0.9759\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1969 | Acc: 0.9420 | F1: 0.9421\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [12:48<00:00,  1.83it/s]\n","Train Loss: 0.0380 | Train Acc: 0.9896\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.2261 | Acc: 0.9396 | F1: 0.9397\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.62it/s]\n","Val/Test Loss: 0.1661 | Acc: 0.9394 | F1: 0.9390\n","===== Final Test Results (Seed 2025) =====\n","Test Loss: 0.1661 | Test Acc: 0.9394 | Test F1: 0.9390\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_4_seed2025_metrics.json\n","2025-10-24 22:30:59.520195: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 22:30:59.538102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761345059.559916  129439 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761345059.566757  129439 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761345059.584167  129439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761345059.584198  129439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761345059.584202  129439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761345059.584205  129439 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 22:30:59.589338: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_8.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 8 encoder layers.\n","[INFO] Trainable parameters: 28,943,618 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [09:47<00:00,  2.39it/s]\n","Train Loss: 0.3034 | Train Acc: 0.8648\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1908 | Acc: 0.9248 | F1: 0.9251\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed42.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [09:47<00:00,  2.40it/s]\n","Train Loss: 0.1869 | Train Acc: 0.9292\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1817 | Acc: 0.9344 | F1: 0.9353\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed42.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [09:47<00:00,  2.40it/s]\n","Train Loss: 0.1445 | Train Acc: 0.9469\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2072 | Acc: 0.9276 | F1: 0.9301\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [09:47<00:00,  2.40it/s]\n","Train Loss: 0.1091 | Train Acc: 0.9604\n","Evaluating: 100% 157/157 [00:43<00:00,  3.63it/s]\n","Val/Test Loss: 0.1925 | Acc: 0.9400 | F1: 0.9399\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.63it/s]\n","Val/Test Loss: 0.1742 | Acc: 0.9325 | F1: 0.9335\n","===== Final Test Results (Seed 42) =====\n","Test Loss: 0.1742 | Test Acc: 0.9325 | Test F1: 0.9335\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_8_seed42_metrics.json\n","2025-10-24 23:20:31.708614: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-24 23:20:31.726041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761348031.747182  141549 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761348031.753699  141549 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761348031.769940  141549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761348031.769966  141549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761348031.769970  141549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761348031.769974  141549 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-24 23:20:31.774892: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_8.yaml\n","[INFO] Set random seed to 123\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 8 encoder layers.\n","[INFO] Trainable parameters: 28,943,618 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.3008 | Train Acc: 0.8636\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.1898 | Acc: 0.9288 | F1: 0.9289\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed123.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.1842 | Train Acc: 0.9286\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.1845 | Acc: 0.9340 | F1: 0.9334\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed123.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.1420 | Train Acc: 0.9490\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1832 | Acc: 0.9380 | F1: 0.9381\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed123.pt\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.1058 | Train Acc: 0.9624\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.1917 | Acc: 0.9380 | F1: 0.9384\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:12<00:00,  3.61it/s]\n","Val/Test Loss: 0.1733 | Acc: 0.9362 | F1: 0.9365\n","===== Final Test Results (Seed 123) =====\n","Test Loss: 0.1733 | Test Acc: 0.9362 | Test F1: 0.9365\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_8_seed123_metrics.json\n","2025-10-25 00:10:08.715753: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 00:10:08.732810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761351008.753866  153685 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761351008.760307  153685 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761351008.776410  153685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761351008.776439  153685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761351008.776442  153685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761351008.776446  153685 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 00:10:08.781155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_8.yaml\n","[INFO] Set random seed to 2025\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 8 encoder layers.\n","[INFO] Trainable parameters: 28,943,618 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.3037 | Train Acc: 0.8658\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.1885 | Acc: 0.9284 | F1: 0.9282\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed2025.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.1821 | Train Acc: 0.9312\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1807 | Acc: 0.9360 | F1: 0.9363\n","[INFO] New best model saved to outputs/models/bert_frozen_8_seed2025.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [09:47<00:00,  2.39it/s]\n","Train Loss: 0.1388 | Train Acc: 0.9492\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.1812 | Acc: 0.9388 | F1: 0.9394\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [09:48<00:00,  2.39it/s]\n","Train Loss: 0.1065 | Train Acc: 0.9631\n","Evaluating: 100% 157/157 [00:43<00:00,  3.61it/s]\n","Val/Test Loss: 0.1893 | Acc: 0.9372 | F1: 0.9374\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:11<00:00,  3.62it/s]\n","Val/Test Loss: 0.1730 | Acc: 0.9350 | F1: 0.9353\n","===== Final Test Results (Seed 2025) =====\n","Test Loss: 0.1730 | Test Acc: 0.9350 | Test F1: 0.9353\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_8_seed2025_metrics.json\n","2025-10-25 00:59:43.224147: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 00:59:43.242649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761353983.264016  165822 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761353983.270551  165822 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761353983.287153  165822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761353983.287180  165822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761353983.287183  165822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761353983.287185  165822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 00:59:43.292094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_11.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 11 encoder layers.\n","[INFO] Trainable parameters: 7,680,002 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [07:29<00:00,  3.13it/s]\n","Train Loss: 0.3679 | Train Acc: 0.8269\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2247 | Acc: 0.9108 | F1: 0.9103\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed42.pt\n","\n","===== Epoch 2/4 =====\n","Training:  41% 573/1407 [03:02<04:20,  3.21it/s]"]}],"source":["# --- Run 3: BERT Full Fine-Tune (Seed 42, 123, 2025) ---\n","!python run_experiment.py --config configs/bert_full_finetune.yaml --seed 42\n","!python run_experiment.py --config configs/bert_full_finetune.yaml --seed 123\n","!python run_experiment.py --config configs/bert_full_finetune.yaml --seed 2025\n","\n","# --- Run 4: BERT Frozen 4 (Seed 42, 123, 2025) ---\n","!python run_experiment.py --config configs/bert_frozen_4.yaml --seed 42\n","!python run_experiment.py --config configs/bert_frozen_4.yaml --seed 123\n","!python run_experiment.py --config configs/bert_frozen_4.yaml --seed 2025\n","\n","# --- Run 5: BERT Frozen 8 (Seed 42, 123, 2025) ---\n","!python run_experiment.py --config configs/bert_frozen_8.yaml --seed 42\n","!python run_experiment.py --config configs/bert_frozen_8.yaml --seed 123\n","!python run_experiment.py --config configs/bert_frozen_8.yaml --seed 2025\n","\n","# --- Run 6: BERT Frozen 11 (Seed 42, 123, 2025) ---\n","!python run_experiment.py --config configs/bert_frozen_11.yaml --seed 42\n","!python run_experiment.py --config configs/bert_frozen_11.yaml --seed 123\n","!python run_experiment.py --config configs/bert_frozen_11.yaml --seed 2025"]},{"cell_type":"code","source":["!python run_experiment.py --config configs/bert_frozen_11.yaml --seed 42\n","!python run_experiment.py --config configs/bert_frozen_11.yaml --seed 123\n","!python run_experiment.py --config configs/bert_frozen_11.yaml --seed 2025"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPlq683i8AUy","executionInfo":{"status":"ok","timestamp":1761362424482,"user_tz":300,"elapsed":513392,"user":{"displayName":"Binamra Aryal","userId":"06227008953896428504"}},"outputId":"a049a489-8db3-4401-be29-bd8a3a43d9ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-10-25 01:17:44.853525: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 01:17:44.871244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761355064.893468    2087 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761355064.900112    2087 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761355064.916741    2087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761355064.916774    2087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761355064.916777    2087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761355064.916779    2087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 01:17:44.921665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_11.yaml\n","[INFO] Set random seed to 42\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 332kB/s]\n","vocab.txt: 100% 232k/232k [00:00<00:00, 517kB/s]\n","tokenizer.json: 100% 466k/466k [00:00<00:00, 1.06MB/s]\n","config.json: 100% 570/570 [00:00<00:00, 5.37MB/s]\n","model.safetensors: 100% 440M/440M [00:01<00:00, 225MB/s]\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 11 encoder layers.\n","[INFO] Trainable parameters: 7,680,002 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [07:27<00:00,  3.15it/s]\n","Train Loss: 0.3679 | Train Acc: 0.8269\n","Evaluating: 100% 157/157 [00:43<00:00,  3.62it/s]\n","Val/Test Loss: 0.2247 | Acc: 0.9108 | F1: 0.9103\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed42.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [07:32<00:00,  3.11it/s]\n","Train Loss: 0.2322 | Train Acc: 0.9072\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2139 | Acc: 0.9180 | F1: 0.9187\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed42.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [07:33<00:00,  3.10it/s]\n","Train Loss: 0.2179 | Train Acc: 0.9137\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2068 | Acc: 0.9192 | F1: 0.9193\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed42.pt\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [07:33<00:00,  3.10it/s]\n","Train Loss: 0.2045 | Train Acc: 0.9207\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2084 | Acc: 0.9216 | F1: 0.9216\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:16<00:00,  3.58it/s]\n","Val/Test Loss: 0.2016 | Acc: 0.9205 | F1: 0.9209\n","===== Final Test Results (Seed 42) =====\n","Test Loss: 0.2016 | Test Acc: 0.9205 | Test F1: 0.9209\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_11_seed42_metrics.json\n","2025-10-25 01:58:54.149275: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 01:58:54.166508: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761357534.187850   12784 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761357534.194238   12784 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761357534.210327   12784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761357534.210364   12784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761357534.210367   12784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761357534.210370   12784 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 01:58:54.215296: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_11.yaml\n","[INFO] Set random seed to 123\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 11 encoder layers.\n","[INFO] Trainable parameters: 7,680,002 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.10it/s]\n","Train Loss: 0.3657 | Train Acc: 0.8256\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2266 | Acc: 0.9056 | F1: 0.9038\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed123.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.10it/s]\n","Train Loss: 0.2319 | Train Acc: 0.9078\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2169 | Acc: 0.9136 | F1: 0.9125\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed123.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [07:33<00:00,  3.10it/s]\n","Train Loss: 0.2156 | Train Acc: 0.9143\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2089 | Acc: 0.9188 | F1: 0.9188\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed123.pt\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.10it/s]\n","Train Loss: 0.2057 | Train Acc: 0.9190\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2080 | Acc: 0.9184 | F1: 0.9183\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed123.pt\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:16<00:00,  3.58it/s]\n","Val/Test Loss: 0.1997 | Acc: 0.9226 | F1: 0.9228\n","===== Final Test Results (Seed 123) =====\n","Test Loss: 0.1997 | Test Acc: 0.9226 | Test F1: 0.9228\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_11_seed123_metrics.json\n","2025-10-25 02:39:40.299936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-10-25 02:39:40.317048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1761359980.338370   22957 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1761359980.344885   22957 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1761359980.360997   22957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761359980.361032   22957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761359980.361036   22957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1761359980.361041   22957 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-10-25 02:39:40.365925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO] Using configuration file: configs/bert_frozen_11.yaml\n","[INFO] Set random seed to 2025\n","[INFO] Using device: cuda\n","[INFO] Initializing BERT pipeline for model: bert-base-uncased\n","[INFO] Embedding layer frozen.\n","[INFO] Frozen first 11 encoder layers.\n","[INFO] Trainable parameters: 7,680,002 / 109,483,778\n","\n","[INFO] Starting training...\n","\n","===== Epoch 1/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.09it/s]\n","Train Loss: 0.3667 | Train Acc: 0.8266\n","Evaluating: 100% 157/157 [00:44<00:00,  3.57it/s]\n","Val/Test Loss: 0.2227 | Acc: 0.9136 | F1: 0.9135\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed2025.pt\n","\n","===== Epoch 2/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.09it/s]\n","Train Loss: 0.2329 | Train Acc: 0.9089\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2123 | Acc: 0.9212 | F1: 0.9217\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed2025.pt\n","\n","===== Epoch 3/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.10it/s]\n","Train Loss: 0.2138 | Train Acc: 0.9153\n","Evaluating: 100% 157/157 [00:43<00:00,  3.57it/s]\n","Val/Test Loss: 0.2086 | Acc: 0.9212 | F1: 0.9214\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed2025.pt\n","\n","===== Epoch 4/4 =====\n","Training: 100% 1407/1407 [07:34<00:00,  3.09it/s]\n","Train Loss: 0.2049 | Train Acc: 0.9191\n","Evaluating: 100% 157/157 [00:43<00:00,  3.58it/s]\n","Val/Test Loss: 0.2065 | Acc: 0.9216 | F1: 0.9216\n","[INFO] New best model saved to outputs/models/bert_frozen_11_seed2025.pt\n","\n","[INFO] Training complete. Loading best model for final test evaluation...\n","Evaluating: 100% 1563/1563 [07:17<00:00,  3.57it/s]\n","Val/Test Loss: 0.2012 | Acc: 0.9220 | F1: 0.9221\n","===== Final Test Results (Seed 2025) =====\n","Test Loss: 0.2012 | Test Acc: 0.9220 | Test F1: 0.9221\n","[INFO] All metrics saved to outputs/metrics/bert_frozen_11_seed2025_metrics.json\n"]}]},{"cell_type":"markdown","source":["### 4.3 Conclusion: Statistical Validation Runs\n","\n","All 18 planned experiments (6 configurations x 3 seeds) have been successfully completed. By running each configuration with multiple random seeds (`42`, `123`, `2025`), we can now analyze both the average performance and the stability (variance) of each approach. Our updated `run_experiment.py` script ensured reproducibility and captured Accuracy, F1 Score, Precision, and Recall for all runs.\n","\n","The table below summarizes the final **Test Set** results, showing the **Mean ()  Standard Deviation ()** for both Accuracy and F1 Score across the three runs for each configuration.\n","\n","| Configuration           | Mean Test Accuracy (  ) | Mean Test F1 Score (  ) |\n","| :---------------------- | :------------------------: | :----------------------: |\n","| Kim CNN (Static)        |     87.17%  0.33%         |     87.00%  0.51%       |\n","| Kim CNN (Non-Static)    |     87.42%  0.17%         |     87.25%  0.68%       |\n","| BERT (Full Fine-Tune)   |     **93.97%  0.25%** |     **93.96%  0.28%** |\n","| BERT (Frozen 4)         |     93.48%  0.33%         |     93.47%  0.31%       |\n","| BERT (Frozen 8)         |     93.46%  0.16%         |     93.51%  0.13%       |\n","| BERT (Frozen 11)        |     92.17%  0.09%         |     92.19%  0.08%       |\n","\n","---\n","\n","### Key Findings & Analysis\n","\n","1.  **CNN Baselines Confirmed:**\n","    * The \"non-static\" CNN (fine-tuned embeddings) slightly outperformed the \"static\" CNN on average (87.42% vs. 87.17% Accuracy), confirming this strategy provides a small benefit.\n","    * Both CNN models exhibit noticeable variance across runs ( up to ~0.7% for F1), underscoring the necessity of multiple seeds for reliable baseline establishment.\n","    * Our overall CNN baseline performance is firmly established around **~87.3% Accuracy / ~87.1% F1 Score**.\n","\n","2.  **BERT's Dominance Reaffirmed:**\n","    * All BERT configurations dramatically outperformed both CNN baselines. The weakest BERT configuration (`bert_frozen_11`) still achieved a mean accuracy of ~92.2%, nearly 5% higher than the best CNN average.\n","\n","3.  **Layer Freezing Analysis (Hypothesis 1 Supported):**\n","    * **Performance Trend:** Averaged across three seeds, the **Full Fine-Tune** model achieved the highest accuracy and F1 score (  93.97%). Performance decreased slightly but remained very strong when freezing 4 or 8 layers (  93.5%). Freezing 11 layers resulted in a more pronounced drop, but still maintained high performance (  92.2%).\n","    * **Variance Reduction:** As hypothesized, the standard deviation ($\\sigma$) generally *decreased* as more layers were frozen. The `bert_frozen_8` and `bert_frozen_11` runs demonstrated the highest stability (  0.1-0.2%), while the Full Fine-Tune and `bert_frozen_4` runs showed slightly more variability (  0.3-0.4%). This confirms that freezing lower layers can lead to more consistent results across different random initializations.\n","    * **Hypothesis 1 Confirmation:** Our hypothesis stated that freezing 8 layers would preserve >98% of the F1 score achieved by full fine-tuning.\n","        * Full Fine-Tune Mean F1: 93.96%\n","        * 98% Threshold: 0.9396 * 0.98 = 0.9208 (92.08%)\n","        * Frozen-8 Mean F1: **93.51%**\n","        * Since 93.51\\% > 92.08\\%, **Hypothesis 1 is strongly supported.** Freezing 8 layers preserved approximately **99.5%** of the peak F1 score.\n","    * **Efficiency Trade-off:** The results clearly quantify the trade-off. Full fine-tuning gives the best average performance. However, freezing 8 layers provides performance within ~ 0.5% of the peak, but with significantly reduced training time (observed ~ 40% speedup) and increased stability. Freezing 11 layers offers the greatest efficiency (~55% speedup vs. full fine-tune) while still achieving a very competitive ~92.2% accuracy. The optimal choice depends on the specific balance desired between performance and computational cost.\n","\n","---\n","\n","With these comprehensive and statistically validated results, we have successfully concluded the layer-freezing investigation. We have robust data to support our findings and have fully addressed the first part of our research question and Hypothesis 1. We are now fully prepared to proceed to the next phase: **Probability Calibration**."],"metadata":{"id":"cgG5BqsnYqUi"}},{"cell_type":"markdown","source":["# Summary Table: All Experiment Runs\n","\n","This table presents the final Test Set results and approximate training time per epoch for all experiments conducted, including the initial runs without explicit seeding (\"Random\") and the subsequent validation runs using specific seeds.\n","\n","| Configuration         | Seed   | Freeze Embed | Freeze Layers | Batch Size | LR       | Epochs | Test Accuracy        | Test F1 Score        | Train Time / Epoch (approx.) |\n","| :-------------------- | :----- | :----------- | :------------ | :--------- | :------- | :----- | :------------------- | :------------------- | :--------------------------- |\n","| `cnn_baseline`        | Random | True         | N/A           | 50         | 1.0      | 15     | 87.29%               | 87.76%               | ~9 sec                       |\n","| `cnn_non_static`      | Random | False        | N/A           | 50         | 1.0      | 15     | 87.48%               | 86.96%               | Highly Variable (~15s to >10m)*       |\n","| `bert_full_finetune`  | Random | False        | 0             | 16         | 0.00002  | 4      | 94.14%               | 94.16%               | ~16 min 24 sec               |\n","| `bert_frozen_4`       | Random | True         | 4             | 16         | 0.00002  | 4      | 93.49%               | 93.56%               | ~13 min 00 sec               |\n","| `bert_frozen_8`       | Random | True         | 8             | 16         | 0.00002  | 4      | 93.65%               | 93.62%               | ~9 min 55 sec                |\n","| `bert_frozen_11`      | Random | True         | 11            | 16         | 0.00002  | 4      | 92.16%               | 92.25%               | ~7 min 27 sec                |\n","| ---                   | ---    | ---          | ---           | ---        | ---      | ---    | ---                  | ---                  | ---                          |\n","| `cnn_baseline`        | 42     | True         | N/A           | 50         | 1.0      | 15     | 86.76%               | 86.41%               | ~14 sec                      |\n","| `cnn_baseline`        | 123    | True         | N/A           | 50         | 1.0      | 15     | 87.41%               | 87.62%               | ~14 sec                      |\n","| `cnn_baseline`        | 2025   | True         | N/A           | 50         | 1.0      | 15     | 87.33%               | 86.97%               | ~14 sec                      |\n","| `cnn_non_static`      | 42     | False        | N/A           | 50         | 1.0      | 15     | 87.34%               | 86.83%               | ~24 sec                      |\n","| `cnn_non_static`      | 123    | False        | N/A           | 50         | 1.0      | 15     | 87.27%               | 86.71%               | ~24 sec                      |\n","| `cnn_non_static`      | 2025   | False        | N/A           | 50         | 1.0      | 15     | 87.65%               | 88.22%               | ~24 sec                      |\n","| `bert_full_finetune`  | 42     | False        | 0             | 16         | 0.00002  | 4      | 93.64%               | 93.61%               | ~16 min 08 sec               |\n","| `bert_full_finetune`  | 123    | False        | 0             | 16         | 0.00002  | 4      | **94.24%** | **94.29%** | ~16 min 11 sec               |\n","| `bert_full_finetune`  | 2025   | False        | 0             | 16         | 0.00002  | 4      | 94.04%               | 93.98%               | ~16 min 09 sec               |\n","| `bert_frozen_4`       | 42     | True         | 4             | 16         | 0.00002  | 4      | 93.28%               | 93.23%               | ~12 min 49 sec               |\n","| `bert_frozen_4`       | 123    | True         | 4             | 16         | 0.00002  | 4      | 93.22%               | 93.29%               | ~12 min 49 sec               |\n","| `bert_frozen_4`       | 2025   | True         | 4             | 16         | 0.00002  | 4      | 93.94%               | 93.90%               | ~12 min 49 sec               |\n","| `bert_frozen_8`       | 42     | True         | 8             | 16         | 0.00002  | 4      | 93.25%               | 93.35%               | ~9 min 47 sec                |\n","| `bert_frozen_8`       | 123    | True         | 8             | 16         | 0.00002  | 4      | 93.62%               | 93.65%               | ~9 min 48 sec                |\n","| `bert_frozen_8`       | 2025   | True         | 8             | 16         | 0.00002  | 4      | 93.50%               | 93.53%               | ~9 min 48 sec                |\n","| `bert_frozen_11`      | 42     | True         | 11            | 16         | 0.00002  | 4      | 92.05%               | 92.09%               | ~7 min 27 sec                |\n","| `bert_frozen_11`      | 123    | True         | 11            | 16         | 0.00002  | 4      | 92.26%               | 92.28%               | ~7 min 34 sec                |\n","| `bert_frozen_11`      | 2025   | True         | 11            | 16         | 0.00002  | 4      | 92.20%               | 92.21%               | ~7 min 34 sec                |\n"],"metadata":{"id":"Tzus2CoOWS0O"}},{"cell_type":"markdown","source":["### Hyperparameter Comparison Table\n","\n","| Parameter            | `cnn_baseline`        | `cnn_non_static`      | `bert_full_finetune` | `bert_frozen_4`       | `bert_frozen_8`       | `bert_frozen_11`      |\n","| :------------------- | :-------------------- | :-------------------- | :------------------- | :-------------------- | :-------------------- | :-------------------- |\n","| **Model Type** | CNN                   | CNN                   | BERT                 | BERT                  | BERT                  | BERT                  |\n","| **Embeddings** | GloVe 300d            | GloVe 300d            | BERT Pretrain        | BERT Pretrain         | BERT Pretrain         | BERT Pretrain         |\n","| **Freeze Embed?** | True                  | False                 | False                | True                  | True                  | True                  |\n","| **Freeze Layers** | N/A                   | N/A                   | 0                    | 4                     | 8                     | 11                    |\n","| **Optimizer** | Adadelta              | Adadelta              | Adam                 | Adam                  | Adam                  | Adam                  |\n","| **Learning Rate** | 1.0                   | 1.0                   | 0.00002              | 0.00002               | 0.00002               | 0.00002               |\n","| **Batch Size** | 50                    | 50                    | 16                   | 16                    | 16                    | 16                    |\n","| **Epochs** | 15                    | 15                    | 4                    | 4                     | 4                     | 4                     |\n","| **Dropout Prob.** | 0.5                   | 0.5                   | 0.1                  | 0.1                   | 0.1                   | 0.1                   |\n","| **Weight Decay** | 0.0                   | 0.0                   | N/A                  | N/A                   | N/A                   | N/A                   |\n","| **Warmup Prop.** | N/A                   | N/A                   | 0.1                  | 0.1                   | 0.1                   | 0.1                   |\n","| **Max Length** | 512                   | 512                   | 512                  | 512                   | 512                   | 512                   |\n","| **CNN Num Filters** | 100                   | 100                   | N/A                  | N/A                   | N/A                   | N/A                   |\n","| **CNN Filter Sizes** | [3, 4, 5]             | [3, 4, 5]             | N/A                  | N/A                   | N/A                   | N/A                   |"],"metadata":{"id":"rm-cp3cYZa4p"}}],"metadata":{"colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}